{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML_25M Deep ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "import itertools\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "# from tab_transformer_pytorch import TabTransformer\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "# from pytorch_tabular import TabularModel\n",
    "# from pytorch_tabular.models import CategoryEmbeddingModelConfig\n",
    "# from pytorch_tabular.config import (\n",
    "#     DataConfig,\n",
    "#     OptimizerConfig,\n",
    "#     TrainerConfig,\n",
    "# )\n",
    "\n",
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "\n",
    "    Args:\n",
    "        seed: the seed to use. \n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "\n",
    "fix_random(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# PyTorch Device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_previous = True\n",
    "\n",
    "if delete_previous:\n",
    "    try:\n",
    "        os.system('rm -rf runs')\n",
    "        os.system('rm -rf models')\n",
    "        os.system('rm -rf best_model.pth')\n",
    "        os.system('rm -rf best_model_config.json')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 9946\n",
      "Number of validation samples: 1106\n",
      "Number of testing samples: 2764\n",
      "\n",
      "Number of features: 552\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('rating', axis=1)\n",
    "Y = df['rating']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "Y_train = Y_train.to_numpy()\n",
    "Y_val = Y_val.to_numpy()\n",
    "Y_test = Y_test.to_numpy()\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_val = pca.transform(X_val)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "print(f'Number of training samples: {X_train.shape[0]}')\n",
    "print(f'Number of validation samples: {X_val.shape[0]}')\n",
    "print(f'Number of testing samples: {X_test.shape[0]}')\n",
    "print(f'\\nNumber of features: {X_train.shape[1]}')\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(Y_val, dtype=torch.float32)), batch_size=Y_val.shape[0], shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(Y_test, dtype=torch.float32)), batch_size=Y_test.shape[0], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['Model', 'Accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_size, hidden_size, dropout_prob=0, depth=1):\n",
    "    model = [\n",
    "        torch.nn.Linear(input_size, hidden_size),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(dropout_prob)\n",
    "    ]\n",
    "\n",
    "    for i in range(depth):\n",
    "        model.append(torch.nn.Linear(hidden_size, hidden_size))\n",
    "        model.append(torch.nn.ReLU())\n",
    "        model.append(torch.nn.Dropout(dropout_prob))\n",
    "\n",
    "    model.append(torch.nn.Linear(hidden_size, 1))\n",
    "\n",
    "    return torch.nn.Sequential(*model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperparameter combinations: 108\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "hidden_sizes =  [64, 128, 256]\n",
    "nums_epochs = [200]\n",
    "depth = [3, 4, 5]\n",
    "batch_sizes = [32, 64, 128]\n",
    "learning_rate = [0.1, 0.01]\n",
    "step_size_lr_decay = [10, 20]\n",
    "momentum = [0.9]\n",
    "dropout_prob = 0.3\n",
    "patience = 10\n",
    "\n",
    "hyperparameters = itertools.product(hidden_sizes, depth, nums_epochs, batch_sizes, learning_rate, step_size_lr_decay, momentum)\n",
    "n_comb = len(hidden_sizes)*len(depth)*len(nums_epochs)*len(batch_sizes)*len(learning_rate)*len(step_size_lr_decay)*len(momentum)\n",
    "print (f'Number of hyperparameter combinations: {n_comb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, criterion, loader):\n",
    "    loss = 0\n",
    "    y_pred = torch.tensor([]).to(device)\n",
    "    y_true = torch.tensor([]).to(device)\n",
    "    i = 0\n",
    "\n",
    "    for _, (x, y) in enumerate(loader):\n",
    "        i += 1\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        output = model(x)\n",
    "        loss += criterion(output.squeeze(), y)\n",
    "        y_pred = torch.cat((y_pred, output), 0)\n",
    "        y_true = torch.cat((y_true, y), 0)\n",
    "\n",
    "    return loss /i, y_pred.squeeze(), y_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process\n",
    "def train_model(model, criterion, optimizer, scheduler, epochs, data_loader, val_loader, device, writer, log_name):\n",
    "    n_iter = 0\n",
    "\n",
    "    best_model = None\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_since_last_improvement = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        start_epoch = time.time()\n",
    "\n",
    "        for data, targets in data_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(data)\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(y_pred.squeeze(), targets)\n",
    "            optimizer.zero_grad()\n",
    "            writer.add_scalar(\"Loss/train\", loss, n_iter)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            n_iter += 1\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Compute Val Loss\n",
    "        val_loss, _, _ = test_model(model, criterion, val_loader)\n",
    "        writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            epochs_since_last_improvement = 0\n",
    "        elif epochs_since_last_improvement >= patience:\n",
    "            break\n",
    "        else:\n",
    "            epochs_since_last_improvement += 1\n",
    "\n",
    "        print('Epoch [{}/{}] - {:.2f} seconds - val_loss: {:.6f} - patience: {}'.format(epoch+1,\n",
    "              epochs, time.time() - start_epoch, val_loss, epochs_since_last_improvement), end='\\r')\n",
    "\n",
    "    print('\\nTraining ended after {:.2f} seconds - Best val_loss: {:.6f}'.format(time.time() - start, best_val_loss))\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iterations 1/108 - Training with hidden_size=64, depth=3, num_epochs=200, batch=32, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [25/200] - 2.36 seconds - val_loss: 0.020780 - patience: 10\n",
      "Training ended after 58.44 seconds - Best val_loss: 0.018834\n",
      "Model MSE: 0.021877 - Best MSE: 0.021877\n",
      "\n",
      "Iterations 2/108 - Training with hidden_size=64, depth=3, num_epochs=200, batch=32, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [58/200] - 1.99 seconds - val_loss: 0.019234 - patience: 10\n",
      "Training ended after 130.06 seconds - Best val_loss: 0.017551\n",
      "Model MSE: 0.018884 - Best MSE: 0.018884\n",
      "\n",
      "Iterations 3/108 - Training with hidden_size=64, depth=3, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [37/200] - 2.05 seconds - val_loss: 0.022351 - patience: 10\n",
      "Training ended after 80.56 seconds - Best val_loss: 0.020260\n",
      "Model MSE: 0.021634 - Best MSE: 0.018884\n",
      "\n",
      "Iterations 4/108 - Training with hidden_size=64, depth=3, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [52/200] - 2.24 seconds - val_loss: 0.019787 - patience: 10\n",
      "Training ended after 112.26 seconds - Best val_loss: 0.017741\n",
      "Model MSE: 0.019565 - Best MSE: 0.018884\n",
      "\n",
      "Iterations 5/108 - Training with hidden_size=64, depth=3, num_epochs=200, batch=64, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [50/200] - 1.27 seconds - val_loss: 0.019379 - patience: 10\n",
      "Training ended after 56.67 seconds - Best val_loss: 0.017159\n",
      "Model MSE: 0.019230 - Best MSE: 0.018884\n",
      "\n",
      "Iterations 6/108 - Training with hidden_size=64, depth=3, num_epochs=200, batch=64, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [43/200] - 1.05 seconds - val_loss: 0.017709 - patience: 10\n",
      "Training ended after 47.65 seconds - Best val_loss: 0.016811\n",
      "Model MSE: 0.018335 - Best MSE: 0.018335\n",
      "\n",
      "Iterations 7/108 - Training with hidden_size=64, depth=3, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [31/200] - 1.13 seconds - val_loss: 0.030084 - patience: 10\n",
      "Training ended after 35.69 seconds - Best val_loss: 0.028825\n",
      "Model MSE: 0.029487 - Best MSE: 0.018335\n",
      "\n",
      "Iterations 8/108 - Training with hidden_size=64, depth=3, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [68/200] - 1.18 seconds - val_loss: 0.025149 - patience: 10\n",
      "Training ended after 76.53 seconds - Best val_loss: 0.019989\n",
      "Model MSE: 0.021584 - Best MSE: 0.018335\n",
      "\n",
      "Iterations 9/108 - Training with hidden_size=64, depth=3, num_epochs=200, batch=128, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [29/200] - 0.56 seconds - val_loss: 0.023850 - patience: 10\n",
      "Training ended after 17.80 seconds - Best val_loss: 0.019948\n",
      "Model MSE: 0.022299 - Best MSE: 0.018335\n",
      "\n",
      "Iterations 10/108 - Training with hidden_size=64, depth=3, num_epochs=200, batch=128, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [51/200] - 0.51 seconds - val_loss: 0.022217 - patience: 10\n",
      "Training ended after 27.75 seconds - Best val_loss: 0.020475\n",
      "Model MSE: 0.021947 - Best MSE: 0.018335\n",
      "\n",
      "Iterations 11/108 - Training with hidden_size=64, depth=3, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [24/200] - 0.58 seconds - val_loss: 0.035909 - patience: 10\n",
      "Training ended after 13.38 seconds - Best val_loss: 0.033003\n",
      "Model MSE: 0.038803 - Best MSE: 0.018335\n",
      "\n",
      "Iterations 12/108 - Training with hidden_size=64, depth=3, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [30/200] - 0.52 seconds - val_loss: 0.029526 - patience: 10\n",
      "Training ended after 16.95 seconds - Best val_loss: 0.029201\n",
      "Model MSE: 0.033608 - Best MSE: 0.018335\n",
      "\n",
      "Iterations 13/108 - Training with hidden_size=64, depth=4, num_epochs=200, batch=32, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [21/200] - 2.73 seconds - val_loss: 0.026673 - patience: 10\n",
      "Training ended after 54.86 seconds - Best val_loss: 0.025813\n",
      "Model MSE: 0.029413 - Best MSE: 0.018335\n",
      "\n",
      "Iterations 14/108 - Training with hidden_size=64, depth=4, num_epochs=200, batch=32, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [27/200] - 2.29 seconds - val_loss: 0.227208 - patience: 10\n",
      "Training ended after 67.46 seconds - Best val_loss: 0.227058\n",
      "Model MSE: 0.222029 - Best MSE: 0.018335\n",
      "\n",
      "Iterations 15/108 - Training with hidden_size=64, depth=4, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [37/200] - 2.35 seconds - val_loss: 0.026730 - patience: 10\n",
      "Training ended after 89.80 seconds - Best val_loss: 0.023803\n",
      "Model MSE: 0.024657 - Best MSE: 0.018335\n",
      "\n",
      "Iterations 16/108 - Training with hidden_size=64, depth=4, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [43/200] - 2.39 seconds - val_loss: 0.021946 - patience: 10\n",
      "Training ended after 104.91 seconds - Best val_loss: 0.019964\n",
      "Model MSE: 0.020247 - Best MSE: 0.018335\n",
      "\n",
      "Iterations 17/108 - Training with hidden_size=64, depth=4, num_epochs=200, batch=64, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [32/200] - 1.20 seconds - val_loss: 0.027967 - patience: 10\n",
      "Training ended after 39.39 seconds - Best val_loss: 0.022187\n",
      "Model MSE: 0.026525 - Best MSE: 0.018335\n",
      "\n",
      "Iterations 18/108 - Training with hidden_size=64, depth=4, num_epochs=200, batch=64, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [49/200] - 1.36 seconds - val_loss: 0.019387 - patience: 10\n",
      "Training ended after 62.74 seconds - Best val_loss: 0.016894\n",
      "Model MSE: 0.018117 - Best MSE: 0.018117\n",
      "\n",
      "Iterations 19/108 - Training with hidden_size=64, depth=4, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [34/200] - 1.18 seconds - val_loss: 0.035943 - patience: 10\n",
      "Training ended after 44.01 seconds - Best val_loss: 0.033172\n",
      "Model MSE: 0.033308 - Best MSE: 0.018117\n",
      "\n",
      "Iterations 20/108 - Training with hidden_size=64, depth=4, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [38/200] - 1.31 seconds - val_loss: 0.030360 - patience: 10\n",
      "Training ended after 49.41 seconds - Best val_loss: 0.027559\n",
      "Model MSE: 0.028945 - Best MSE: 0.018117\n",
      "\n",
      "Iterations 21/108 - Training with hidden_size=64, depth=4, num_epochs=200, batch=128, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [42/200] - 0.66 seconds - val_loss: 0.029199 - patience: 10\n",
      "Training ended after 28.89 seconds - Best val_loss: 0.024914\n",
      "Model MSE: 0.029693 - Best MSE: 0.018117\n",
      "\n",
      "Iterations 22/108 - Training with hidden_size=64, depth=4, num_epochs=200, batch=128, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [36/200] - 0.61 seconds - val_loss: 0.023907 - patience: 10\n",
      "Training ended after 22.43 seconds - Best val_loss: 0.023306\n",
      "Model MSE: 0.024474 - Best MSE: 0.018117\n",
      "\n",
      "Iterations 23/108 - Training with hidden_size=64, depth=4, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [35/200] - 0.72 seconds - val_loss: 0.226105 - patience: 10\n",
      "Training ended after 23.41 seconds - Best val_loss: 0.225528\n",
      "Model MSE: 0.220719 - Best MSE: 0.018117\n",
      "\n",
      "Iterations 24/108 - Training with hidden_size=64, depth=4, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [29/200] - 0.64 seconds - val_loss: 0.035926 - patience: 10\n",
      "Training ended after 19.91 seconds - Best val_loss: 0.034017\n",
      "Model MSE: 0.039306 - Best MSE: 0.018117\n",
      "\n",
      "Iterations 25/108 - Training with hidden_size=64, depth=5, num_epochs=200, batch=32, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [32/200] - 2.70 seconds - val_loss: 0.024023 - patience: 10\n",
      "Training ended after 88.76 seconds - Best val_loss: 0.023310\n",
      "Model MSE: 0.024823 - Best MSE: 0.018117\n",
      "\n",
      "Iterations 26/108 - Training with hidden_size=64, depth=5, num_epochs=200, batch=32, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [40/200] - 2.50 seconds - val_loss: 0.028326 - patience: 10\n",
      "Training ended after 104.34 seconds - Best val_loss: 0.027535\n",
      "Model MSE: 0.030568 - Best MSE: 0.018117\n",
      "\n",
      "Iterations 27/108 - Training with hidden_size=64, depth=5, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [41/200] - 2.51 seconds - val_loss: 0.034716 - patience: 10\n",
      "Training ended after 105.19 seconds - Best val_loss: 0.031291\n",
      "Model MSE: 0.033236 - Best MSE: 0.018117\n",
      "\n",
      "Iterations 28/108 - Training with hidden_size=64, depth=5, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [49/200] - 2.50 seconds - val_loss: 0.027745 - patience: 10\n",
      "Training ended after 125.43 seconds - Best val_loss: 0.023722\n",
      "Model MSE: 0.026185 - Best MSE: 0.018117\n",
      "\n",
      "Iterations 29/108 - Training with hidden_size=64, depth=5, num_epochs=200, batch=64, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [24/200] - 1.29 seconds - val_loss: 0.227087 - patience: 10\n",
      "Training ended after 31.70 seconds - Best val_loss: 0.227050\n",
      "Model MSE: 0.221993 - Best MSE: 0.018117\n",
      "\n",
      "Iterations 30/108 - Training with hidden_size=64, depth=5, num_epochs=200, batch=64, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [15/200] - 1.27 seconds - val_loss: 0.233946 - patience: 10\n",
      "Training ended after 20.56 seconds - Best val_loss: 0.227138\n",
      "Model MSE: 0.222502 - Best MSE: 0.018117\n",
      "\n",
      "Iterations 31/108 - Training with hidden_size=64, depth=5, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [46/200] - 1.28 seconds - val_loss: 0.091678 - patience: 10\n",
      "Training ended after 60.31 seconds - Best val_loss: 0.088251\n",
      "Model MSE: 0.091032 - Best MSE: 0.018117\n",
      "\n",
      "Iterations 32/108 - Training with hidden_size=64, depth=5, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [42/200] - 1.26 seconds - val_loss: 0.028325 - patience: 10\n",
      "Training ended after 54.85 seconds - Best val_loss: 0.027399\n",
      "Model MSE: 0.029546 - Best MSE: 0.018117\n",
      "\n",
      "Iterations 33/108 - Training with hidden_size=64, depth=5, num_epochs=200, batch=128, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [25/200] - 0.66 seconds - val_loss: 0.227070 - patience: 10\n",
      "Training ended after 16.89 seconds - Best val_loss: 0.227057\n",
      "Model MSE: 0.222063 - Best MSE: 0.018117\n",
      "\n",
      "Iterations 34/108 - Training with hidden_size=64, depth=5, num_epochs=200, batch=128, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [88/200] - 0.65 seconds - val_loss: 0.061383 - patience: 10\n",
      "Training ended after 58.12 seconds - Best val_loss: 0.053521\n",
      "Model MSE: 0.060212 - Best MSE: 0.018117\n",
      "\n",
      "Iterations 35/108 - Training with hidden_size=64, depth=5, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [33/200] - 0.66 seconds - val_loss: 0.069935 - patience: 10\n",
      "Training ended after 22.21 seconds - Best val_loss: 0.068538\n",
      "Model MSE: 0.070841 - Best MSE: 0.018117\n",
      "\n",
      "Iterations 36/108 - Training with hidden_size=64, depth=5, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [36/200] - 0.65 seconds - val_loss: 0.226945 - patience: 10\n",
      "Training ended after 24.25 seconds - Best val_loss: 0.226772\n",
      "Model MSE: 0.221852 - Best MSE: 0.018117\n",
      "\n",
      "Iterations 37/108 - Training with hidden_size=128, depth=3, num_epochs=200, batch=32, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [42/200] - 2.37 seconds - val_loss: 0.013861 - patience: 10\n",
      "Training ended after 96.22 seconds - Best val_loss: 0.013151\n",
      "Model MSE: 0.014459 - Best MSE: 0.014459\n",
      "\n",
      "Iterations 38/108 - Training with hidden_size=128, depth=3, num_epochs=200, batch=32, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [45/200] - 2.29 seconds - val_loss: 0.013088 - patience: 10\n",
      "Training ended after 106.82 seconds - Best val_loss: 0.012497\n",
      "Model MSE: 0.013561 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 39/108 - Training with hidden_size=128, depth=3, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [34/200] - 2.24 seconds - val_loss: 0.016357 - patience: 10\n",
      "Training ended after 80.57 seconds - Best val_loss: 0.015297\n",
      "Model MSE: 0.015183 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 40/108 - Training with hidden_size=128, depth=3, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [42/200] - 2.25 seconds - val_loss: 0.013547 - patience: 10\n",
      "Training ended after 99.42 seconds - Best val_loss: 0.012896\n",
      "Model MSE: 0.013965 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 41/108 - Training with hidden_size=128, depth=3, num_epochs=200, batch=64, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [29/200] - 0.98 seconds - val_loss: 0.016223 - patience: 10\n",
      "Training ended after 29.64 seconds - Best val_loss: 0.013860\n",
      "Model MSE: 0.014737 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 42/108 - Training with hidden_size=128, depth=3, num_epochs=200, batch=64, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [31/200] - 0.98 seconds - val_loss: 0.014291 - patience: 10\n",
      "Training ended after 31.55 seconds - Best val_loss: 0.013638\n",
      "Model MSE: 0.014875 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 43/108 - Training with hidden_size=128, depth=3, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [33/200] - 0.98 seconds - val_loss: 0.023039 - patience: 10\n",
      "Training ended after 33.48 seconds - Best val_loss: 0.020635\n",
      "Model MSE: 0.021891 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 44/108 - Training with hidden_size=128, depth=3, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [60/200] - 0.98 seconds - val_loss: 0.015854 - patience: 10\n",
      "Training ended after 60.00 seconds - Best val_loss: 0.014846\n",
      "Model MSE: 0.016746 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 45/108 - Training with hidden_size=128, depth=3, num_epochs=200, batch=128, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [27/200] - 0.50 seconds - val_loss: 0.017093 - patience: 10\n",
      "Training ended after 14.37 seconds - Best val_loss: 0.015796\n",
      "Model MSE: 0.016696 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 46/108 - Training with hidden_size=128, depth=3, num_epochs=200, batch=128, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [31/200] - 0.51 seconds - val_loss: 0.014444 - patience: 10\n",
      "Training ended after 16.35 seconds - Best val_loss: 0.014216\n",
      "Model MSE: 0.015067 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 47/108 - Training with hidden_size=128, depth=3, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [31/200] - 0.52 seconds - val_loss: 0.031357 - patience: 10\n",
      "Training ended after 16.30 seconds - Best val_loss: 0.026383\n",
      "Model MSE: 0.029498 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 48/108 - Training with hidden_size=128, depth=3, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [43/200] - 0.51 seconds - val_loss: 0.022348 - patience: 10\n",
      "Training ended after 22.40 seconds - Best val_loss: 0.019013\n",
      "Model MSE: 0.021401 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 49/108 - Training with hidden_size=128, depth=4, num_epochs=200, batch=32, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [34/200] - 2.35 seconds - val_loss: 0.015188 - patience: 10\n",
      "Training ended after 80.76 seconds - Best val_loss: 0.013892\n",
      "Model MSE: 0.015871 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 50/108 - Training with hidden_size=128, depth=4, num_epochs=200, batch=32, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [78/200] - 2.33 seconds - val_loss: 0.015160 - patience: 10\n",
      "Training ended after 184.76 seconds - Best val_loss: 0.013460\n",
      "Model MSE: 0.013912 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 51/108 - Training with hidden_size=128, depth=4, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [38/200] - 2.37 seconds - val_loss: 0.018888 - patience: 10\n",
      "Training ended after 91.12 seconds - Best val_loss: 0.018030\n",
      "Model MSE: 0.019932 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 52/108 - Training with hidden_size=128, depth=4, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [39/200] - 2.35 seconds - val_loss: 0.015162 - patience: 10\n",
      "Training ended after 93.81 seconds - Best val_loss: 0.014112\n",
      "Model MSE: 0.015417 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 53/108 - Training with hidden_size=128, depth=4, num_epochs=200, batch=64, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [36/200] - 1.11 seconds - val_loss: 0.018626 - patience: 10\n",
      "Training ended after 41.16 seconds - Best val_loss: 0.017086\n",
      "Model MSE: 0.017867 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 54/108 - Training with hidden_size=128, depth=4, num_epochs=200, batch=64, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [31/200] - 1.10 seconds - val_loss: 0.015338 - patience: 10\n",
      "Training ended after 35.80 seconds - Best val_loss: 0.014550\n",
      "Model MSE: 0.015376 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 55/108 - Training with hidden_size=128, depth=4, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [45/200] - 1.09 seconds - val_loss: 0.029960 - patience: 10\n",
      "Training ended after 50.56 seconds - Best val_loss: 0.028534\n",
      "Model MSE: 0.030370 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 56/108 - Training with hidden_size=128, depth=4, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [33/200] - 1.11 seconds - val_loss: 0.018164 - patience: 10\n",
      "Training ended after 37.48 seconds - Best val_loss: 0.018004\n",
      "Model MSE: 0.019056 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 57/108 - Training with hidden_size=128, depth=4, num_epochs=200, batch=128, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [46/200] - 0.57 seconds - val_loss: 0.021395 - patience: 10\n",
      "Training ended after 26.93 seconds - Best val_loss: 0.019560\n",
      "Model MSE: 0.022775 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 58/108 - Training with hidden_size=128, depth=4, num_epochs=200, batch=128, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [35/200] - 0.58 seconds - val_loss: 0.016952 - patience: 10\n",
      "Training ended after 20.67 seconds - Best val_loss: 0.014565\n",
      "Model MSE: 0.016744 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 59/108 - Training with hidden_size=128, depth=4, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [46/200] - 0.58 seconds - val_loss: 0.059784 - patience: 10\n",
      "Training ended after 27.00 seconds - Best val_loss: 0.058114\n",
      "Model MSE: 0.060748 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 60/108 - Training with hidden_size=128, depth=4, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [45/200] - 0.57 seconds - val_loss: 0.026768 - patience: 10\n",
      "Training ended after 26.41 seconds - Best val_loss: 0.025435\n",
      "Model MSE: 0.026299 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 61/108 - Training with hidden_size=128, depth=5, num_epochs=200, batch=32, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [36/200] - 2.43 seconds - val_loss: 0.227074 - patience: 10\n",
      "Training ended after 89.80 seconds - Best val_loss: 0.227059\n",
      "Model MSE: 0.222008 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 62/108 - Training with hidden_size=128, depth=5, num_epochs=200, batch=32, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [28/200] - 2.43 seconds - val_loss: 0.227583 - patience: 10\n",
      "Training ended after 70.42 seconds - Best val_loss: 0.227059\n",
      "Model MSE: 0.222123 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 63/108 - Training with hidden_size=128, depth=5, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [33/200] - 2.42 seconds - val_loss: 0.030124 - patience: 10\n",
      "Training ended after 82.55 seconds - Best val_loss: 0.023092\n",
      "Model MSE: 0.025641 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 64/108 - Training with hidden_size=128, depth=5, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [60/200] - 2.42 seconds - val_loss: 0.017874 - patience: 10\n",
      "Training ended after 148.12 seconds - Best val_loss: 0.015535\n",
      "Model MSE: 0.018006 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 65/108 - Training with hidden_size=128, depth=5, num_epochs=200, batch=64, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [41/200] - 1.32 seconds - val_loss: 0.045625 - patience: 10\n",
      "Training ended after 52.05 seconds - Best val_loss: 0.041538\n",
      "Model MSE: 0.044249 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 66/108 - Training with hidden_size=128, depth=5, num_epochs=200, batch=64, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [14/200] - 1.24 seconds - val_loss: 0.228217 - patience: 10\n",
      "Training ended after 18.63 seconds - Best val_loss: 0.227143\n",
      "Model MSE: 0.222516 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 67/108 - Training with hidden_size=128, depth=5, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [38/200] - 1.23 seconds - val_loss: 0.035907 - patience: 10\n",
      "Training ended after 48.25 seconds - Best val_loss: 0.032736\n",
      "Model MSE: 0.033007 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 68/108 - Training with hidden_size=128, depth=5, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [32/200] - 1.25 seconds - val_loss: 0.025068 - patience: 10\n",
      "Training ended after 41.02 seconds - Best val_loss: 0.024039\n",
      "Model MSE: 0.024771 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 69/108 - Training with hidden_size=128, depth=5, num_epochs=200, batch=128, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [17/200] - 0.64 seconds - val_loss: 0.227137 - patience: 10\n",
      "Training ended after 11.73 seconds - Best val_loss: 0.227065\n",
      "Model MSE: 0.221964 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 70/108 - Training with hidden_size=128, depth=5, num_epochs=200, batch=128, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [44/200] - 0.64 seconds - val_loss: 0.023051 - patience: 10\n",
      "Training ended after 28.96 seconds - Best val_loss: 0.018079\n",
      "Model MSE: 0.019689 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 71/108 - Training with hidden_size=128, depth=5, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [48/200] - 0.64 seconds - val_loss: 0.225444 - patience: 10\n",
      "Training ended after 31.41 seconds - Best val_loss: 0.224998\n",
      "Model MSE: 0.220653 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 72/108 - Training with hidden_size=128, depth=5, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [50/200] - 0.62 seconds - val_loss: 0.046849 - patience: 10\n",
      "Training ended after 32.87 seconds - Best val_loss: 0.042855\n",
      "Model MSE: 0.045603 - Best MSE: 0.013561\n",
      "\n",
      "Iterations 73/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=32, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [38/200] - 1.90 seconds - val_loss: 0.010552 - patience: 10\n",
      "Training ended after 75.04 seconds - Best val_loss: 0.010360\n",
      "Model MSE: 0.011580 - Best MSE: 0.011580\n",
      "\n",
      "Iterations 74/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=32, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [49/200] - 1.92 seconds - val_loss: 0.010723 - patience: 10\n",
      "Training ended after 96.56 seconds - Best val_loss: 0.009553\n",
      "Model MSE: 0.010168 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 75/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [22/200] - 1.93 seconds - val_loss: 0.014419 - patience: 10\n",
      "Training ended after 44.22 seconds - Best val_loss: 0.012551\n",
      "Model MSE: 0.013343 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 76/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [39/200] - 1.92 seconds - val_loss: 0.011326 - patience: 10\n",
      "Training ended after 76.78 seconds - Best val_loss: 0.010524\n",
      "Model MSE: 0.011017 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 77/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=64, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [34/200] - 0.99 seconds - val_loss: 0.013681 - patience: 10\n",
      "Training ended after 34.71 seconds - Best val_loss: 0.011705\n",
      "Model MSE: 0.012185 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 78/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=64, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [44/200] - 1.01 seconds - val_loss: 0.010525 - patience: 10\n",
      "Training ended after 44.64 seconds - Best val_loss: 0.009608\n",
      "Model MSE: 0.010759 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 79/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [41/200] - 0.98 seconds - val_loss: 0.017117 - patience: 10\n",
      "Training ended after 41.42 seconds - Best val_loss: 0.014996\n",
      "Model MSE: 0.016951 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 80/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [34/200] - 0.99 seconds - val_loss: 0.013951 - patience: 10\n",
      "Training ended after 34.53 seconds - Best val_loss: 0.012594\n",
      "Model MSE: 0.013910 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 81/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=128, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [57/200] - 0.51 seconds - val_loss: 0.015115 - patience: 10\n",
      "Training ended after 30.01 seconds - Best val_loss: 0.012952\n",
      "Model MSE: 0.014823 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 82/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=128, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [49/200] - 0.51 seconds - val_loss: 0.012351 - patience: 10\n",
      "Training ended after 25.68 seconds - Best val_loss: 0.010951\n",
      "Model MSE: 0.012069 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 83/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [33/200] - 0.52 seconds - val_loss: 0.025045 - patience: 10\n",
      "Training ended after 17.69 seconds - Best val_loss: 0.023176\n",
      "Model MSE: 0.024381 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 84/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [52/200] - 0.51 seconds - val_loss: 0.017610 - patience: 10\n",
      "Training ended after 27.40 seconds - Best val_loss: 0.016500\n",
      "Model MSE: 0.017299 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 85/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=32, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [24/200] - 2.24 seconds - val_loss: 0.011672 - patience: 10\n",
      "Training ended after 56.16 seconds - Best val_loss: 0.010581\n",
      "Model MSE: 0.011428 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 86/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=32, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [52/200] - 2.23 seconds - val_loss: 0.010782 - patience: 10\n",
      "Training ended after 118.56 seconds - Best val_loss: 0.009338\n",
      "Model MSE: 0.010593 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 87/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [31/200] - 2.21 seconds - val_loss: 0.014400 - patience: 10\n",
      "Training ended after 71.27 seconds - Best val_loss: 0.014231\n",
      "Model MSE: 0.015022 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 88/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [45/200] - 2.21 seconds - val_loss: 0.013163 - patience: 10\n",
      "Training ended after 102.40 seconds - Best val_loss: 0.011603\n",
      "Model MSE: 0.012331 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 89/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=64, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [45/200] - 1.14 seconds - val_loss: 0.013201 - patience: 10\n",
      "Training ended after 51.98 seconds - Best val_loss: 0.011602\n",
      "Model MSE: 0.013555 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 90/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=64, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [33/200] - 1.12 seconds - val_loss: 0.011654 - patience: 10\n",
      "Training ended after 38.27 seconds - Best val_loss: 0.011162\n",
      "Model MSE: 0.011213 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 91/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [28/200] - 1.12 seconds - val_loss: 0.019666 - patience: 10\n",
      "Training ended after 32.64 seconds - Best val_loss: 0.018227\n",
      "Model MSE: 0.020360 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 92/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [36/200] - 1.13 seconds - val_loss: 0.015127 - patience: 10\n",
      "Training ended after 41.87 seconds - Best val_loss: 0.013702\n",
      "Model MSE: 0.015083 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 93/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=128, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [61/200] - 0.58 seconds - val_loss: 0.015308 - patience: 10\n",
      "Training ended after 36.14 seconds - Best val_loss: 0.012830\n",
      "Model MSE: 0.014861 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 94/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=128, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [66/200] - 0.58 seconds - val_loss: 0.013393 - patience: 10\n",
      "Training ended after 39.22 seconds - Best val_loss: 0.011678\n",
      "Model MSE: 0.013427 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 95/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [40/200] - 0.59 seconds - val_loss: 0.038535 - patience: 10\n",
      "Training ended after 23.93 seconds - Best val_loss: 0.034520\n",
      "Model MSE: 0.035029 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 96/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [41/200] - 0.58 seconds - val_loss: 0.020054 - patience: 10\n",
      "Training ended after 24.63 seconds - Best val_loss: 0.019937\n",
      "Model MSE: 0.020951 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 97/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=32, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [28/200] - 2.47 seconds - val_loss: 0.013734 - patience: 10\n",
      "Training ended after 71.33 seconds - Best val_loss: 0.012370\n",
      "Model MSE: 0.013747 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 98/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=32, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [21/200] - 2.39 seconds - val_loss: 0.228335 - patience: 10\n",
      "Training ended after 53.74 seconds - Best val_loss: 0.227067\n",
      "Model MSE: 0.222215 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 99/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [38/200] - 2.45 seconds - val_loss: 0.016579 - patience: 10\n",
      "Training ended after 95.75 seconds - Best val_loss: 0.014476\n",
      "Model MSE: 0.015648 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 100/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [37/200] - 2.46 seconds - val_loss: 0.012920 - patience: 10\n",
      "Training ended after 93.29 seconds - Best val_loss: 0.012204\n",
      "Model MSE: 0.013877 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 101/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=64, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [26/200] - 1.28 seconds - val_loss: 0.012487 - patience: 10\n",
      "Training ended after 34.28 seconds - Best val_loss: 0.012112\n",
      "Model MSE: 0.013371 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 102/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=64, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [47/200] - 1.27 seconds - val_loss: 0.011435 - patience: 10\n",
      "Training ended after 60.87 seconds - Best val_loss: 0.010211\n",
      "Model MSE: 0.012044 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 103/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [48/200] - 1.28 seconds - val_loss: 0.027281 - patience: 10\n",
      "Training ended after 62.07 seconds - Best val_loss: 0.026056\n",
      "Model MSE: 0.027946 - Best MSE: 0.010168\n",
      "\n",
      "Iterations 104/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 105/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=128, lr=0.1, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 106/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=128, lr=0.1, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 107/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 108/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n"
     ]
    }
   ],
   "source": [
    "current_iter = 0\n",
    "\n",
    "best_model = None\n",
    "best_mse = float('inf')\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "if os.path.exists('best_model.pth'):\n",
    "    # read best model config\n",
    "    with open('best_model_config.json', 'r') as f:\n",
    "        best_model_config = json.load(f)\n",
    "\n",
    "    # load best model\n",
    "    best_model = get_model(X_train.shape[1], best_model_config['hidden_size'], dropout_prob, best_model_config['depth'])\n",
    "    best_model.load_state_dict(torch.load('best_model.pth'))\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "\n",
    "    # evaluate best model\n",
    "    best_mse, _, _ = test_model(best_model, criterion, test_loader)\n",
    "    \n",
    "    print(\"Best model - MSE: {:.6f}\".format(best_mse))\n",
    "\n",
    "for hidden_size, depth, num_epochs, batch, lr, step_size, momentum in hyperparameters:\n",
    "    current_iter += 1\n",
    "\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "\n",
    "    print(\"\\nIterations {}/{} - Training with hidden_size={}, depth={}, num_epochs={}, batch={}, lr={}, step_size={}, momentum={}\".format(\n",
    "        current_iter, n_comb, hidden_size, depth, num_epochs, batch, lr, step_size, momentum))\n",
    "    log_name = \"dim:\"+str(hidden_size)+\"_depth:\"+str(depth)+\"_epochs:\"+str(num_epochs)+\"_batch:\" + \\\n",
    "        str(batch)+\"_lr:\"+str(lr)+\"_step_size:\" + \\\n",
    "        str(step_size)+\"_momentum:\"+str(momentum)\n",
    "\n",
    "    if os.path.exists('runs/'+log_name):\n",
    "        print(\"Model already trained, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # start tensorboard\n",
    "    writer = SummaryWriter('runs/'+log_name)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(Y_train, dtype=torch.float32)), batch_size=batch, shuffle=True)\n",
    "\n",
    "    model = get_model(X_train.shape[1], hidden_size, dropout_prob, depth=depth)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)\n",
    "\n",
    "    # train\n",
    "    model = train_model(model, criterion, optimizer, scheduler, num_epochs, train_loader, val_loader, device, writer, log_name)\n",
    "\n",
    "    # validate model on test set\n",
    "    mse, _, _ = test_model(model, criterion, test_loader)\n",
    "    writer.add_hparams({'hidden_size': hidden_size, 'depth': depth, 'num_epochs': num_epochs, 'batch': batch,'lr': lr, 'step_size': step_size, 'momentum': momentum}, {'hparam/mse': mse})\n",
    "\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_model = model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        # save config\n",
    "        with open('best_model_config.json', 'w') as f:\n",
    "            json.dump({'hidden_size': hidden_size, 'depth': depth, 'num_epochs': num_epochs, 'batch': batch,\n",
    "                       'lr': lr, 'step_size': step_size}, f)\n",
    "            \n",
    "            \n",
    "    print(\"Model MSE: {:.6f} - Best MSE: {:.6f}\".format(mse, best_mse))\n",
    "\n",
    "    torch.save(model.state_dict(), 'models/'+log_name+'.pth')\n",
    "    writer.flush()\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_loss, y_pred, y_true \u001b[39m=\u001b[39m test_model(best_model, criterion, test_loader)\n\u001b[1;32m      3\u001b[0m y_pred \u001b[39m=\u001b[39m y_pred\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m      4\u001b[0m y_true \u001b[39m=\u001b[39m y_true\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_model' is not defined"
     ]
    }
   ],
   "source": [
    "test_loss, y_pred, y_true = test_model(best_model, criterion, test_loader)\n",
    "\n",
    "y_pred = y_pred.cpu().detach().numpy()\n",
    "y_true = y_true.cpu().detach().numpy()\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(\"MSE: {:.6f} - R2: {:.6f}\".format(test_loss, r2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TabNetClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Define the TabNet model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tabnet \u001b[39m=\u001b[39m TabNetClassifier(\n\u001b[1;32m      3\u001b[0m     \u001b[39m# n_d: the dimensionality of the output space of the feature transformer network (default 64)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     n_d\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[39m# n_a: the dimensionality of the output space of the attention network (default 64)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     n_a\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[39m# n_steps: the number of sequential steps in the attention mechanism (default 3)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     n_steps\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[39m# n_steps: the number of sequential steps in the attention mechanism (default 3)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     gamma\u001b[39m=\u001b[39m\u001b[39m1.5\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[39m# n_independent: the number of independent feature transformer networks to use (default 2)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     n_independent\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     13\u001b[0m     \u001b[39m# n_shared: the number of shared feature transformer networks to use (default 2)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     n_shared\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[39m# epsilon: a small value to add to the denominator of the feature importance calculation to avoid division by zero (default 1e-15)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     epsilon\u001b[39m=\u001b[39m\u001b[39m1e-15\u001b[39m,\n\u001b[1;32m     17\u001b[0m     seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m,  \u001b[39m# seed: the random seed to use for reproducibility (default None)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m     21\u001b[0m tabnet\u001b[39m.\u001b[39mfit(\n\u001b[1;32m     22\u001b[0m     X_train\u001b[39m=\u001b[39mX_train,\n\u001b[1;32m     23\u001b[0m     y_train\u001b[39m=\u001b[39mY_train,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     max_epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[1;32m     37\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TabNetClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the TabNet model\n",
    "tabnet = TabNetClassifier(\n",
    "    # n_d: the dimensionality of the output space of the feature transformer network (default 64)\n",
    "    n_d=16,\n",
    "    # n_a: the dimensionality of the output space of the attention network (default 64)\n",
    "    n_a=16,\n",
    "    # n_steps: the number of sequential steps in the attention mechanism (default 3)\n",
    "    n_steps=4,\n",
    "    # n_steps: the number of sequential steps in the attention mechanism (default 3)\n",
    "    gamma=1.5,\n",
    "    # n_independent: the number of independent feature transformer networks to use (default 2)\n",
    "    n_independent=2,\n",
    "    # n_shared: the number of shared feature transformer networks to use (default 2)\n",
    "    n_shared=2,\n",
    "    # epsilon: a small value to add to the denominator of the feature importance calculation to avoid division by zero (default 1e-15)\n",
    "    epsilon=1e-15,\n",
    "    seed=42,  # seed: the random seed to use for reproducibility (default None)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "tabnet.fit(\n",
    "    X_train=X_train,\n",
    "    y_train=Y_train,\n",
    "    eval_set=[(X_val, Y_val)],\n",
    "    # patience: the number of epochs to wait without improvement in validation loss before early stopping (default 10)\n",
    "    patience=10,\n",
    "    # batch_size: the number of samples per batch (default 1024)\n",
    "    batch_size=1024,\n",
    "    # virtual_batch_size: the number of samples per virtual batch (default 128)\n",
    "    virtual_batch_size=128,\n",
    "    # num_workers: the number of worker processes to use for data loading (default 0)\n",
    "    num_workers=0,\n",
    "    # drop_last: whether to drop the last incomplete batch if the dataset size is not divisible by the batch size (default False)\n",
    "    drop_last=False,\n",
    "    # max_epochs: the maximum number of epochs to train for (default 100)\n",
    "    max_epochs=100,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.67      0.22      0.33         9\n",
      "           2       0.82      0.53      0.64        80\n",
      "           3       0.79      0.59      0.68       284\n",
      "           4       0.78      0.89      0.83       804\n",
      "           5       0.84      0.90      0.87      1159\n",
      "           6       0.90      0.71      0.79       426\n",
      "           7       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.82      2764\n",
      "   macro avg       0.60      0.48      0.52      2764\n",
      "weighted avg       0.82      0.82      0.82      2764\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "y_pred = tabnet.predict(X_test)\n",
    "print(classification_report(Y_test, y_pred))\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
