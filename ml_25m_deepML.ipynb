{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML_25M Deep ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "import itertools\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "# from tab_transformer_pytorch import TabTransformer\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "# from pytorch_tabular import TabularModel\n",
    "# from pytorch_tabular.models import CategoryEmbeddingModelConfig\n",
    "# from pytorch_tabular.config import (\n",
    "#     DataConfig,\n",
    "#     OptimizerConfig,\n",
    "#     TrainerConfig,\n",
    "# )\n",
    "\n",
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "\n",
    "    Args:\n",
    "        seed: the seed to use. \n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "\n",
    "\n",
    "fix_random(42)\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# PyTorch Device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using device:', device)\n",
    "\n",
    "\n",
    "delete_previous = True\n",
    "\n",
    "if delete_previous:\n",
    "    try:\n",
    "        os.system('rm -rf runs')\n",
    "        os.system('rm -rf models')\n",
    "        os.system('rm -rf best_model.pth')\n",
    "        os.system('rm -rf best_model_config.json')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 9946\n",
      "Number of validation samples: 1106\n",
      "Number of testing samples: 2764\n",
      "\n",
      "Number of features: 1128\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('rating', axis=1)\n",
    "Y = df['rating']\n",
    "\n",
    "# encode Y\n",
    "Y = LabelEncoder().fit_transform(Y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "\n",
    "# PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# pca.fit(X_train)\n",
    "# X_train = pca.transform(X_train)\n",
    "# X_test = pca.transform(X_test)\n",
    "\n",
    "# LDA\n",
    "# lda = LinearDiscriminantAnalysis()\n",
    "# lda.fit(X_train, Y_train)\n",
    "# X_train = lda.transform(X_train)\n",
    "# X_val = lda.transform(X_val)\n",
    "# X_test = lda.transform(X_test)\n",
    "\n",
    "# # # smote\n",
    "# sm = SMOTE(random_state=42)\n",
    "# X_train, Y_train = sm.fit_resample(X_train, Y_train)\n",
    "\n",
    "\n",
    "print(f'Number of training samples: {X_train.shape[0]}')\n",
    "print(f'Number of validation samples: {X_val.shape[0]}')\n",
    "print(f'Number of testing samples: {X_test.shape[0]}')\n",
    "print(f'\\nNumber of features: {X_train.shape[1]}')\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(Y_val, dtype=torch.long)), batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(Y_test, dtype=torch.long)), batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['Model', 'Accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_size, hidden_size, dropout_prob=0, depth=1):\n",
    "    model = [\n",
    "        torch.nn.Linear(input_size, hidden_size),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(dropout_prob)\n",
    "    ]\n",
    "\n",
    "    for i in range(depth):\n",
    "        model.append(torch.nn.Linear(hidden_size, hidden_size))\n",
    "        model.append(torch.nn.ReLU())\n",
    "        model.append(torch.nn.Dropout(dropout_prob))\n",
    "\n",
    "    model.append(torch.nn.Linear(hidden_size, 1))\n",
    "\n",
    "    return torch.nn.Sequential(*model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperparameter combinations: 144\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "hidden_sizes = [32, 64, 128]\n",
    "nums_epochs = [200]\n",
    "depth = [3, 4, 5]\n",
    "batch_sizes = [16, 32, 64, 128]\n",
    "learning_rate = [0.1, 0.01]\n",
    "step_size_lr_decay = [10, 20]\n",
    "momentum = [0.9]\n",
    "dropout_prob = 0.3\n",
    "patience = 10\n",
    "\n",
    "hyperparameters = itertools.product(hidden_sizes, depth, nums_epochs, batch_sizes, learning_rate, step_size_lr_decay, momentum)\n",
    "n_comb = len(hidden_sizes)*len(depth)*len(nums_epochs)*len(batch_sizes)*len(learning_rate)*len(step_size_lr_decay)*len(momentum)\n",
    "print (f'Number of hyperparameter combinations: {n_comb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process\n",
    "def train_model(model, criterion, optimizer, scheduler, epochs, data_loader, val_loader, device, writer):\n",
    "    n_iter = 0\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_since_last_improvement = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        start_epoch = time.time()\n",
    "\n",
    "        for data, targets in data_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(data)\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(y_pred, targets)\n",
    "            optimizer.zero_grad()\n",
    "            writer.add_scalar(\"Loss/train\", loss, n_iter)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            n_iter += 1\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Compute Val Loss\n",
    "        labels, _, y_pred = test_model(model, val_loader, device)\n",
    "        val_loss = criterion(y_pred, labels).item()\n",
    "        writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'models/best_model.pth')\n",
    "            epochs_since_last_improvement = 0\n",
    "        elif epochs_since_last_improvement >= patience:\n",
    "            break\n",
    "        else:\n",
    "            epochs_since_last_improvement += 1\n",
    "\n",
    "        print('Epoch [{}/{}] - {:.2f} seconds - loss: {:.6f} - val_loss: {:.6f} - patience: {}'.format(epoch+1,\n",
    "              epochs, time.time() - start_epoch, loss.item(), val_loss.item(), epochs_since_last_improvement), end='\\r')\n",
    "\n",
    "    print('\\nTraining ended after {:.2f} seconds'.format(time.time() - start))\n",
    "\n",
    "    # Restore best model\n",
    "    model.load_state_dict(torch.load('models/best_model.pth'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation process\n",
    "def test_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in data_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            y_pred.append(model(data))\n",
    "            y_test.append(targets)\n",
    "\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    y_test = torch.cat(y_test, dim=0)\n",
    "\n",
    "    y_pred_c = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "    return y_test, y_pred_c, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iterations 1/144 - Training with hidden_size=32, depth=3, num_epochs=200, batch=16, lr=0.1, step_size=10, momentum=0.9\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "current_iter = 0\n",
    "\n",
    "best_model = None\n",
    "best_mse = float('inf')\n",
    "\n",
    "\n",
    "if os.path.exists('best_model.pth'):\n",
    "    # read best model config\n",
    "    with open('best_model_config.json', 'r') as f:\n",
    "        best_model_config = json.load(f)\n",
    "\n",
    "    # load best model\n",
    "    best_model = get_model(X_train.shape[1], best_model_config['hidden_size'], dropout_prob, best_model_config['depth'])\n",
    "    best_model.load_state_dict(torch.load('best_model.pth'))\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "\n",
    "    # evaluate best model\n",
    "    y_test, y_pred_c, y_pred = test_model(best_model, test_loader, device)\n",
    "\n",
    "    best_mse = torch.nn.MSELoss()(y_pred, y_test)\n",
    "    best_r2 = r2_score(y_test.cpu().numpy(), y_pred.cpu().numpy())\n",
    "    \n",
    "    print(\"Best model - MSE: {:.6f}\".format(best_mse))\n",
    "\n",
    "for hidden_size, depth, num_epochs, batch, lr, step_size, momentum in hyperparameters:\n",
    "    current_iter += 1\n",
    "\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "\n",
    "    print(\"\\nIterations {}/{} - Training with hidden_size={}, depth={}, num_epochs={}, batch={}, lr={}, step_size={}, momentum={}\".format(\n",
    "        current_iter, n_comb, hidden_size, depth, num_epochs, batch, lr, step_size, momentum))\n",
    "    log_name = \"dim:\"+str(hidden_size)+\"_depth:\"+str(depth)+\"_epochs:\"+str(num_epochs)+\"_batch:\" + \\\n",
    "        str(batch)+\"_lr:\"+str(lr)+\"_step_size:\" + \\\n",
    "        str(step_size)+\"_momentum:\"+str(momentum)\n",
    "\n",
    "    if os.path.exists('runs/'+log_name):\n",
    "        print(\"Model already trained, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # start tensorboard\n",
    "    writer = SummaryWriter('runs/'+log_name)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(Y_train, dtype=torch.long)), batch_size=batch, shuffle=True)\n",
    "\n",
    "    model = get_model(X_train.shape[1], hidden_size, dropout_prob, depth=depth)\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)\n",
    "\n",
    "    # train\n",
    "    model = train_model(model, criterion, optimizer, scheduler, num_epochs, train_loader, val_loader, device, writer)\n",
    "\n",
    "    # validate\n",
    "    y_test, y_pred_c, y_pred = test_model(model, test_loader, device)\n",
    "\n",
    "    mse = torch.nn.MSELoss()(y_pred, y_test)\n",
    "\n",
    "    writer.add_hparams({'hidden_size': hidden_size, 'depth': depth, 'num_epochs': num_epochs, 'batch': batch,'lr': lr, 'step_size': step_size, 'momentum': momentum}, {'hparam/mse': mse, 'hparam/r2': r2})\n",
    "\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_model = model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        # save config\n",
    "        with open('best_model_config.json', 'w') as f:\n",
    "            json.dump({'hidden_size': hidden_size, 'depth': depth, 'num_epochs': num_epochs, 'batch': batch,\n",
    "                       'lr': lr, 'step_size': step_size}, f)\n",
    "            \n",
    "            \n",
    "    print(\"Model MSE: {:.6f} - Best MSE: {:.6f}\".format(mse, best_mse))\n",
    "\n",
    "    torch.save(model.state_dict(), 'models/'+log_name+'.pth')\n",
    "    writer.flush()\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.00      0.00      0.00         9\n",
      "           2       0.79      0.90      0.84        80\n",
      "           3       0.80      0.88      0.84       284\n",
      "           4       0.87      0.87      0.87       804\n",
      "           5       0.92      0.80      0.86      1159\n",
      "           6       0.72      0.93      0.81       426\n",
      "           7       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.85      2764\n",
      "   macro avg       0.51      0.55      0.53      2764\n",
      "weighted avg       0.86      0.85      0.85      2764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "best_model.eval()\n",
    "y_test, y_pred_c, y_pred = test_model(best_model, test_loader, device)\n",
    "print(classification_report(y_test.cpu(), y_pred_c.cpu(), zero_division=0))\n",
    "accuracy = accuracy_score(y_test.cpu(), y_pred_c.cpu())\n",
    "\n",
    "results = pd.concat([results, pd.DataFrame({'Model': 'NeuralNetwork', 'Accuracy': accuracy}, index=[0])], ignore_index=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.20964 | val_0_accuracy: 0.42586 |  0:00:02s\n",
      "epoch 1  | loss: 1.61665 | val_0_accuracy: 0.40958 |  0:00:05s\n",
      "epoch 2  | loss: 1.41317 | val_0_accuracy: 0.41139 |  0:00:07s\n",
      "epoch 3  | loss: 1.285   | val_0_accuracy: 0.4141  |  0:00:10s\n",
      "epoch 4  | loss: 1.19069 | val_0_accuracy: 0.4123  |  0:00:13s\n",
      "epoch 5  | loss: 1.09648 | val_0_accuracy: 0.41049 |  0:00:15s\n",
      "epoch 6  | loss: 1.02561 | val_0_accuracy: 0.42586 |  0:00:18s\n",
      "epoch 7  | loss: 0.94954 | val_0_accuracy: 0.43219 |  0:00:20s\n",
      "epoch 8  | loss: 0.916   | val_0_accuracy: 0.42315 |  0:00:23s\n",
      "epoch 9  | loss: 0.88823 | val_0_accuracy: 0.42767 |  0:00:26s\n",
      "epoch 10 | loss: 0.87582 | val_0_accuracy: 0.43128 |  0:00:28s\n",
      "epoch 11 | loss: 0.82223 | val_0_accuracy: 0.51266 |  0:00:31s\n",
      "epoch 12 | loss: 0.8066  | val_0_accuracy: 0.48101 |  0:00:34s\n",
      "epoch 13 | loss: 0.78798 | val_0_accuracy: 0.46564 |  0:00:36s\n",
      "epoch 14 | loss: 0.7668  | val_0_accuracy: 0.45389 |  0:00:39s\n",
      "epoch 15 | loss: 0.75445 | val_0_accuracy: 0.50723 |  0:00:42s\n",
      "epoch 16 | loss: 0.74595 | val_0_accuracy: 0.4792  |  0:00:44s\n",
      "epoch 17 | loss: 0.73674 | val_0_accuracy: 0.50271 |  0:00:47s\n",
      "epoch 18 | loss: 0.73976 | val_0_accuracy: 0.5009  |  0:00:50s\n",
      "epoch 19 | loss: 0.73404 | val_0_accuracy: 0.51356 |  0:00:52s\n",
      "epoch 20 | loss: 0.7336  | val_0_accuracy: 0.48011 |  0:00:55s\n",
      "epoch 21 | loss: 0.72156 | val_0_accuracy: 0.52351 |  0:00:58s\n",
      "epoch 22 | loss: 0.71514 | val_0_accuracy: 0.51175 |  0:01:00s\n",
      "epoch 23 | loss: 0.70543 | val_0_accuracy: 0.56148 |  0:01:03s\n",
      "epoch 24 | loss: 0.70187 | val_0_accuracy: 0.55244 |  0:01:06s\n",
      "epoch 25 | loss: 0.68846 | val_0_accuracy: 0.58861 |  0:01:09s\n",
      "epoch 26 | loss: 0.6751  | val_0_accuracy: 0.61392 |  0:01:11s\n",
      "epoch 27 | loss: 0.66537 | val_0_accuracy: 0.57776 |  0:01:14s\n",
      "epoch 28 | loss: 0.64147 | val_0_accuracy: 0.59946 |  0:01:17s\n",
      "epoch 29 | loss: 0.60743 | val_0_accuracy: 0.60307 |  0:01:19s\n",
      "epoch 30 | loss: 0.59541 | val_0_accuracy: 0.61754 |  0:01:22s\n",
      "epoch 31 | loss: 0.58123 | val_0_accuracy: 0.63291 |  0:01:25s\n",
      "epoch 32 | loss: 0.55541 | val_0_accuracy: 0.63653 |  0:01:27s\n",
      "epoch 33 | loss: 0.53636 | val_0_accuracy: 0.66004 |  0:01:30s\n",
      "epoch 34 | loss: 0.54223 | val_0_accuracy: 0.63472 |  0:01:33s\n",
      "epoch 35 | loss: 0.52382 | val_0_accuracy: 0.6519  |  0:01:35s\n",
      "epoch 36 | loss: 0.50886 | val_0_accuracy: 0.65732 |  0:01:38s\n",
      "epoch 37 | loss: 0.48938 | val_0_accuracy: 0.70253 |  0:01:41s\n",
      "epoch 38 | loss: 0.49099 | val_0_accuracy: 0.74864 |  0:01:44s\n",
      "epoch 39 | loss: 0.48767 | val_0_accuracy: 0.75316 |  0:01:46s\n",
      "epoch 40 | loss: 0.48044 | val_0_accuracy: 0.7387  |  0:01:49s\n",
      "epoch 41 | loss: 0.43791 | val_0_accuracy: 0.71248 |  0:01:52s\n",
      "epoch 42 | loss: 0.41768 | val_0_accuracy: 0.74593 |  0:01:54s\n",
      "epoch 43 | loss: 0.40996 | val_0_accuracy: 0.75859 |  0:01:57s\n",
      "epoch 44 | loss: 0.39328 | val_0_accuracy: 0.77125 |  0:02:00s\n",
      "epoch 45 | loss: 0.40691 | val_0_accuracy: 0.77758 |  0:02:02s\n",
      "epoch 46 | loss: 0.36928 | val_0_accuracy: 0.79566 |  0:02:05s\n",
      "epoch 47 | loss: 0.37343 | val_0_accuracy: 0.7821  |  0:02:08s\n",
      "epoch 48 | loss: 0.37912 | val_0_accuracy: 0.79024 |  0:02:11s\n",
      "epoch 49 | loss: 0.36842 | val_0_accuracy: 0.80289 |  0:02:13s\n",
      "epoch 50 | loss: 0.35866 | val_0_accuracy: 0.78933 |  0:02:16s\n",
      "epoch 51 | loss: 0.33412 | val_0_accuracy: 0.79204 |  0:02:19s\n",
      "epoch 52 | loss: 0.35227 | val_0_accuracy: 0.81555 |  0:02:22s\n",
      "epoch 53 | loss: 0.34649 | val_0_accuracy: 0.80832 |  0:02:24s\n",
      "epoch 54 | loss: 0.31041 | val_0_accuracy: 0.81103 |  0:02:27s\n",
      "epoch 55 | loss: 0.3116  | val_0_accuracy: 0.8255  |  0:02:30s\n",
      "epoch 56 | loss: 0.29704 | val_0_accuracy: 0.82278 |  0:02:32s\n",
      "epoch 57 | loss: 0.28089 | val_0_accuracy: 0.81736 |  0:02:35s\n",
      "epoch 58 | loss: 0.28481 | val_0_accuracy: 0.8255  |  0:02:38s\n",
      "epoch 59 | loss: 0.25331 | val_0_accuracy: 0.83183 |  0:02:40s\n",
      "epoch 60 | loss: 0.25878 | val_0_accuracy: 0.83183 |  0:02:43s\n",
      "epoch 61 | loss: 0.27843 | val_0_accuracy: 0.81465 |  0:02:46s\n",
      "epoch 62 | loss: 0.27093 | val_0_accuracy: 0.81736 |  0:02:49s\n",
      "epoch 63 | loss: 0.23612 | val_0_accuracy: 0.82188 |  0:02:51s\n",
      "epoch 64 | loss: 0.22849 | val_0_accuracy: 0.81465 |  0:02:54s\n",
      "epoch 65 | loss: 0.2169  | val_0_accuracy: 0.82821 |  0:02:57s\n",
      "epoch 66 | loss: 0.19248 | val_0_accuracy: 0.83363 |  0:02:59s\n",
      "epoch 67 | loss: 0.17725 | val_0_accuracy: 0.83002 |  0:03:02s\n",
      "epoch 68 | loss: 0.1859  | val_0_accuracy: 0.81555 |  0:03:05s\n",
      "epoch 69 | loss: 0.23268 | val_0_accuracy: 0.81374 |  0:03:07s\n",
      "epoch 70 | loss: 0.21933 | val_0_accuracy: 0.81103 |  0:03:10s\n",
      "epoch 71 | loss: 0.22726 | val_0_accuracy: 0.82188 |  0:03:13s\n",
      "epoch 72 | loss: 0.1999  | val_0_accuracy: 0.82278 |  0:03:15s\n",
      "epoch 73 | loss: 0.19216 | val_0_accuracy: 0.82459 |  0:03:18s\n",
      "epoch 74 | loss: 0.21343 | val_0_accuracy: 0.81284 |  0:03:21s\n",
      "epoch 75 | loss: 0.17646 | val_0_accuracy: 0.83183 |  0:03:23s\n",
      "epoch 76 | loss: 0.17696 | val_0_accuracy: 0.81465 |  0:03:26s\n",
      "\n",
      "Early stopping occurred at epoch 76 with best_epoch = 66 and best_val_0_accuracy = 0.83363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Define the TabNet model\n",
    "tabnet = TabNetClassifier(\n",
    "    # n_d: the dimensionality of the output space of the feature transformer network (default 64)\n",
    "    n_d=16,\n",
    "    # n_a: the dimensionality of the output space of the attention network (default 64)\n",
    "    n_a=16,\n",
    "    # n_steps: the number of sequential steps in the attention mechanism (default 3)\n",
    "    n_steps=4,\n",
    "    # n_steps: the number of sequential steps in the attention mechanism (default 3)\n",
    "    gamma=1.5,\n",
    "    # n_independent: the number of independent feature transformer networks to use (default 2)\n",
    "    n_independent=2,\n",
    "    # n_shared: the number of shared feature transformer networks to use (default 2)\n",
    "    n_shared=2,\n",
    "    # epsilon: a small value to add to the denominator of the feature importance calculation to avoid division by zero (default 1e-15)\n",
    "    epsilon=1e-15,\n",
    "    seed=42,  # seed: the random seed to use for reproducibility (default None)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "tabnet.fit(\n",
    "    X_train=X_train,\n",
    "    y_train=Y_train,\n",
    "    eval_set=[(X_val, Y_val)],\n",
    "    # patience: the number of epochs to wait without improvement in validation loss before early stopping (default 10)\n",
    "    patience=10,\n",
    "    # batch_size: the number of samples per batch (default 1024)\n",
    "    batch_size=1024,\n",
    "    # virtual_batch_size: the number of samples per virtual batch (default 128)\n",
    "    virtual_batch_size=128,\n",
    "    # num_workers: the number of worker processes to use for data loading (default 0)\n",
    "    num_workers=0,\n",
    "    # drop_last: whether to drop the last incomplete batch if the dataset size is not divisible by the batch size (default False)\n",
    "    drop_last=False,\n",
    "    # max_epochs: the maximum number of epochs to train for (default 100)\n",
    "    max_epochs=100,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.67      0.22      0.33         9\n",
      "           2       0.82      0.53      0.64        80\n",
      "           3       0.79      0.59      0.68       284\n",
      "           4       0.78      0.89      0.83       804\n",
      "           5       0.84      0.90      0.87      1159\n",
      "           6       0.90      0.71      0.79       426\n",
      "           7       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.82      2764\n",
      "   macro avg       0.60      0.48      0.52      2764\n",
      "weighted avg       0.82      0.82      0.82      2764\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "y_pred = tabnet.predict(X_test)\n",
    "print(classification_report(Y_test, y_pred))\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
