{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML_25M Deep ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import itertools\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def fix_random(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "fix_random(42)\n",
    "\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')\n",
    "\n",
    "if not os.path.exists('results/pca'):\n",
    "    os.mkdir('results/pca')\n",
    "\n",
    "if not os.path.exists('results/no_pca'):\n",
    "    os.mkdir('results/no_pca')\n",
    "\n",
    "pca = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# PyTorch Device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_previous = False\n",
    "\n",
    "if delete_previous:\n",
    "    try:\n",
    "        os.system('rm -rf deepML')\n",
    "        os.system('rm -rf models')\n",
    "        os.system('rm -rf best_model.pth')\n",
    "        os.system('rm -rf best_model_config.json')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('rating', axis=1)\n",
    "Y = df['rating']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "Y_train = Y_train.to_numpy()\n",
    "Y_val = Y_val.to_numpy()\n",
    "Y_test = Y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCA\n",
    "# pca = PCA(n_components=0.95)\n",
    "# pca.fit(X_train)\n",
    "# X_train = pca.transform(X_train)\n",
    "# X_val = pca.transform(X_val)\n",
    "# X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 9946\n",
      "Number of validation samples: 1106\n",
      "Number of testing samples: 2764\n",
      "\n",
      "Number of features: 1128\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training samples: {X_train.shape[0]}')\n",
    "print(f'Number of validation samples: {X_val.shape[0]}')\n",
    "print(f'Number of testing samples: {X_test.shape[0]}')\n",
    "print(f'\\nNumber of features: {X_train.shape[1]}')\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(Y_val, dtype=torch.float32)), batch_size=Y_val.shape[0], shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(Y_test, dtype=torch.float32)), batch_size=Y_test.shape[0], shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_size, hidden_size, dropout_prob=0, depth=1):\n",
    "    model = [\n",
    "        torch.nn.Linear(input_size, hidden_size),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(dropout_prob)\n",
    "    ]\n",
    "\n",
    "    for i in range(depth):\n",
    "        model.append(torch.nn.Linear(hidden_size, hidden_size))\n",
    "        model.append(torch.nn.ReLU())\n",
    "        model.append(torch.nn.Dropout(dropout_prob))\n",
    "\n",
    "    model.append(torch.nn.Linear(hidden_size, 1))\n",
    "    return torch.nn.Sequential(*model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperparameter combinations: 108\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "hidden_sizes =  [256, 512, 1024]\n",
    "nums_epochs = [200]\n",
    "depth = [3, 4, 5]\n",
    "batch_sizes = [32, 64, 128]\n",
    "learning_rate = [0.01, 0.001]\n",
    "step_size_lr_decay = [10, 20]\n",
    "momentum = [0.9]\n",
    "dropout_prob = 0.2\n",
    "patience = 10\n",
    "\n",
    "hyperparameters = itertools.product(hidden_sizes, depth, nums_epochs, batch_sizes, learning_rate, step_size_lr_decay, momentum)\n",
    "n_comb = len(hidden_sizes)*len(depth)*len(nums_epochs)*len(batch_sizes)*len(learning_rate)*len(step_size_lr_decay)*len(momentum)\n",
    "print (f'Number of hyperparameter combinations: {n_comb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, criterion, loader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    y_pred = torch.tensor([]).to(device)\n",
    "    y_true = torch.tensor([]).to(device)\n",
    "\n",
    "    val_loss = 0.0\n",
    "    for data in loader:\n",
    "        data, targets = data[0].to(device), data[1].to(device)\n",
    "        pred = model(data)\n",
    "        loss = criterion(pred.squeeze(), targets)\n",
    "        val_loss += loss.item()\n",
    "        y_pred = torch.cat((y_pred, pred.squeeze()))\n",
    "        y_true = torch.cat((y_true, targets.detach()))\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    return loss / len(loader), y_pred.squeeze(), y_true.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process\n",
    "def train_model(model, criterion, optimizer, scheduler, epochs, data_loader, val_loader, device, writer, log_name):\n",
    "    n_iter = 0\n",
    "\n",
    "    best_model = None\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_since_last_improvement = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        start_epoch = time.time()\n",
    "\n",
    "        loss_train = 0\n",
    "        for data, targets in data_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            y_pred = model(data)\n",
    "            loss = criterion(y_pred.squeeze(), targets)\n",
    "            optimizer.zero_grad()\n",
    "            writer.add_scalar(\"Loss/train_iter\", loss, n_iter)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            n_iter += 1\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        loss_train /= len(data_loader)\n",
    "        writer.add_scalar(\"Loss/train\", loss_train, epoch)\n",
    "\n",
    "        # Compute Val Loss\n",
    "        val_loss, _ , _ = test_model(model, criterion, val_loader)\n",
    "\n",
    "        writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "\n",
    "        loss_history.append(loss_train)\n",
    "        val_loss_history.append(val_loss.item())\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            epochs_since_last_improvement = 0\n",
    "        elif epochs_since_last_improvement >= patience:\n",
    "            break\n",
    "        else:\n",
    "            epochs_since_last_improvement += 1\n",
    "\n",
    "        print('Epoch [{}/{}] - {:.2f} seconds - val_loss: {:.6f} - patience: {}'.format(epoch+1,\n",
    "              epochs, time.time() - start_epoch, val_loss, epochs_since_last_improvement), end='\\r')\n",
    "\n",
    "    print('\\nTraining ended after {:.2f} seconds - Best val_loss: {:.6f}'.format(time.time() - start, best_val_loss))\n",
    "\n",
    "    return best_model, loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model - MSE: 0.004751\n",
      "\n",
      "Iterations 1/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 2/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 3/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=32, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 4/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=32, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 5/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 6/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 7/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=64, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 8/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=64, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 9/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 10/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 11/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=128, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 12/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=128, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 13/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 14/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 15/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=32, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 16/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=32, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 17/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 18/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 19/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=64, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 20/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=64, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 21/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 22/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 23/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=128, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 24/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=128, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 25/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 26/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 27/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=32, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 28/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=32, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 29/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 30/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 31/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=64, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 32/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=64, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 33/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 34/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 35/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=128, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 36/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=128, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 37/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 38/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 39/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=32, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 40/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=32, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 41/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 42/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 43/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=64, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 44/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=64, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 45/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 46/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 47/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=128, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 48/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=128, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 49/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 50/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 51/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=32, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 52/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=32, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 53/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 54/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 55/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=64, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 56/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=64, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 57/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 58/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 59/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=128, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 60/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=128, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 61/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 62/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 63/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=32, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 64/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=32, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 65/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 66/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 67/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=64, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 68/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=64, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 69/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 70/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 71/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=128, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 72/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=128, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 73/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 74/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 75/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=32, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 76/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=32, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 77/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 78/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 79/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=64, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 80/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=64, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 81/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 82/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 83/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=128, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 84/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=128, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 85/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 86/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 87/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=32, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 88/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=32, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 89/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 90/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 91/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=64, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 92/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=64, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 93/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 94/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 95/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=128, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 96/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=128, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 97/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 98/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 99/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=32, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 100/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=32, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 101/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 102/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 103/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=64, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 104/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=64, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 105/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 106/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 107/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=128, lr=0.001, step_size=10, momentum=0.9\n",
      "Model already trained, skipping...\n",
      "\n",
      "Iterations 108/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=128, lr=0.001, step_size=20, momentum=0.9\n",
      "Model already trained, skipping...\n"
     ]
    }
   ],
   "source": [
    "current_iter = 0\n",
    "\n",
    "best_model = None\n",
    "best_mse = float('inf')\n",
    "criterion = torch.nn.MSELoss()\n",
    "history_loss = []\n",
    "history_val_loss = []\n",
    "\n",
    "if os.path.exists('best_model.pth'):\n",
    "    # read best model config\n",
    "    with open('best_model_config.json', 'r') as f:\n",
    "        best_model_config = json.load(f)\n",
    "\n",
    "    # load best model\n",
    "    best_model = get_model(X_train.shape[1], best_model_config['hidden_size'], dropout_prob, best_model_config['depth'])\n",
    "    best_model.load_state_dict(torch.load('best_model.pth'))\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "\n",
    "    # evaluate best model\n",
    "    best_mse, _, _ = test_model(best_model, criterion, test_loader)\n",
    "    \n",
    "    print(\"Best model - MSE: {:.6f}\".format(best_mse))\n",
    "\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "for hidden_size, depth, num_epochs, batch, lr, step_size, momentum in hyperparameters:\n",
    "    current_iter += 1\n",
    "\n",
    "    print(\"\\nIterations {}/{} - Training with hidden_size={}, depth={}, num_epochs={}, batch={}, lr={}, step_size={}, momentum={}\".format(\n",
    "        current_iter, n_comb, hidden_size, depth, num_epochs, batch, lr, step_size, momentum))\n",
    "    \n",
    "    log_name = \"dim:\"+str(hidden_size)+\"_depth:\"+str(depth)+\"_batch:\" + \\\n",
    "        str(batch)+\"_lr:\"+str(lr)+\"_step_size:\" + \\\n",
    "        str(step_size)\n",
    "\n",
    "    if os.path.exists('deepML/'+log_name):\n",
    "        print(\"Model already trained, skipping...\")\n",
    "        continue\n",
    "\n",
    "    writer = SummaryWriter('deepML/'+log_name)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(Y_train, dtype=torch.float32)), batch_size=batch, shuffle=True)\n",
    "\n",
    "    model = get_model(X_train.shape[1], hidden_size, dropout_prob, depth=depth)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)\n",
    "\n",
    "    # train\n",
    "    model, hl, hvl = train_model(model, criterion, optimizer, scheduler, num_epochs, train_loader, val_loader, device, writer, log_name)\n",
    "\n",
    "    # validate model on test set\n",
    "    mse, _, _ = test_model(model, criterion, test_loader)\n",
    "    writer.add_hparams({'hidden_size': hidden_size, 'depth': depth, 'batch': batch,'lr': lr, 'step_size': step_size, 'momentum': momentum}, {'hparam/mse': mse})\n",
    "\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_model = model\n",
    "        \n",
    "        history_loss = hl\n",
    "        history_val_loss = hvl\n",
    "\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        # save config\n",
    "        with open('best_model_config.json', 'w') as f:\n",
    "            json.dump({'name':log_name,'hidden_size': hidden_size, 'depth': depth, 'num_epochs': num_epochs, 'batch': batch,\n",
    "                       'lr': lr, 'step_size': step_size}, f)\n",
    "            \n",
    "    print(\"Model MSE: {:.6f} - Best MSE: {:.6f}\".format(mse, best_mse))\n",
    "    torch.save(model.state_dict(), 'models/'+log_name+'.pth')\n",
    "    writer.flush()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "_, y_pred, y_true = test_model(best_model, criterion, test_loader)\n",
    "y_pred = y_pred.squeeze().cpu().detach().numpy()\n",
    "y_true = y_true.squeeze().cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.978565 - MSE: 0.004751\n"
     ]
    }
   ],
   "source": [
    "r2 = r2_score(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print(\"R2: {:.6f} - MSE: {:.6f}\".format(r2, mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'loss': [h for h in history_loss], 'val_loss': [h for h in history_val_loss]})\n",
    "\n",
    "if pca is not None:\n",
    "    results.to_csv('results/pca/deepML.csv', index=False)\n",
    "else:\n",
    "    results.to_csv('results/deepML.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
