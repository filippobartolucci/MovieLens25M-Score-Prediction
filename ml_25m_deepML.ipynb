{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML_25M Deep ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import itertools\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "\n",
    "    Args:\n",
    "        seed: the seed to use. \n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "\n",
    "fix_random(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# PyTorch Device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_previous = True\n",
    "\n",
    "if delete_previous:\n",
    "    try:\n",
    "        os.system('rm -rf runs')\n",
    "        os.system('rm -rf models')\n",
    "        os.system('rm -rf best_model.pth')\n",
    "        os.system('rm -rf best_model_config.json')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('rating', axis=1)\n",
    "Y = df['rating']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "Y_train = Y_train.to_numpy()\n",
    "Y_val = Y_val.to_numpy()\n",
    "Y_test = Y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_val = pca.transform(X_val)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 9946\n",
      "Number of validation samples: 1106\n",
      "Number of testing samples: 2764\n",
      "\n",
      "Number of features: 552\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training samples: {X_train.shape[0]}')\n",
    "print(f'Number of validation samples: {X_val.shape[0]}')\n",
    "print(f'Number of testing samples: {X_test.shape[0]}')\n",
    "print(f'\\nNumber of features: {X_train.shape[1]}')\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(Y_val, dtype=torch.float32)), batch_size=Y_val.shape[0], shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(Y_test, dtype=torch.float32)), batch_size=Y_test.shape[0], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('results.csv'):\n",
    "    results = pd.read_csv('results.csv')\n",
    "else:\n",
    "    results = pd.DataFrame(columns=['model', 'mse', 'r2'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_size, hidden_size, dropout_prob=0, depth=1):\n",
    "    model = [\n",
    "        torch.nn.Linear(input_size, hidden_size),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(dropout_prob)\n",
    "    ]\n",
    "\n",
    "    for i in range(depth):\n",
    "        model.append(torch.nn.Linear(hidden_size, hidden_size))\n",
    "        model.append(torch.nn.ReLU())\n",
    "        model.append(torch.nn.Dropout(dropout_prob))\n",
    "\n",
    "    model.append(torch.nn.Linear(hidden_size, 1))\n",
    "    return torch.nn.Sequential(*model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperparameter combinations: 108\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "hidden_sizes =  [256, 512, 1024]\n",
    "nums_epochs = [200]\n",
    "depth = [3, 4, 5]\n",
    "batch_sizes = [32, 64, 128]\n",
    "learning_rate = [0.1, 0.01]\n",
    "step_size_lr_decay = [10, 20]\n",
    "momentum = [0.9]\n",
    "dropout_prob = 0.2\n",
    "patience = 10\n",
    "20\n",
    "hyperparameters = itertools.product(hidden_sizes, depth, nums_epochs, batch_sizes, learning_rate, step_size_lr_decay, momentum)\n",
    "n_comb = len(hidden_sizes)*len(depth)*len(nums_epochs)*len(batch_sizes)*len(learning_rate)*len(step_size_lr_decay)*len(momentum)\n",
    "print (f'Number of hyperparameter combinations: {n_comb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, criterion, loader):\n",
    "    loss = 0\n",
    "    y_pred = torch.tensor([]).to(device)\n",
    "    y_true = torch.tensor([]).to(device)\n",
    "    i = 0\n",
    "\n",
    "    for _, (x, y) in enumerate(loader):\n",
    "        i += 1\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        output = model(x)\n",
    "        loss += criterion(output.squeeze(), y)\n",
    "        y_pred = torch.cat((y_pred, output), 0)\n",
    "        y_true = torch.cat((y_true, y), 0)\n",
    "\n",
    "    return loss /i, y_pred.squeeze(), y_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process\n",
    "def train_model(model, criterion, optimizer, scheduler, epochs, data_loader, val_loader, device, writer, log_name):\n",
    "    n_iter = 0\n",
    "\n",
    "    best_model = None\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_since_last_improvement = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        start_epoch = time.time()\n",
    "\n",
    "        for data, targets in data_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            y_pred = model(data)\n",
    "            loss = criterion(y_pred.squeeze(), targets)\n",
    "            optimizer.zero_grad()\n",
    "            writer.add_scalar(\"Loss/train\", loss, n_iter)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            n_iter += 1\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Compute Val Loss\n",
    "        val_loss, _, _ = test_model(model, criterion, val_loader)\n",
    "        writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            epochs_since_last_improvement = 0\n",
    "        elif epochs_since_last_improvement >= patience:\n",
    "            break\n",
    "        else:\n",
    "            epochs_since_last_improvement += 1\n",
    "\n",
    "        print('Epoch [{}/{}] - {:.2f} seconds - val_loss: {:.6f} - patience: {}'.format(epoch+1,\n",
    "              epochs, time.time() - start_epoch, val_loss, epochs_since_last_improvement), end='\\r')\n",
    "\n",
    "    print('\\nTraining ended after {:.2f} seconds - Best val_loss: {:.6f}'.format(time.time() - start, best_val_loss))\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iterations 1/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=32, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [30/200] - 2.01 seconds - val_loss: 0.008845 - patience: 10\n",
      "Training ended after 69.16 seconds - Best val_loss: 0.008492\n",
      "Model MSE: 0.008336 - Best MSE: 0.008336\n",
      "\n",
      "Iterations 2/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=32, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [33/200] - 2.08 seconds - val_loss: 0.008476 - patience: 10\n",
      "Training ended after 73.27 seconds - Best val_loss: 0.008005\n",
      "Model MSE: 0.008578 - Best MSE: 0.008336\n",
      "\n",
      "Iterations 3/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [50/200] - 2.16 seconds - val_loss: 0.011943 - patience: 10\n",
      "Training ended after 106.89 seconds - Best val_loss: 0.010776\n",
      "Model MSE: 0.011673 - Best MSE: 0.008336\n",
      "\n",
      "Iterations 4/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [49/200] - 2.19 seconds - val_loss: 0.009403 - patience: 10\n",
      "Training ended after 107.71 seconds - Best val_loss: 0.008747\n",
      "Model MSE: 0.008923 - Best MSE: 0.008336\n",
      "\n",
      "Iterations 5/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=64, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [29/200] - 1.26 seconds - val_loss: 0.009644 - patience: 10\n",
      "Training ended after 34.13 seconds - Best val_loss: 0.009065\n",
      "Model MSE: 0.009572 - Best MSE: 0.008336\n",
      "\n",
      "Iterations 6/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=64, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [36/200] - 1.07 seconds - val_loss: 0.009361 - patience: 10\n",
      "Training ended after 40.76 seconds - Best val_loss: 0.008225\n",
      "Model MSE: 0.008987 - Best MSE: 0.008336\n",
      "\n",
      "Iterations 7/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [23/200] - 1.12 seconds - val_loss: 0.013480 - patience: 10\n",
      "Training ended after 26.54 seconds - Best val_loss: 0.013455\n",
      "Model MSE: 0.014466 - Best MSE: 0.008336\n",
      "\n",
      "Iterations 8/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [31/200] - 1.06 seconds - val_loss: 0.011555 - patience: 10\n",
      "Training ended after 36.91 seconds - Best val_loss: 0.010582\n",
      "Model MSE: 0.010778 - Best MSE: 0.008336\n",
      "\n",
      "Iterations 9/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=128, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [26/200] - 0.65 seconds - val_loss: 0.009790 - patience: 10\n",
      "Training ended after 16.25 seconds - Best val_loss: 0.009118\n",
      "Model MSE: 0.010606 - Best MSE: 0.008336\n",
      "\n",
      "Iterations 10/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=128, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [54/200] - 0.59 seconds - val_loss: 0.009068 - patience: 10\n",
      "Training ended after 33.00 seconds - Best val_loss: 0.008751\n",
      "Model MSE: 0.009742 - Best MSE: 0.008336\n",
      "\n",
      "Iterations 11/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [37/200] - 0.56 seconds - val_loss: 0.021225 - patience: 10\n",
      "Training ended after 22.43 seconds - Best val_loss: 0.018545\n",
      "Model MSE: 0.019693 - Best MSE: 0.008336\n",
      "\n",
      "Iterations 12/108 - Training with hidden_size=256, depth=3, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [37/200] - 0.55 seconds - val_loss: 0.014392 - patience: 10\n",
      "Training ended after 22.33 seconds - Best val_loss: 0.013767\n",
      "Model MSE: 0.014593 - Best MSE: 0.008336\n",
      "\n",
      "Iterations 13/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=32, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [34/200] - 2.58 seconds - val_loss: 0.008982 - patience: 10\n",
      "Training ended after 89.06 seconds - Best val_loss: 0.008723\n",
      "Model MSE: 0.008852 - Best MSE: 0.008336\n",
      "\n",
      "Iterations 14/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=32, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [38/200] - 2.57 seconds - val_loss: 0.008603 - patience: 10\n",
      "Training ended after 100.23 seconds - Best val_loss: 0.007726\n",
      "Model MSE: 0.008062 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 15/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [40/200] - 2.46 seconds - val_loss: 0.012469 - patience: 10\n",
      "Training ended after 106.52 seconds - Best val_loss: 0.011194\n",
      "Model MSE: 0.012485 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 16/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [33/200] - 2.72 seconds - val_loss: 0.010206 - patience: 10\n",
      "Training ended after 89.00 seconds - Best val_loss: 0.009138\n",
      "Model MSE: 0.010506 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 17/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=64, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [29/200] - 1.27 seconds - val_loss: 0.009524 - patience: 10\n",
      "Training ended after 38.68 seconds - Best val_loss: 0.009414\n",
      "Model MSE: 0.010123 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 18/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=64, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [32/200] - 1.26 seconds - val_loss: 0.008600 - patience: 10\n",
      "Training ended after 41.93 seconds - Best val_loss: 0.008208\n",
      "Model MSE: 0.008775 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 19/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [57/200] - 1.24 seconds - val_loss: 0.016566 - patience: 10\n",
      "Training ended after 73.28 seconds - Best val_loss: 0.014456\n",
      "Model MSE: 0.016864 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 20/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [57/200] - 1.23 seconds - val_loss: 0.011954 - patience: 10\n",
      "Training ended after 73.03 seconds - Best val_loss: 0.011234\n",
      "Model MSE: 0.011629 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 21/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=128, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [40/200] - 0.64 seconds - val_loss: 0.010903 - patience: 10\n",
      "Training ended after 26.23 seconds - Best val_loss: 0.010238\n",
      "Model MSE: 0.011127 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 22/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=128, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [35/200] - 0.64 seconds - val_loss: 0.009912 - patience: 10\n",
      "Training ended after 23.03 seconds - Best val_loss: 0.009576\n",
      "Model MSE: 0.009637 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 23/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [40/200] - 0.65 seconds - val_loss: 0.035589 - patience: 10\n",
      "Training ended after 26.41 seconds - Best val_loss: 0.032294\n",
      "Model MSE: 0.033697 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 24/108 - Training with hidden_size=256, depth=4, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [46/200] - 0.64 seconds - val_loss: 0.015801 - patience: 10\n",
      "Training ended after 30.16 seconds - Best val_loss: 0.014804\n",
      "Model MSE: 0.016655 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 25/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=32, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [30/200] - 2.80 seconds - val_loss: 0.010852 - patience: 10\n",
      "Training ended after 87.69 seconds - Best val_loss: 0.010294\n",
      "Model MSE: 0.011348 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 26/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=32, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [27/200] - 2.86 seconds - val_loss: 0.227223 - patience: 10\n",
      "Training ended after 80.06 seconds - Best val_loss: 0.227057\n",
      "Model MSE: 0.222031 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 27/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [53/200] - 2.75 seconds - val_loss: 0.013804 - patience: 10\n",
      "Training ended after 153.82 seconds - Best val_loss: 0.011665\n",
      "Model MSE: 0.012740 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 28/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [31/200] - 2.79 seconds - val_loss: 0.010410 - patience: 10\n",
      "Training ended after 89.18 seconds - Best val_loss: 0.009785\n",
      "Model MSE: 0.010931 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 29/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=64, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [30/200] - 1.46 seconds - val_loss: 0.010234 - patience: 10\n",
      "Training ended after 45.10 seconds - Best val_loss: 0.009507\n",
      "Model MSE: 0.009532 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 30/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=64, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [33/200] - 1.45 seconds - val_loss: 0.009574 - patience: 10\n",
      "Training ended after 49.66 seconds - Best val_loss: 0.009267\n",
      "Model MSE: 0.010305 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 31/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [36/200] - 1.45 seconds - val_loss: 0.027865 - patience: 10\n",
      "Training ended after 53.80 seconds - Best val_loss: 0.025700\n",
      "Model MSE: 0.026467 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 32/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [46/200] - 1.45 seconds - val_loss: 0.014205 - patience: 10\n",
      "Training ended after 68.36 seconds - Best val_loss: 0.012632\n",
      "Model MSE: 0.013188 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 33/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=128, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [52/200] - 0.74 seconds - val_loss: 0.012271 - patience: 10\n",
      "Training ended after 38.98 seconds - Best val_loss: 0.010603\n",
      "Model MSE: 0.012565 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 34/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=128, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [33/200] - 0.76 seconds - val_loss: 0.010297 - patience: 10\n",
      "Training ended after 25.38 seconds - Best val_loss: 0.009827\n",
      "Model MSE: 0.010497 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 35/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [33/200] - 0.80 seconds - val_loss: 0.224577 - patience: 10\n",
      "Training ended after 26.04 seconds - Best val_loss: 0.224235\n",
      "Model MSE: 0.219347 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 36/108 - Training with hidden_size=256, depth=5, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [52/200] - 0.75 seconds - val_loss: 0.024377 - patience: 10\n",
      "Training ended after 40.42 seconds - Best val_loss: 0.021612\n",
      "Model MSE: 0.023325 - Best MSE: 0.008062\n",
      "\n",
      "Iterations 37/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=32, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [40/200] - 2.31 seconds - val_loss: 0.007173 - patience: 10\n",
      "Training ended after 89.63 seconds - Best val_loss: 0.007157\n",
      "Model MSE: 0.007279 - Best MSE: 0.007279\n",
      "\n",
      "Iterations 38/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=32, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [32/200] - 2.16 seconds - val_loss: 0.007120 - patience: 10\n",
      "Training ended after 71.29 seconds - Best val_loss: 0.006969\n",
      "Model MSE: 0.007374 - Best MSE: 0.007279\n",
      "\n",
      "Iterations 39/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [27/200] - 2.12 seconds - val_loss: 0.009496 - patience: 10\n",
      "Training ended after 59.34 seconds - Best val_loss: 0.008939\n",
      "Model MSE: 0.009459 - Best MSE: 0.007279\n",
      "\n",
      "Iterations 40/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [31/200] - 2.10 seconds - val_loss: 0.009222 - patience: 10\n",
      "Training ended after 68.13 seconds - Best val_loss: 0.007986\n",
      "Model MSE: 0.008899 - Best MSE: 0.007279\n",
      "\n",
      "Iterations 41/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=64, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [26/200] - 1.29 seconds - val_loss: 0.008336 - patience: 10\n",
      "Training ended after 29.93 seconds - Best val_loss: 0.007879\n",
      "Model MSE: 0.008298 - Best MSE: 0.007279\n",
      "\n",
      "Iterations 42/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=64, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [47/200] - 1.09 seconds - val_loss: 0.008716 - patience: 10\n",
      "Training ended after 53.40 seconds - Best val_loss: 0.007181\n",
      "Model MSE: 0.007313 - Best MSE: 0.007279\n",
      "\n",
      "Iterations 43/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [29/200] - 1.11 seconds - val_loss: 0.012110 - patience: 10\n",
      "Training ended after 33.58 seconds - Best val_loss: 0.011575\n",
      "Model MSE: 0.012528 - Best MSE: 0.007279\n",
      "\n",
      "Iterations 44/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [37/200] - 1.09 seconds - val_loss: 0.009599 - patience: 10\n",
      "Training ended after 41.47 seconds - Best val_loss: 0.009432\n",
      "Model MSE: 0.010076 - Best MSE: 0.007279\n",
      "\n",
      "Iterations 45/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=128, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [28/200] - 0.65 seconds - val_loss: 0.009731 - patience: 10\n",
      "Training ended after 17.00 seconds - Best val_loss: 0.008809\n",
      "Model MSE: 0.008823 - Best MSE: 0.007279\n",
      "\n",
      "Iterations 46/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=128, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [36/200] - 0.57 seconds - val_loss: 0.007934 - patience: 10\n",
      "Training ended after 21.56 seconds - Best val_loss: 0.007867\n",
      "Model MSE: 0.008222 - Best MSE: 0.007279\n",
      "\n",
      "Iterations 47/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [35/200] - 0.58 seconds - val_loss: 0.016480 - patience: 10\n",
      "Training ended after 20.76 seconds - Best val_loss: 0.015662\n",
      "Model MSE: 0.016222 - Best MSE: 0.007279\n",
      "\n",
      "Iterations 48/108 - Training with hidden_size=512, depth=3, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [50/200] - 0.59 seconds - val_loss: 0.011460 - patience: 10\n",
      "Training ended after 29.96 seconds - Best val_loss: 0.011369\n",
      "Model MSE: 0.012595 - Best MSE: 0.007279\n",
      "\n",
      "Iterations 49/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=32, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [30/200] - 2.45 seconds - val_loss: 0.008014 - patience: 10\n",
      "Training ended after 75.89 seconds - Best val_loss: 0.007196\n",
      "Model MSE: 0.007822 - Best MSE: 0.007279\n",
      "\n",
      "Iterations 50/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=32, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [53/200] - 2.43 seconds - val_loss: 0.007480 - patience: 10\n",
      "Training ended after 132.49 seconds - Best val_loss: 0.006972\n",
      "Model MSE: 0.007260 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 51/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [35/200] - 2.43 seconds - val_loss: 0.010866 - patience: 10\n",
      "Training ended after 88.13 seconds - Best val_loss: 0.010152\n",
      "Model MSE: 0.010086 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 52/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [32/200] - 2.43 seconds - val_loss: 0.009065 - patience: 10\n",
      "Training ended after 80.52 seconds - Best val_loss: 0.008394\n",
      "Model MSE: 0.008311 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 53/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=64, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [24/200] - 1.25 seconds - val_loss: 0.008310 - patience: 10\n",
      "Training ended after 32.21 seconds - Best val_loss: 0.007598\n",
      "Model MSE: 0.008226 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 54/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=64, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [48/200] - 1.25 seconds - val_loss: 0.008400 - patience: 10\n",
      "Training ended after 62.36 seconds - Best val_loss: 0.007227\n",
      "Model MSE: 0.007412 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 55/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [35/200] - 1.25 seconds - val_loss: 0.013052 - patience: 10\n",
      "Training ended after 45.33 seconds - Best val_loss: 0.012713\n",
      "Model MSE: 0.013869 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 56/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [42/200] - 1.25 seconds - val_loss: 0.010209 - patience: 10\n",
      "Training ended after 54.04 seconds - Best val_loss: 0.009425\n",
      "Model MSE: 0.010795 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 57/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=128, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [22/200] - 0.65 seconds - val_loss: 0.009369 - patience: 10\n",
      "Training ended after 15.11 seconds - Best val_loss: 0.008715\n",
      "Model MSE: 0.009526 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 58/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=128, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [61/200] - 0.66 seconds - val_loss: 0.008122 - patience: 10\n",
      "Training ended after 40.82 seconds - Best val_loss: 0.007193\n",
      "Model MSE: 0.008013 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 59/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [26/200] - 0.65 seconds - val_loss: 0.022035 - patience: 10\n",
      "Training ended after 17.82 seconds - Best val_loss: 0.021657\n",
      "Model MSE: 0.021894 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 60/108 - Training with hidden_size=512, depth=4, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [40/200] - 0.65 seconds - val_loss: 0.013945 - patience: 10\n",
      "Training ended after 26.90 seconds - Best val_loss: 0.012905\n",
      "Model MSE: 0.014897 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 61/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=32, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [41/200] - 3.02 seconds - val_loss: 0.008059 - patience: 10\n",
      "Training ended after 122.36 seconds - Best val_loss: 0.007326\n",
      "Model MSE: 0.008162 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 62/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=32, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [68/200] - 2.99 seconds - val_loss: 0.227066 - patience: 10\n",
      "Training ended after 197.14 seconds - Best val_loss: 0.227053\n",
      "Model MSE: 0.222115 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 63/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [37/200] - 3.07 seconds - val_loss: 0.011048 - patience: 10\n",
      "Training ended after 110.43 seconds - Best val_loss: 0.010566\n",
      "Model MSE: 0.011771 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 64/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [50/200] - 2.80 seconds - val_loss: 0.009037 - patience: 10\n",
      "Training ended after 144.47 seconds - Best val_loss: 0.008741\n",
      "Model MSE: 0.008827 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 65/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=64, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [35/200] - 1.53 seconds - val_loss: 0.008809 - patience: 10\n",
      "Training ended after 52.47 seconds - Best val_loss: 0.008057\n",
      "Model MSE: 0.008490 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 66/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=64, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [29/200] - 1.53 seconds - val_loss: 0.008232 - patience: 10\n",
      "Training ended after 45.00 seconds - Best val_loss: 0.007406\n",
      "Model MSE: 0.008041 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 67/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [33/200] - 1.48 seconds - val_loss: 0.018620 - patience: 10\n",
      "Training ended after 51.04 seconds - Best val_loss: 0.016661\n",
      "Model MSE: 0.018464 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 68/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [48/200] - 1.43 seconds - val_loss: 0.011281 - patience: 10\n",
      "Training ended after 71.04 seconds - Best val_loss: 0.009931\n",
      "Model MSE: 0.010671 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 69/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=128, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [31/200] - 0.84 seconds - val_loss: 0.010143 - patience: 10\n",
      "Training ended after 23.94 seconds - Best val_loss: 0.009025\n",
      "Model MSE: 0.009561 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 70/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=128, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [40/200] - 0.74 seconds - val_loss: 0.008849 - patience: 10\n",
      "Training ended after 30.60 seconds - Best val_loss: 0.007756\n",
      "Model MSE: 0.009045 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 71/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [56/200] - 0.75 seconds - val_loss: 0.094266 - patience: 10\n",
      "Training ended after 42.58 seconds - Best val_loss: 0.092052\n",
      "Model MSE: 0.091655 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 72/108 - Training with hidden_size=512, depth=5, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [38/200] - 0.72 seconds - val_loss: 0.018354 - patience: 10\n",
      "Training ended after 28.94 seconds - Best val_loss: 0.016756\n",
      "Model MSE: 0.018237 - Best MSE: 0.007260\n",
      "\n",
      "Iterations 73/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=32, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [33/200] - 2.19 seconds - val_loss: 0.007132 - patience: 10\n",
      "Training ended after 74.90 seconds - Best val_loss: 0.006709\n",
      "Model MSE: 0.006878 - Best MSE: 0.006878\n",
      "\n",
      "Iterations 74/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=32, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [45/200] - 2.19 seconds - val_loss: 0.006525 - patience: 10\n",
      "Training ended after 100.83 seconds - Best val_loss: 0.006348\n",
      "Model MSE: 0.006360 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 75/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [35/200] - 2.37 seconds - val_loss: 0.008528 - patience: 10\n",
      "Training ended after 83.65 seconds - Best val_loss: 0.007900\n",
      "Model MSE: 0.008895 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 76/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [29/200] - 2.36 seconds - val_loss: 0.007875 - patience: 10\n",
      "Training ended after 71.34 seconds - Best val_loss: 0.006930\n",
      "Model MSE: 0.007781 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 77/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=64, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [42/200] - 1.22 seconds - val_loss: 0.007158 - patience: 10\n",
      "Training ended after 53.14 seconds - Best val_loss: 0.006712\n",
      "Model MSE: 0.006958 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 78/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=64, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [52/200] - 1.20 seconds - val_loss: 0.007010 - patience: 10\n",
      "Training ended after 63.98 seconds - Best val_loss: 0.006238\n",
      "Model MSE: 0.006696 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 79/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [43/200] - 1.22 seconds - val_loss: 0.010242 - patience: 10\n",
      "Training ended after 54.01 seconds - Best val_loss: 0.009819\n",
      "Model MSE: 0.010014 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 80/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [46/200] - 1.13 seconds - val_loss: 0.009124 - patience: 10\n",
      "Training ended after 53.85 seconds - Best val_loss: 0.008307\n",
      "Model MSE: 0.008740 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 81/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=128, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [35/200] - 0.63 seconds - val_loss: 0.008221 - patience: 10\n",
      "Training ended after 22.98 seconds - Best val_loss: 0.007522\n",
      "Model MSE: 0.007965 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 82/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=128, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [31/200] - 0.64 seconds - val_loss: 0.007179 - patience: 10\n",
      "Training ended after 20.27 seconds - Best val_loss: 0.007080\n",
      "Model MSE: 0.007311 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 83/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [33/200] - 0.63 seconds - val_loss: 0.013753 - patience: 10\n",
      "Training ended after 21.63 seconds - Best val_loss: 0.013181\n",
      "Model MSE: 0.014223 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 84/108 - Training with hidden_size=1024, depth=3, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [30/200] - 0.63 seconds - val_loss: 0.011414 - patience: 10\n",
      "Training ended after 19.64 seconds - Best val_loss: 0.010519\n",
      "Model MSE: 0.011330 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 85/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=32, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [36/200] - 2.55 seconds - val_loss: 0.006689 - patience: 10\n",
      "Training ended after 94.63 seconds - Best val_loss: 0.006513\n",
      "Model MSE: 0.006768 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 86/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=32, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [33/200] - 2.51 seconds - val_loss: 0.006994 - patience: 10\n",
      "Training ended after 85.24 seconds - Best val_loss: 0.006442\n",
      "Model MSE: 0.006883 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 87/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [32/200] - 2.53 seconds - val_loss: 0.008767 - patience: 10\n",
      "Training ended after 83.80 seconds - Best val_loss: 0.008500\n",
      "Model MSE: 0.009114 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 88/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [26/200] - 2.54 seconds - val_loss: 0.007979 - patience: 10\n",
      "Training ended after 68.54 seconds - Best val_loss: 0.007597\n",
      "Model MSE: 0.008498 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 89/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=64, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [28/200] - 1.32 seconds - val_loss: 0.007664 - patience: 10\n",
      "Training ended after 38.11 seconds - Best val_loss: 0.006929\n",
      "Model MSE: 0.007183 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 90/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=64, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [31/200] - 1.32 seconds - val_loss: 0.006962 - patience: 10\n",
      "Training ended after 42.29 seconds - Best val_loss: 0.006473\n",
      "Model MSE: 0.006950 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 91/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [40/200] - 1.33 seconds - val_loss: 0.012628 - patience: 10\n",
      "Training ended after 54.53 seconds - Best val_loss: 0.011055\n",
      "Model MSE: 0.012091 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 92/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [61/200] - 1.34 seconds - val_loss: 0.009233 - patience: 10\n",
      "Training ended after 82.73 seconds - Best val_loss: 0.008030\n",
      "Model MSE: 0.008932 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 93/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=128, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [39/200] - 0.72 seconds - val_loss: 0.007839 - patience: 10\n",
      "Training ended after 28.89 seconds - Best val_loss: 0.007536\n",
      "Model MSE: 0.007956 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 94/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=128, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [56/200] - 0.72 seconds - val_loss: 0.007866 - patience: 10\n",
      "Training ended after 41.20 seconds - Best val_loss: 0.006661\n",
      "Model MSE: 0.007649 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 95/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [40/200] - 0.71 seconds - val_loss: 0.017284 - patience: 10\n",
      "Training ended after 29.54 seconds - Best val_loss: 0.015938\n",
      "Model MSE: 0.018164 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 96/108 - Training with hidden_size=1024, depth=4, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [65/200] - 0.73 seconds - val_loss: 0.012427 - patience: 10\n",
      "Training ended after 47.36 seconds - Best val_loss: 0.010563\n",
      "Model MSE: 0.011931 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 97/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=32, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [37/200] - 2.92 seconds - val_loss: 0.007074 - patience: 10\n",
      "Training ended after 111.12 seconds - Best val_loss: 0.006548\n",
      "Model MSE: 0.006915 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 98/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=32, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [38/200] - 2.83 seconds - val_loss: 0.006799 - patience: 10\n",
      "Training ended after 113.06 seconds - Best val_loss: 0.006480\n",
      "Model MSE: 0.006774 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 99/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=32, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [24/200] - 2.90 seconds - val_loss: 0.010101 - patience: 10\n",
      "Training ended after 71.98 seconds - Best val_loss: 0.009310\n",
      "Model MSE: 0.010235 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 100/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=32, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [31/200] - 2.94 seconds - val_loss: 0.008114 - patience: 10\n",
      "Training ended after 93.40 seconds - Best val_loss: 0.007973\n",
      "Model MSE: 0.008360 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 101/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=64, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [40/200] - 1.49 seconds - val_loss: 0.007284 - patience: 10\n",
      "Training ended after 61.25 seconds - Best val_loss: 0.006822\n",
      "Model MSE: 0.007605 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 102/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=64, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [32/200] - 1.50 seconds - val_loss: 0.006844 - patience: 10\n",
      "Training ended after 49.46 seconds - Best val_loss: 0.006656\n",
      "Model MSE: 0.006802 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 103/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=64, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [29/200] - 1.50 seconds - val_loss: 0.015554 - patience: 10\n",
      "Training ended after 44.77 seconds - Best val_loss: 0.013880\n",
      "Model MSE: 0.015367 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 104/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=64, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [50/200] - 1.45 seconds - val_loss: 0.009620 - patience: 10\n",
      "Training ended after 76.00 seconds - Best val_loss: 0.009180\n",
      "Model MSE: 0.009865 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 105/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=128, lr=0.1, step_size=10, momentum=0.9\n",
      "Epoch [36/200] - 0.79 seconds - val_loss: 0.007980 - patience: 10\n",
      "Training ended after 28.88 seconds - Best val_loss: 0.007460\n",
      "Model MSE: 0.008181 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 106/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=128, lr=0.1, step_size=20, momentum=0.9\n",
      "Epoch [47/200] - 0.80 seconds - val_loss: 0.007113 - patience: 10\n",
      "Training ended after 38.06 seconds - Best val_loss: 0.007067\n",
      "Model MSE: 0.007184 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 107/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=128, lr=0.01, step_size=10, momentum=0.9\n",
      "Epoch [37/200] - 0.78 seconds - val_loss: 0.023847 - patience: 10\n",
      "Training ended after 29.41 seconds - Best val_loss: 0.022123\n",
      "Model MSE: 0.024124 - Best MSE: 0.006360\n",
      "\n",
      "Iterations 108/108 - Training with hidden_size=1024, depth=5, num_epochs=200, batch=128, lr=0.01, step_size=20, momentum=0.9\n",
      "Epoch [44/200] - 0.79 seconds - val_loss: 0.014598 - patience: 10\n",
      "Training ended after 35.38 seconds - Best val_loss: 0.013887\n",
      "Model MSE: 0.015196 - Best MSE: 0.006360\n"
     ]
    }
   ],
   "source": [
    "current_iter = 0\n",
    "\n",
    "best_model = None\n",
    "best_mse = float('inf')\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "if os.path.exists('best_model.pth'):\n",
    "    # read best model config\n",
    "    with open('best_model_config.json', 'r') as f:\n",
    "        best_model_config = json.load(f)\n",
    "\n",
    "    # load best model\n",
    "    best_model = get_model(X_train.shape[1], best_model_config['hidden_size'], dropout_prob, best_model_config['depth'])\n",
    "    best_model.load_state_dict(torch.load('best_model.pth'))\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "\n",
    "    # evaluate best model\n",
    "    best_mse, _, _ = test_model(best_model, criterion, test_loader)\n",
    "    \n",
    "    print(\"Best model - MSE: {:.6f}\".format(best_mse))\n",
    "\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "for hidden_size, depth, num_epochs, batch, lr, step_size, momentum in hyperparameters:\n",
    "    current_iter += 1\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"\\nIterations {}/{} - Training with hidden_size={}, depth={}, num_epochs={}, batch={}, lr={}, step_size={}, momentum={}\".format(\n",
    "        current_iter, n_comb, hidden_size, depth, num_epochs, batch, lr, step_size, momentum))\n",
    "    \n",
    "    log_name = \"dim:\"+str(hidden_size)+\"_depth:\"+str(depth)+\"_batch:\" + \\\n",
    "        str(batch)+\"_lr:\"+str(lr)+\"_step_size:\" + \\\n",
    "        str(step_size)+\"_momentum:\"+str(momentum)\n",
    "\n",
    "    if os.path.exists('runs/'+log_name):\n",
    "        print(\"Model already trained, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # start tensorboard\n",
    "    writer = SummaryWriter('runs/'+log_name)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(Y_train, dtype=torch.float32)), batch_size=batch, shuffle=True)\n",
    "\n",
    "    model = get_model(X_train.shape[1], hidden_size, dropout_prob, depth=depth)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)\n",
    "\n",
    "    # train\n",
    "    model = train_model(model, criterion, optimizer, scheduler, num_epochs, train_loader, val_loader, device, writer, log_name)\n",
    "\n",
    "    # validate model on test set\n",
    "    mse, _, _ = test_model(model, criterion, test_loader)\n",
    "    writer.add_hparams({'hidden_size': hidden_size, 'depth': depth, 'num_epochs': num_epochs, 'batch': batch,'lr': lr, 'step_size': step_size, 'momentum': momentum}, {'hparam/mse': mse})\n",
    "\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_model = model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        # save config\n",
    "        with open('best_model_config.json', 'w') as f:\n",
    "            json.dump({'name':log_name,'hidden_size': hidden_size, 'depth': depth, 'num_epochs': num_epochs, 'batch': batch,\n",
    "                       'lr': lr, 'step_size': step_size}, f)\n",
    "            \n",
    "            \n",
    "    print(\"Model MSE: {:.6f} - Best MSE: {:.6f}\".format(mse, best_mse))\n",
    "\n",
    "    torch.save(model.state_dict(), 'models/'+log_name+'.pth')\n",
    "    writer.flush()\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model config: {'name': 'dim:1024_depth:3_batch:32_lr:0.1_step_size:20_momentum:0.9', 'hidden_size': 1024, 'depth': 3, 'num_epochs': 200, 'batch': 32, 'lr': 0.1, 'step_size': 20}\n"
     ]
    }
   ],
   "source": [
    "# print best config\n",
    "with open('best_model_config.json', 'r') as f:\n",
    "    best_model_config = json.load(f)\n",
    "    print(\"Best model config:\", best_model_config)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.006031 - R2: 0.972794\n"
     ]
    }
   ],
   "source": [
    "test_loss, y_pred, y_true = test_model(best_model, criterion, test_loader)\n",
    "\n",
    "y_pred = y_pred.cpu().detach().numpy()\n",
    "y_true = y_true.cpu().detach().numpy()\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(\"MSE: {:.6f} - R2: {:.6f}\".format(test_loss, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([results, pd.DataFrame({'MSE': [mse], 'R2': [r2], 'Model': \"deep_network\"})]) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
