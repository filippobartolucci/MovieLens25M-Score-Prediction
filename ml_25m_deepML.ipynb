{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML_25M Deep ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "import itertools\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "# from tab_transformer_pytorch import TabTransformer\n",
    "# from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "# from pytorch_tabular import TabularModel\n",
    "# from pytorch_tabular.models import CategoryEmbeddingModelConfig\n",
    "# from pytorch_tabular.config import (\n",
    "#     DataConfig,\n",
    "#     OptimizerConfig,\n",
    "#     TrainerConfig,\n",
    "# )\n",
    "\n",
    "\n",
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "\n",
    "    Args:\n",
    "        seed: the seed to use. \n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "\n",
    "\n",
    "fix_random(42)\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# PyTorch Device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using device:', device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: (9946, 9)\n",
      "Number of validation samples: 1106\n",
      "Number of testing samples: (2764, 9)\n"
     ]
    }
   ],
   "source": [
    "N_LABELS = df.rating.nunique()\n",
    "\n",
    "X = df.drop('rating', axis=1)\n",
    "Y = df['rating']\n",
    "\n",
    "# encode Y\n",
    "Y = LabelEncoder().fit_transform(Y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# Normalization\n",
    "\n",
    "\n",
    "# PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# pca.fit(X_train)\n",
    "# X_train = pca.transform(X_train)\n",
    "# X_test = pca.transform(X_test)\n",
    "\n",
    "# LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, Y_train)\n",
    "X_train = lda.transform(X_train)\n",
    "X_val = lda.transform(X_val)\n",
    "X_test = lda.transform(X_test)\n",
    "\n",
    "print(f'Number of training samples: {X_train.shape}')\n",
    "print(f'Number of validation samples: {X_val.shape[0]}')\n",
    "print(f'Number of testing samples: {X_test.shape}')\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(Y_train, dtype=torch.long)), batch_size=batch, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(Y_val, dtype=torch.long)), batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(Y_test, dtype=torch.long)), batch_size=64, shuffle=True)\n",
    "\n",
    "results = pd.DataFrame(columns=['Model', 'Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effective Number of Samples\n",
    "def get_weights_effective_num_of_samples(no_of_classes, beta, samples_per_cls):\n",
    "    effective_num = 1.0 - np.power(beta, samples_per_cls)\n",
    "    class_weights = (1.0 - beta) / np.array(effective_num)\n",
    "    class_weights = class_weights / np.sum(class_weights)*no_of_classes\n",
    "    return {i: w for i, w in enumerate(class_weights)}\n",
    "\n",
    "weights = get_weights_effective_num_of_samples(N_LABELS, 0.999, np.bincount(Y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test label distribution\n",
    "plt.hist(Y_test )\n",
    "plt.title('Label Distribution')\n",
    "for p in plt.gca().patches:\n",
    "    plt.gca().annotate('{:.0f}'.format(p.get_height()), (p.get_x() + p.get_width() / 2.,\n",
    "                                                         p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_model(input_size, hidden_size, num_classes, dropout_prob=0, depth=1):\n",
    "    model = [\n",
    "        torch.nn.Linear(input_size, hidden_size),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(dropout_prob)\n",
    "    ]\n",
    "\n",
    "    for i in range(depth):\n",
    "        model.append(torch.nn.Linear(hidden_size, hidden_size))\n",
    "        model.append(torch.nn.ReLU())\n",
    "        model.append(torch.nn.Dropout(dropout_prob))\n",
    "\n",
    "    model.append(torch.nn.Linear(hidden_size, num_classes))\n",
    "    model.append(torch.nn.Softmax(dim=1))\n",
    "\n",
    "    return torch.nn.Sequential(*model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperparameter combinations: 72\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "hidden_sizes = [64, 128, 256]\n",
    "nums_epochs = [100]\n",
    "depth = [3, 4]\n",
    "batch_sizes = [16, 32, 64, 128]\n",
    "learning_rate = [0.01]\n",
    "step_size_lr_decay = [10, 25, 50]\n",
    "dropout_prob = 0.2\n",
    "patience = 10\n",
    "\n",
    "hyperparameters = itertools.product(hidden_sizes, depth, nums_epochs, batch_sizes, learning_rate, step_size_lr_decay)\n",
    "print (f'Number of hyperparameter combinations: {len(hidden_sizes)*len(depth)*len(nums_epochs)*len(batch_sizes)*len(learning_rate)*len(step_size_lr_decay)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process\n",
    "def train_model(model, criterion, optimizer, scheduler, epochs, data_loader, val_loader, device, writer):\n",
    "    n_iter = 0\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_since_last_improvement = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        start_epoch = time.time()\n",
    "\n",
    "        for data, targets in data_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(data)\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(y_pred, targets)\n",
    "            writer.add_scalar(\"Loss/train\", loss, n_iter)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            n_iter += 1\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Compute Val Loss\n",
    "        labels, _, y_pred = test_model(model, val_loader, device)\n",
    "        val_loss = criterion(y_pred, labels)\n",
    "        writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'models/best_model.pth')\n",
    "            epochs_since_last_improvement = 0\n",
    "        else:\n",
    "            epochs_since_last_improvement += 1\n",
    "\n",
    "        if epochs_since_last_improvement >= patience:\n",
    "            break\n",
    "\n",
    "        print('Epoch [{}/{}] - {:.2f} seconds - loss: {:.4f} - val_loss: {:.4f} - patience: {}'.format(epoch+1,\n",
    "              epochs, time.time() - start_epoch, loss.item(), val_loss.item(), epochs_since_last_improvement), end='\\r')\n",
    "\n",
    "    print('\\nTraining ended after {:.2f} seconds'.format(time.time() - start))\n",
    "\n",
    "    # Restore best model\n",
    "    model.load_state_dict(torch.load('models/best_model.pth'))\n",
    "    return model\n",
    "\n",
    "# evaluation process\n",
    "def test_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in data_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            y_pred.append(model(data))\n",
    "            y_test.append(targets)\n",
    "\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    y_test = torch.cat(y_test, dim=0)\n",
    "\n",
    "    y_pred_c = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "    return y_test, y_pred_c, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model accuracy so far: 72.25%\n",
      "\n",
      "Training with hidden_size=64, depth=3, num_epochs=100, batch=64, lr=0.01, step_size=25\n",
      "Epoch [13/100] - 1.40 seconds - loss: 1.7029 - val_loss: 2.0739 - patience: 9\n",
      "Training ended after 20.05 seconds\n",
      "Model accuracy: 0.70%\n",
      "Best accuracy so far: 72.25\n",
      "\n",
      "Training with hidden_size=64, depth=3, num_epochs=100, batch=64, lr=0.01, step_size=50\n",
      "Epoch [10/100] - 1.57 seconds - loss: 1.9983 - val_loss: 2.0646 - patience: 9\n",
      "Training ended after 16.14 seconds\n",
      "Model accuracy: 0.71%\n",
      "Best accuracy so far: 72.25\n",
      "\n",
      "Training with hidden_size=64, depth=3, num_epochs=100, batch=128, lr=0.01, step_size=10\n",
      "Epoch [15/100] - 0.71 seconds - loss: 2.1978 - val_loss: 2.0548 - patience: 9\n",
      "Training ended after 12.29 seconds\n",
      "Model accuracy: 0.71%\n",
      "Best accuracy so far: 72.25\n",
      "\n",
      "Training with hidden_size=64, depth=3, num_epochs=100, batch=128, lr=0.01, step_size=25\n",
      "Epoch [10/100] - 0.72 seconds - loss: 2.0795 - val_loss: 2.0605 - patience: 9\n",
      "Training ended after 7.97 seconds\n",
      "Model accuracy: 0.71%\n",
      "Best accuracy so far: 72.25\n",
      "\n",
      "Training with hidden_size=64, depth=3, num_epochs=100, batch=128, lr=0.01, step_size=50\n",
      "Epoch [16/100] - 0.71 seconds - loss: 1.9505 - val_loss: 2.0563 - patience: 9\n",
      "Training ended after 12.28 seconds\n",
      "Model accuracy: 0.71%\n",
      "Best accuracy so far: 72.25\n",
      "\n",
      "Training with hidden_size=64, depth=4, num_epochs=100, batch=16, lr=0.01, step_size=10\n",
      "Epoch [10/100] - 6.18 seconds - loss: 2.2896 - val_loss: 2.2413 - patience: 9\n",
      "Training ended after 69.66 seconds\n",
      "Model accuracy: 0.67%\n",
      "Best accuracy so far: 72.25\n",
      "\n",
      "Training with hidden_size=64, depth=4, num_epochs=100, batch=16, lr=0.01, step_size=25\n",
      "Epoch [10/100] - 6.16 seconds - loss: 2.2530 - val_loss: 2.2413 - patience: 9\n",
      "Training ended after 68.09 seconds\n",
      "Model accuracy: 0.68%\n",
      "Best accuracy so far: 72.25\n",
      "\n",
      "Training with hidden_size=64, depth=4, num_epochs=100, batch=16, lr=0.01, step_size=50\n",
      "Epoch [10/100] - 6.21 seconds - loss: 2.2711 - val_loss: 2.2413 - patience: 9\n",
      "Training ended after 68.08 seconds\n",
      "Model accuracy: 0.69%\n",
      "Best accuracy so far: 72.25\n",
      "\n",
      "Training with hidden_size=64, depth=4, num_epochs=100, batch=32, lr=0.01, step_size=10\n",
      "Epoch [11/100] - 3.12 seconds - loss: 2.2570 - val_loss: 2.2413 - patience: 9\n",
      "Training ended after 37.83 seconds\n",
      "Model accuracy: 0.69%\n",
      "Best accuracy so far: 72.25\n",
      "\n",
      "Training with hidden_size=64, depth=4, num_epochs=100, batch=32, lr=0.01, step_size=25\n",
      "Epoch [11/100] - 3.15 seconds - loss: 2.2252 - val_loss: 2.1997 - patience: 9\n",
      "Training ended after 37.70 seconds\n",
      "Model accuracy: 0.72%\n",
      "Best accuracy so far: 72.25\n",
      "\n",
      "Training with hidden_size=64, depth=4, num_epochs=100, batch=32, lr=0.01, step_size=50\n",
      "Epoch [17/100] - 3.16 seconds - loss: 2.1710 - val_loss: 2.2413 - patience: 9\n",
      "Training ended after 56.41 seconds\n",
      "Model accuracy: 0.71%\n",
      "Best accuracy so far: 72.25\n",
      "\n",
      "Training with hidden_size=64, depth=4, num_epochs=100, batch=64, lr=0.01, step_size=10\n",
      "Epoch [18/100] - 1.59 seconds - loss: 2.0590 - val_loss: 2.0563 - patience: 9\n",
      "Training ended after 30.47 seconds\n",
      "Model accuracy: 0.71%\n",
      "Best accuracy so far: 72.25\n",
      "\n",
      "Training with hidden_size=64, depth=4, num_epochs=100, batch=64, lr=0.01, step_size=25\n",
      "Epoch [10/100] - 1.59 seconds - loss: 2.2222 - val_loss: 2.2226 - patience: 9\n",
      "Training ended after 17.70 seconds\n",
      "Model accuracy: 0.70%\n",
      "Best accuracy so far: 72.25\n",
      "\n",
      "Training with hidden_size=64, depth=4, num_epochs=100, batch=64, lr=0.01, step_size=50\n",
      "Epoch [11/100] - 1.60 seconds - loss: 1.9910 - val_loss: 2.0712 - patience: 9\n",
      "Training ended after 19.23 seconds\n",
      "Model accuracy: 0.71%\n",
      "Best accuracy so far: 72.25\n",
      "\n",
      "Training with hidden_size=64, depth=4, num_epochs=100, batch=128, lr=0.01, step_size=10\n",
      "Epoch [11/100] - 0.82 seconds - loss: 2.0457 - val_loss: 2.0576 - patience: 9\n",
      "Training ended after 9.95 seconds\n",
      "Model accuracy: 0.69%\n",
      "Best accuracy so far: 72.25\n",
      "\n",
      "Training with hidden_size=64, depth=4, num_epochs=100, batch=128, lr=0.01, step_size=25\n",
      "Epoch [21/100] - 0.84 seconds - loss: 2.1072 - val_loss: 2.0653 - patience: 9\n",
      "Training ended after 18.21 seconds\n",
      "Model accuracy: 0.71%\n",
      "Best accuracy so far: 72.25\n",
      "\n",
      "Training with hidden_size=64, depth=4, num_epochs=100, batch=128, lr=0.01, step_size=50\n",
      "Epoch [15/100] - 0.82 seconds - loss: 2.2113 - val_loss: 2.0568 - patience: 9\n",
      "Training ended after 13.27 seconds\n",
      "Model accuracy: 0.70%\n",
      "Best accuracy so far: 72.25\n",
      "\n",
      "Training with hidden_size=128, depth=3, num_epochs=100, batch=16, lr=0.01, step_size=10\n",
      "Epoch [12/100] - 5.88 seconds - loss: 2.2740 - val_loss: 2.2226 - patience: 9\n",
      "Training ended after 75.66 seconds\n",
      "Model accuracy: 0.43%\n",
      "Best accuracy so far: 72.25\n",
      "\n",
      "Training with hidden_size=128, depth=3, num_epochs=100, batch=16, lr=0.01, step_size=25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mStepLR(optimizer, step_size\u001b[39m=\u001b[39mstep_size, gamma\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[39m# train\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m model \u001b[39m=\u001b[39m train_model(model, criterion, optimizer, scheduler,\n\u001b[1;32m     52\u001b[0m                     num_epochs, train_loader, val_loader, device, writer)\n\u001b[1;32m     54\u001b[0m \u001b[39m# validate\u001b[39;00m\n\u001b[1;32m     55\u001b[0m y_test, y_pred_c, y_pred \u001b[39m=\u001b[39m test_model(model, test_loader, device)\n",
      "Cell \u001b[0;32mIn[44], line 29\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, epochs, data_loader, val_loader, device, writer)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[39m# Backward pass\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 29\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     31\u001b[0m     n_iter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     33\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    235\u001b[0m          grads,\n\u001b[1;32m    236\u001b[0m          exp_avgs,\n\u001b[1;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    239\u001b[0m          state_steps,\n\u001b[1;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[1;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m func(params,\n\u001b[1;32m    301\u001b[0m      grads,\n\u001b[1;32m    302\u001b[0m      exp_avgs,\n\u001b[1;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    305\u001b[0m      state_steps,\n\u001b[1;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/torch/optim/adam.py:412\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m--> 412\u001b[0m param\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "best_model = None\n",
    "\n",
    "if os.path.exists('best_model.pth'):\n",
    "    # read best model config\n",
    "    with open('best_model_config.json', 'r') as f:\n",
    "        best_model_config = json.load(f)\n",
    "\n",
    "    # load best model\n",
    "    best_model = get_nn_model(X_train.shape[1], best_model_config['hidden_size'], N_LABELS, dropout_prob, best_model_config['depth'])\n",
    "    best_model.load_state_dict(torch.load('best_model.pth'))\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "\n",
    "    # evaluate best model\n",
    "    y_test, y_pred_c, y_pred = test_model(best_model, test_loader, device)\n",
    "    metrics = classification_report(y_test.cpu(), y_pred_c.cpu(), output_dict=True, zero_division=0)\n",
    "    best_acc = metrics['accuracy']*100\n",
    "    \n",
    "print(\"Best model accuracy so far: {:.2f}%\".format(best_acc))\n",
    "\n",
    "for hidden_size, depth, num_epochs, batch, lr, step_size in hyperparameters:\n",
    "    \n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "\n",
    "    print(\"\\nTraining with hidden_size={}, depth={}, num_epochs={}, batch={}, lr={}, step_size={}\".format(\n",
    "        hidden_size, depth, num_epochs, batch, lr, step_size))\n",
    "    log_name = \"dim:\"+str(hidden_size)+\"_depth:\"+str(depth)+\"_epochs:\"+str(num_epochs)+\"_batch:\" + \\\n",
    "        str(batch)+\"_lr:\"+str(lr)+\"_step_size:\" + \\\n",
    "        str(step_size)\n",
    "\n",
    "    if os.path.exists('runs/'+log_name):\n",
    "        print(\"Model already trained, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # start tensorboard\n",
    "    writer = SummaryWriter('runs/'+log_name)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(\n",
    "        X_train, dtype=torch.float32), torch.tensor(Y_train, dtype=torch.long)), batch_size=batch, shuffle=True)\n",
    "\n",
    "    model = get_nn_model(X_train.shape[1], hidden_size, N_LABELS, dropout_prob, depth=depth)\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(list(weights.values()), dtype=torch.float32).to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)\n",
    "\n",
    "    # train\n",
    "    model = train_model(model, criterion, optimizer, scheduler,\n",
    "                        num_epochs, train_loader, val_loader, device, writer)\n",
    "\n",
    "    # validate\n",
    "    y_test, y_pred_c, y_pred = test_model(model, test_loader, device)\n",
    "\n",
    "    metrics = classification_report(y_test.cpu(), y_pred_c.cpu(), output_dict=True, zero_division=0)\n",
    "    writer.add_scalar('metrics/test accuracy', metrics['accuracy'])\n",
    "\n",
    "    writer.add_hparams({'hidden_size': hidden_size, 'depth': depth, 'num_epochs': num_epochs, 'batch': batch,'lr': lr, 'step_size': step_size, 'momentum': momentum}, {'hparam/accuracy': metrics['accuracy']})\n",
    "\n",
    "    if metrics['accuracy']*100 > best_acc:\n",
    "        best_acc = metrics['accuracy']*100\n",
    "        best_model = model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        # save config\n",
    "        with open('best_model_config.json', 'w') as f:\n",
    "            json.dump({'hidden_size': hidden_size, 'depth': depth, 'num_epochs': num_epochs, 'batch': batch,\n",
    "                       'lr': lr, 'step_size': step_size}, f)\n",
    "            \n",
    "            \n",
    "    print(\"Model accuracy: {:.2f}%\".format(metrics['accuracy']))\n",
    "    print(\"Best accuracy so far: {:.2f}\".format(best_acc))\n",
    "\n",
    "    torch.save(model.state_dict(), 'models/'+log_name+'.pth')\n",
    "    writer.flush()\n",
    "\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.00      0.00      0.00         9\n",
      "           2       0.00      0.00      0.00        80\n",
      "           3       0.55      0.63      0.59       284\n",
      "           4       0.78      0.69      0.73       804\n",
      "           5       0.72      0.93      0.81      1159\n",
      "           6       0.84      0.46      0.59       426\n",
      "           7       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.72      2764\n",
      "   macro avg       0.36      0.34      0.34      2764\n",
      "weighted avg       0.71      0.72      0.70      2764\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "best_model.eval()\n",
    "y_test, y_pred_c, y_pred = test_model(best_model, test_loader, device)\n",
    "print(classification_report(y_test.cpu(), y_pred_c.cpu()))\n",
    "accuracy = accuracy_score(y_test.cpu(), y_pred_c.cpu())\n",
    "\n",
    "results = pd.concat([results, pd.DataFrame({'Model': 'NeuralNetwork', 'Accuracy': accuracy}, index=[0])], ignore_index=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
