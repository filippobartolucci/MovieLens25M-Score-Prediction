{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analytics - ML25M "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "import itertools\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "# from tab_transformer_pytorch import TabTransformer\n",
    "# from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "# from pytorch_tabular import TabularModel\n",
    "# from pytorch_tabular.models import CategoryEmbeddingModelConfig\n",
    "# from pytorch_tabular.config import (\n",
    "#     DataConfig,\n",
    "#     OptimizerConfig,\n",
    "#     TrainerConfig,\n",
    "# )\n",
    "\n",
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "\n",
    "    Args:\n",
    "        seed: the seed to use. \n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "\n",
    "fix_random(42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "movies.csv contains the following fields:\n",
    "\n",
    "* movieId - a unique identifier for each movie.\n",
    "* title - the title of the movie.\n",
    "* genres - a pipe-separated list of genres for the movie.\n",
    "\n",
    "It will be used to get the movie title and the genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>(no genres listed)</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Children</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>...</th>\n",
       "      <th>Film-Noir</th>\n",
       "      <th>Horror</th>\n",
       "      <th>IMAX</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                    title  (no genres listed)  Action  Adventure  \\\n",
       "0        1         Toy Story (1995)                   0       0          1   \n",
       "1        2           Jumanji (1995)                   0       0          1   \n",
       "2        3  Grumpier Old Men (1995)                   0       0          0   \n",
       "\n",
       "   Animation  Children  Comedy  Crime  Documentary  ...  Film-Noir  Horror  \\\n",
       "0          1         1       1      0            0  ...          0       0   \n",
       "1          0         1       0      0            0  ...          0       0   \n",
       "2          0         0       1      0            0  ...          0       0   \n",
       "\n",
       "   IMAX  Musical  Mystery  Romance  Sci-Fi  Thriller  War  Western  \n",
       "0     0        0        0        0       0         0    0        0  \n",
       "1     0        0        0        0       0         0    0        0  \n",
       "2     0        0        0        1       0         0    0        0  \n",
       "\n",
       "[3 rows x 22 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df = pd.read_csv('./ml-25m/movies.csv')\n",
    "genres = movies_df['genres'].str.get_dummies(sep='|')\n",
    "movies_df = pd.concat([movies_df, genres], axis=1)\n",
    "movies_df.drop('genres', axis=1, inplace=True)\n",
    "movies_df.head(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "genome-scores.csv contains the following fields:\n",
    "\n",
    "* movieId - a unique identifier for each movie.\n",
    "* tagId - a unique identifier for each tag.\n",
    "* relevance - a score from 0.0 to 1.0 representing the relevance of the tag to the movie.\n",
    "\n",
    "Combined with the tags.csv file, this will be used to assign tags and their relevance to each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.read_csv('./ml-25m/genome-scores.csv')\n",
    "tags_df = pd.read_csv('./ml-25m/genome-tags.csv')\n",
    "ratings_df = pd.read_csv('./ml-25m/ratings.csv')\n",
    "\n",
    "df = movies_df.merge(scores_df, on='movieId')\n",
    "df = df.merge(tags_df, on='tagId')\n",
    "df = df.pivot_table(index=['movieId', 'title'], columns='tag', values='relevance', fill_value=0).reset_index().rename_axis(None, axis=1)\n",
    "\n",
    "# average rating for each movie\n",
    "# ratings_df = ratings_df.groupby(['movieId'])['rating'].mean().reset_index()\n",
    "# # round ratings to the nearest 0.5\n",
    "# ratings_df['rating'] = ratings_df['rating'].apply(lambda x: round(x * 2) / 2)\n",
    "\n",
    "# # mode rating for each movie\n",
    "ratings_df = ratings_df.groupby(['movieId'])['rating'].agg(lambda x: x.value_counts().index[0]).reset_index()\n",
    "\n",
    "df = ratings_df.merge(df, on='movieId')\n",
    "\n",
    "# movieId and title are not needed for the model\n",
    "df.drop(['movieId', 'title'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 13816\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>007</th>\n",
       "      <th>007 (series)</th>\n",
       "      <th>18th century</th>\n",
       "      <th>1920s</th>\n",
       "      <th>1930s</th>\n",
       "      <th>1950s</th>\n",
       "      <th>1960s</th>\n",
       "      <th>1970s</th>\n",
       "      <th>1980s</th>\n",
       "      <th>...</th>\n",
       "      <th>world politics</th>\n",
       "      <th>world war i</th>\n",
       "      <th>world war ii</th>\n",
       "      <th>writer's life</th>\n",
       "      <th>writers</th>\n",
       "      <th>writing</th>\n",
       "      <th>wuxia</th>\n",
       "      <th>wwii</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.02875</td>\n",
       "      <td>0.02375</td>\n",
       "      <td>0.06250</td>\n",
       "      <td>0.07575</td>\n",
       "      <td>0.14075</td>\n",
       "      <td>0.14675</td>\n",
       "      <td>0.06350</td>\n",
       "      <td>0.20375</td>\n",
       "      <td>0.2020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04050</td>\n",
       "      <td>0.01425</td>\n",
       "      <td>0.03050</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.14125</td>\n",
       "      <td>0.05775</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.02975</td>\n",
       "      <td>0.08475</td>\n",
       "      <td>0.02200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.04125</td>\n",
       "      <td>0.04050</td>\n",
       "      <td>0.06275</td>\n",
       "      <td>0.08275</td>\n",
       "      <td>0.09100</td>\n",
       "      <td>0.06125</td>\n",
       "      <td>0.06925</td>\n",
       "      <td>0.09600</td>\n",
       "      <td>0.0765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05250</td>\n",
       "      <td>0.01575</td>\n",
       "      <td>0.01250</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.12225</td>\n",
       "      <td>0.03275</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.01100</td>\n",
       "      <td>0.10525</td>\n",
       "      <td>0.01975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.04675</td>\n",
       "      <td>0.05550</td>\n",
       "      <td>0.02925</td>\n",
       "      <td>0.08700</td>\n",
       "      <td>0.04750</td>\n",
       "      <td>0.04775</td>\n",
       "      <td>0.04600</td>\n",
       "      <td>0.14275</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06275</td>\n",
       "      <td>0.01950</td>\n",
       "      <td>0.02225</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.12200</td>\n",
       "      <td>0.03475</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.01800</td>\n",
       "      <td>0.09100</td>\n",
       "      <td>0.01775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 1129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating      007  007 (series)  18th century    1920s    1930s    1950s  \\\n",
       "0     4.0  0.02875       0.02375       0.06250  0.07575  0.14075  0.14675   \n",
       "1     3.0  0.04125       0.04050       0.06275  0.08275  0.09100  0.06125   \n",
       "2     3.0  0.04675       0.05550       0.02925  0.08700  0.04750  0.04775   \n",
       "\n",
       "     1960s    1970s   1980s  ...  world politics  world war i  world war ii  \\\n",
       "0  0.06350  0.20375  0.2020  ...         0.04050      0.01425       0.03050   \n",
       "1  0.06925  0.09600  0.0765  ...         0.05250      0.01575       0.01250   \n",
       "2  0.04600  0.14275  0.0285  ...         0.06275      0.01950       0.02225   \n",
       "\n",
       "   writer's life  writers  writing  wuxia     wwii   zombie  zombies  \n",
       "0          0.035  0.14125  0.05775  0.039  0.02975  0.08475  0.02200  \n",
       "1          0.020  0.12225  0.03275  0.021  0.01100  0.10525  0.01975  \n",
       "2          0.023  0.12200  0.03475  0.017  0.01800  0.09100  0.01775  \n",
       "\n",
       "[3 rows x 1129 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Number of samples: {df.shape[0]}')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode:  0    4.0\n",
      "Name: rating, dtype: float64\n",
      "Median: 3.50\n",
      "Std: 0.73\n"
     ]
    }
   ],
   "source": [
    "# Ratings distribution\n",
    "sns.countplot(x='rating', data=df, palette='flare')\n",
    "plt.title('Ratings distribution')\n",
    "for p in plt.gca().patches:\n",
    "    plt.gca().annotate('{:.0f}'.format(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "plt.show()\n",
    "\n",
    "# mean, median, and standard deviation of ratings\n",
    "print(\"Mode: \", df['rating'].mode())\n",
    "print('Median: {:.2f}'.format(df['rating'].median()))\n",
    "print('Std: {:.2f}'.format(df['rating'].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 10\n",
      "Nan:  0\n",
      "Duplicates:  0\n"
     ]
    }
   ],
   "source": [
    "N_LABELS = df.rating.nunique()\n",
    "print(f'Number of labels: {N_LABELS}')\n",
    "\n",
    "# looking for missing values and duplicates\n",
    "print(\"Nan: \", df.isna().sum().sum())\n",
    "print(\"Duplicates: \", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a csv file\n",
    "df.to_csv('./dataset.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: (11052, 9)\n",
      "Number of testing samples: (2764, 9)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('rating', axis=1)\n",
    "Y = df['rating']\n",
    "\n",
    "# encode Y\n",
    "Y = LabelEncoder().fit_transform(Y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalization \n",
    "\n",
    "\n",
    "# PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# pca.fit(X_train)\n",
    "# X_train = pca.transform(X_train)\n",
    "# X_test = pca.transform(X_test)\n",
    "\n",
    "# LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, Y_train)\n",
    "X_train = lda.transform(X_train)\n",
    "X_test = lda.transform(X_test)\n",
    "\n",
    "print(f'Number of training samples: {X_train.shape}')\n",
    "print(f'Number of testing samples: {X_test.shape}')\n",
    "\n",
    "results = pd.DataFrame(columns=['Model', 'Accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning for Tabular Data\n",
    "\n",
    "[PyTorch Tabular](https://github.com/manujosephv/pytorch_tabular#installation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 9946\n",
      "Number of validation samples: 1106\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f'Number of training samples: {X_train.shape[0]}')\n",
    "print(f'Number of validation samples: {X_val.shape[0]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TabNetClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Define the TabNet model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tabnet \u001b[39m=\u001b[39m TabNetClassifier(\n\u001b[1;32m      3\u001b[0m     n_d\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, \u001b[39m# n_d: the dimensionality of the output space of the feature transformer network (default 64)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     n_a\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, \u001b[39m# n_a: the dimensionality of the output space of the attention network (default 64)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     n_steps\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, \u001b[39m# n_steps: the number of sequential steps in the attention mechanism (default 3)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     gamma\u001b[39m=\u001b[39m\u001b[39m1.5\u001b[39m, \u001b[39m# n_steps: the number of sequential steps in the attention mechanism (default 3)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     n_independent\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m# n_independent: the number of independent feature transformer networks to use (default 2)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     n_shared\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m# n_shared: the number of shared feature transformer networks to use (default 2)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     epsilon\u001b[39m=\u001b[39m\u001b[39m1e-15\u001b[39m, \u001b[39m# epsilon: a small value to add to the denominator of the feature importance calculation to avoid division by zero (default 1e-15)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, \u001b[39m# seed: the random seed to use for reproducibility (default None)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m tabnet\u001b[39m.\u001b[39mfit(\n\u001b[1;32m     15\u001b[0m     X_train\u001b[39m=\u001b[39mX_train,\n\u001b[1;32m     16\u001b[0m     y_train\u001b[39m=\u001b[39mY_train,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     max_epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, \u001b[39m# max_epochs: the maximum number of epochs to train for (default 100)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TabNetClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the TabNet model\n",
    "tabnet = TabNetClassifier(\n",
    "    n_d=16, # n_d: the dimensionality of the output space of the feature transformer network (default 64)\n",
    "    n_a=16, # n_a: the dimensionality of the output space of the attention network (default 64)\n",
    "    n_steps=4, # n_steps: the number of sequential steps in the attention mechanism (default 3)\n",
    "    gamma=1.5, # n_steps: the number of sequential steps in the attention mechanism (default 3)\n",
    "    n_independent=2, # n_independent: the number of independent feature transformer networks to use (default 2)\n",
    "    n_shared=2, # n_shared: the number of shared feature transformer networks to use (default 2)\n",
    "    epsilon=1e-15, # epsilon: a small value to add to the denominator of the feature importance calculation to avoid division by zero (default 1e-15)\n",
    "    seed=42, # seed: the random seed to use for reproducibility (default None)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "tabnet.fit(\n",
    "    X_train=X_train,\n",
    "    y_train=Y_train,\n",
    "    eval_set=[(X_val, Y_val)],\n",
    "    patience=10, #patience: the number of epochs to wait without improvement in validation loss before early stopping (default 10)\n",
    "    batch_size=1024, # batch_size: the number of samples per batch (default 1024)\n",
    "    virtual_batch_size=128, # virtual_batch_size: the number of samples per virtual batch (default 128)\n",
    "    num_workers=0, # num_workers: the number of worker processes to use for data loading (default 0)\n",
    "    drop_last=False, # drop_last: whether to drop the last incomplete batch if the dataset size is not divisible by the batch size (default False)\n",
    "    max_epochs=100, # max_epochs: the maximum number of epochs to train for (default 100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.36      0.44      0.40         9\n",
      "           2       0.87      0.72      0.79        80\n",
      "           3       0.76      0.88      0.82       284\n",
      "           4       0.89      0.81      0.85       804\n",
      "           5       0.88      0.89      0.89      1159\n",
      "           6       0.84      0.88      0.86       426\n",
      "           7       0.33      1.00      0.50         1\n",
      "\n",
      "    accuracy                           0.86      2764\n",
      "   macro avg       0.62      0.70      0.64      2764\n",
      "weighted avg       0.86      0.86      0.86      2764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "y_pred = tabnet.predict(X_test)\n",
    "print(classification_report(Y_test, y_pred))\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "\n",
    "\n",
    "# results = pd.concat([results, pd.DataFrame({'Model': 'TabNet', 'Accuracy': accuracy_score(Y_val, y_pred)}, index=[0])], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"]=\"1\"\n",
    "\n",
    "train, test = train_test_split(df, random_state=42)\n",
    "train, val = train_test_split(train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical headers\n",
    "cat_headers = ['rating']\n",
    "\n",
    "# numerical headers\n",
    "num_headers = list(set(df.columns) - set(cat_headers))\n",
    "\n",
    "data_config = DataConfig(\n",
    "    target=[\n",
    "        \"rating\"\n",
    "    ],  # target should always be a list. Multi-targets are only supported for regression. Multi-Task Classification is not implemented\n",
    "    continuous_cols=num_headers,\n",
    "    categorical_cols=cat_headers,\n",
    ")\n",
    "trainer_config = TrainerConfig(\n",
    "    auto_lr_find=True,  # Runs the LRFinder to automatically derive a learning rate\n",
    "    batch_size=1024,\n",
    "    max_epochs=100,\n",
    ")\n",
    "optimizer_config = OptimizerConfig()\n",
    "\n",
    "model_config = CategoryEmbeddingModelConfig(\n",
    "    task=\"classification\",\n",
    "    layers=\"1024-512-512\",  # Number of nodes in each layer\n",
    "    activation=\"LeakyReLU\",  # Activation between each layers\n",
    "    learning_rate=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 14:26:59,618 - {pytorch_tabular.tabular_model:102} - INFO - Experiment Tracking is turned off\n",
      "Global seed set to 42\n",
      "2023-03-02 14:26:59,686 - {pytorch_tabular.tabular_model:465} - INFO - Preparing the DataLoaders\n",
      "2023-03-02 14:26:59,700 - {pytorch_tabular.tabular_datamodule:286} - INFO - Setting up the datamodule for classification task\n",
      "2023-03-02 14:27:00,708 - {pytorch_tabular.tabular_model:508} - INFO - Preparing the Model: CategoryEmbeddingModel\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_tabular/models/base_model.py:132: UserWarning: Plotly is not installed. Please install plotly to log logits. You can install plotly using pip install plotly or install PyTorch Tabular using pip install pytorch-tabular[all]\n",
      "  warnings.warn(\n",
      "2023-03-02 14:27:00,857 - {pytorch_tabular.tabular_model:264} - INFO - Preparing the Trainer\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-02 14:27:00,919 - {pytorch_tabular.tabular_model:558} - INFO - Auto LR Find Started\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::_unique2' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Initialize the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m tabular_model \u001b[39m=\u001b[39m TabularModel(\n\u001b[1;32m      3\u001b[0m     data_config\u001b[39m=\u001b[39mdata_config,\n\u001b[1;32m      4\u001b[0m     model_config\u001b[39m=\u001b[39mmodel_config,\n\u001b[1;32m      5\u001b[0m     optimizer_config\u001b[39m=\u001b[39moptimizer_config,\n\u001b[1;32m      6\u001b[0m     trainer_config\u001b[39m=\u001b[39mtrainer_config,\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m tabular_model\u001b[39m.\u001b[39;49mfit(train\u001b[39m=\u001b[39;49mtrain, validation\u001b[39m=\u001b[39;49mval)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_tabular/tabular_model.py:657\u001b[0m, in \u001b[0;36mTabularModel.fit\u001b[0;34m(self, train, validation, test, loss, metrics, optimizer, optimizer_params, train_sampler, target_transform, max_epochs, min_epochs, seed, callbacks, datamodule)\u001b[0m\n\u001b[1;32m    646\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    647\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mProviding test data in `fit` is deprecated and will be removed in next major release. Plese use `evaluate` for evaluating on test data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    648\u001b[0m         )\n\u001b[1;32m    649\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_model(\n\u001b[1;32m    650\u001b[0m     datamodule,\n\u001b[1;32m    651\u001b[0m     loss,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m     optimizer_params,\n\u001b[1;32m    655\u001b[0m )\n\u001b[0;32m--> 657\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(model, datamodule, callbacks, max_epochs, min_epochs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_tabular/tabular_model.py:559\u001b[0m, in \u001b[0;36mTabularModel.train\u001b[0;34m(self, model, datamodule, callbacks, max_epochs, min_epochs)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mauto_lr_find \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mfast_dev_run):\n\u001b[1;32m    558\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mAuto LR Find Started\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 559\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mtune(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, train_loader, val_loader)\n\u001b[1;32m    560\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    561\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSuggested LR: \u001b[39m\u001b[39m{\u001b[39;00mresult[\u001b[39m'\u001b[39m\u001b[39mlr_find\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msuggestion()\u001b[39m}\u001b[39;00m\u001b[39m. For plot and detailed analysis, use `find_learning_rate` method.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    562\u001b[0m     )\n\u001b[1;32m    563\u001b[0m     \u001b[39m# Parameters in models needs to be initialized again after LR find\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:969\u001b[0m, in \u001b[0;36mTrainer.tune\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, dataloaders, datamodule, scale_batch_size_kwargs, lr_find_kwargs, method)\u001b[0m\n\u001b[1;32m    966\u001b[0m Trainer\u001b[39m.\u001b[39m_log_api_event(\u001b[39m\"\u001b[39m\u001b[39mtune\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    968\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m--> 969\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtuner\u001b[39m.\u001b[39;49m_tune(\n\u001b[1;32m    970\u001b[0m         model,\n\u001b[1;32m    971\u001b[0m         train_dataloaders,\n\u001b[1;32m    972\u001b[0m         val_dataloaders,\n\u001b[1;32m    973\u001b[0m         dataloaders,\n\u001b[1;32m    974\u001b[0m         datamodule,\n\u001b[1;32m    975\u001b[0m         scale_batch_size_kwargs\u001b[39m=\u001b[39;49mscale_batch_size_kwargs,\n\u001b[1;32m    976\u001b[0m         lr_find_kwargs\u001b[39m=\u001b[39;49mlr_find_kwargs,\n\u001b[1;32m    977\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    978\u001b[0m     )\n\u001b[1;32m    980\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/tuner/tuning.py:101\u001b[0m, in \u001b[0;36mTuner._tune\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, dataloaders, datamodule, scale_batch_size_kwargs, lr_find_kwargs, method)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m     97\u001b[0m         model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m     98\u001b[0m     )\n\u001b[1;32m    100\u001b[0m     lr_find_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mupdate_attr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m     result[\u001b[39m\"\u001b[39m\u001b[39mlr_find\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlr_find(\n\u001b[1;32m    102\u001b[0m         model, train_dataloaders, val_dataloaders, dataloaders, datamodule, method, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mlr_find_kwargs\n\u001b[1;32m    103\u001b[0m     )\n\u001b[1;32m    104\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mFINISHED\n\u001b[1;32m    106\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/tuner/tuning.py:267\u001b[0m, in \u001b[0;36mTuner.lr_find\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, dataloaders, datamodule, method, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr)\u001b[0m\n\u001b[1;32m    264\u001b[0m lr_finder_callback\u001b[39m.\u001b[39m_early_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mcallbacks \u001b[39m=\u001b[39m [lr_finder_callback] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mcallbacks\n\u001b[0;32m--> 267\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mfit(model, train_dataloaders, val_dataloaders, datamodule)\n\u001b[1;32m    269\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mcallbacks \u001b[39m=\u001b[39m [cb \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mcallbacks \u001b[39mif\u001b[39;00m cb \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m lr_finder_callback]\n\u001b[1;32m    271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mauto_lr_find \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:603\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    602\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 603\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    604\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    605\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:645\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    638\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    639\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    641\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    643\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    644\u001b[0m )\n\u001b[0;32m--> 645\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    647\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    648\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1083\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39m==\u001b[39m TrainerFn\u001b[39m.\u001b[39mFITTING:\n\u001b[0;32m-> 1083\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_callback_hooks(\u001b[39m\"\u001b[39;49m\u001b[39mon_fit_start\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_lightning_module_hook(\u001b[39m\"\u001b[39m\u001b[39mon_fit_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1086\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_hyperparams()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1380\u001b[0m, in \u001b[0;36mTrainer._call_callback_hooks\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1378\u001b[0m     \u001b[39mif\u001b[39;00m callable(fn):\n\u001b[1;32m   1379\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Callback]\u001b[39m\u001b[39m{\u001b[39;00mcallback\u001b[39m.\u001b[39mstate_key\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1380\u001b[0m             fn(\u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1382\u001b[0m \u001b[39mif\u001b[39;00m pl_module:\n\u001b[1;32m   1383\u001b[0m     \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1384\u001b[0m     pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/callbacks/lr_finder.py:122\u001b[0m, in \u001b[0;36mLearningRateFinder.on_fit_start\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_fit_start\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m, pl_module: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlr_find(trainer, pl_module)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/callbacks/lr_finder.py:107\u001b[0m, in \u001b[0;36mLearningRateFinder.lr_find\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlr_find\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m, pl_module: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m--> 107\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimal_lr \u001b[39m=\u001b[39m lr_find(\n\u001b[1;32m    108\u001b[0m             trainer,\n\u001b[1;32m    109\u001b[0m             pl_module,\n\u001b[1;32m    110\u001b[0m             min_lr\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_min_lr,\n\u001b[1;32m    111\u001b[0m             max_lr\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_max_lr,\n\u001b[1;32m    112\u001b[0m             num_training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_training_steps,\n\u001b[1;32m    113\u001b[0m             mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mode,\n\u001b[1;32m    114\u001b[0m             early_stop_threshold\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_early_stop_threshold,\n\u001b[1;32m    115\u001b[0m             update_attr\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_attr,\n\u001b[1;32m    116\u001b[0m         )\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_early_exit:\n\u001b[1;32m    119\u001b[0m         \u001b[39mraise\u001b[39;00m _TunerExitException()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/tuner/lr_finder.py:241\u001b[0m, in \u001b[0;36mlr_find\u001b[0;34m(trainer, model, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr)\u001b[0m\n\u001b[1;32m    238\u001b[0m lr_finder\u001b[39m.\u001b[39m_exchange_scheduler(trainer)\n\u001b[1;32m    240\u001b[0m \u001b[39m# Fit, lr & loss logged in callback\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m _try_loop_run(trainer, params)\n\u001b[1;32m    243\u001b[0m \u001b[39m# Prompt if we stopped early\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mglobal_step \u001b[39m!=\u001b[39m num_training \u001b[39m+\u001b[39m start_steps:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/tuner/lr_finder.py:471\u001b[0m, in \u001b[0;36m_try_loop_run\u001b[0;34m(trainer, params)\u001b[0m\n\u001b[1;32m    469\u001b[0m loop\u001b[39m.\u001b[39mload_state_dict(deepcopy(params[\u001b[39m\"\u001b[39m\u001b[39mloop_state_dict\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[1;32m    470\u001b[0m loop\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 471\u001b[0m loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:267\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(dataloader, batch_to_device\u001b[39m=\u001b[39mbatch_to_device)\n\u001b[1;32m    266\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:251\u001b[0m, in \u001b[0;36mTrainingEpochLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mif\u001b[39;00m should_check_val:\n\u001b[1;32m    250\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mvalidating \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_validation()\n\u001b[1;32m    252\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[39m# update plateau LR scheduler after metrics are logged\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:310\u001b[0m, in \u001b[0;36mTrainingEpochLoop._run_validation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_loop\u001b[39m.\u001b[39m_reload_evaluation_dataloaders()\n\u001b[1;32m    309\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mval_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:152\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    151\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> 152\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    154\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:137\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    136\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    138\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:234\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \n\u001b[1;32m    225\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39m    the outputs of the step\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    233\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 234\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(hook_name, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    236\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1480\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1480\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1482\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[1;32m    389\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, ValidationStep)\n\u001b[0;32m--> 390\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_tabular/models/base_model.py:423\u001b[0m, in \u001b[0;36mBaseModel.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    421\u001b[0m     y_hat \u001b[39m=\u001b[39m output[\u001b[39m\"\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    422\u001b[0m     _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculate_loss(output, y, tag\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 423\u001b[0m     _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcalculate_metrics(y, y_hat, tag\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mvalid\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    424\u001b[0m \u001b[39mreturn\u001b[39;00m y_hat, y\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_tabular/models/base_model.py:289\u001b[0m, in \u001b[0;36mBaseModel.calculate_metrics\u001b[0;34m(self, y, y_hat, tag)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     y_hat \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)(y_hat\u001b[39m.\u001b[39msqueeze())\n\u001b[0;32m--> 289\u001b[0m     avg_metric \u001b[39m=\u001b[39m metric(y_hat, y\u001b[39m.\u001b[39;49msqueeze(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmetric_params)\n\u001b[1;32m    290\u001b[0m metrics\u001b[39m.\u001b[39mappend(avg_metric)\n\u001b[1;32m    291\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\n\u001b[1;32m    292\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtag\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mmetric_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    293\u001b[0m     avg_metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m     prog_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    298\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/torchmetrics/functional/classification/accuracy.py:418\u001b[0m, in \u001b[0;36maccuracy\u001b[0;34m(preds, target, task, threshold, num_classes, num_labels, average, multidim_average, top_k, ignore_index, validate_args)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(num_classes, \u001b[39mint\u001b[39m)\n\u001b[1;32m    417\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(top_k, \u001b[39mint\u001b[39m)\n\u001b[0;32m--> 418\u001b[0m     \u001b[39mreturn\u001b[39;00m multiclass_accuracy(\n\u001b[1;32m    419\u001b[0m         preds, target, num_classes, average, top_k, multidim_average, ignore_index, validate_args\n\u001b[1;32m    420\u001b[0m     )\n\u001b[1;32m    421\u001b[0m \u001b[39mif\u001b[39;00m task \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    422\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(num_labels, \u001b[39mint\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/torchmetrics/functional/classification/accuracy.py:263\u001b[0m, in \u001b[0;36mmulticlass_accuracy\u001b[0;34m(preds, target, num_classes, average, top_k, multidim_average, ignore_index, validate_args)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m validate_args:\n\u001b[1;32m    262\u001b[0m     _multiclass_stat_scores_arg_validation(num_classes, top_k, average, multidim_average, ignore_index)\n\u001b[0;32m--> 263\u001b[0m     _multiclass_stat_scores_tensor_validation(preds, target, num_classes, multidim_average, ignore_index)\n\u001b[1;32m    264\u001b[0m preds, target \u001b[39m=\u001b[39m _multiclass_stat_scores_format(preds, target, top_k)\n\u001b[1;32m    265\u001b[0m tp, fp, tn, fn \u001b[39m=\u001b[39m _multiclass_stat_scores_update(\n\u001b[1;32m    266\u001b[0m     preds, target, num_classes, top_k, average, multidim_average, ignore_index\n\u001b[1;32m    267\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/torchmetrics/functional/classification/stat_scores.py:303\u001b[0m, in \u001b[0;36m_multiclass_stat_scores_tensor_validation\u001b[0;34m(preds, target, num_classes, multidim_average, ignore_index)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    299\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEither `preds` and `target` both should have the (same) shape (N, ...), or `target` should be (N, ...)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m and `preds` should be (N, C, ...).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    301\u001b[0m     )\n\u001b[0;32m--> 303\u001b[0m num_unique_values \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(torch\u001b[39m.\u001b[39;49munique(target))\n\u001b[1;32m    304\u001b[0m \u001b[39mif\u001b[39;00m ignore_index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     check \u001b[39m=\u001b[39m num_unique_values \u001b[39m>\u001b[39m num_classes\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/torch/_jit_internal.py:485\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    484\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 485\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/torch/_jit_internal.py:485\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    484\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 485\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/torch/functional.py:877\u001b[0m, in \u001b[0;36m_return_output\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39minput\u001b[39m):\n\u001b[1;32m    875\u001b[0m     \u001b[39mreturn\u001b[39;00m _unique_impl(\u001b[39minput\u001b[39m, \u001b[39msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[0;32m--> 877\u001b[0m output, _, _ \u001b[39m=\u001b[39m _unique_impl(\u001b[39minput\u001b[39;49m, \u001b[39msorted\u001b[39;49m, return_inverse, return_counts, dim)\n\u001b[1;32m    878\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/torch/functional.py:791\u001b[0m, in \u001b[0;36m_unique_impl\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    783\u001b[0m     output, inverse_indices, counts \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39munique_dim(\n\u001b[1;32m    784\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[1;32m    785\u001b[0m         dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    788\u001b[0m         return_counts\u001b[39m=\u001b[39mreturn_counts,\n\u001b[1;32m    789\u001b[0m     )\n\u001b[1;32m    790\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 791\u001b[0m     output, inverse_indices, counts \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_unique2(\n\u001b[1;32m    792\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    793\u001b[0m         \u001b[39msorted\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39msorted\u001b[39;49m,\n\u001b[1;32m    794\u001b[0m         return_inverse\u001b[39m=\u001b[39;49mreturn_inverse,\n\u001b[1;32m    795\u001b[0m         return_counts\u001b[39m=\u001b[39;49mreturn_counts,\n\u001b[1;32m    796\u001b[0m     )\n\u001b[1;32m    797\u001b[0m \u001b[39mreturn\u001b[39;00m output, inverse_indices, counts\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::_unique2' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    ")\n",
    "tabular_model.fit(train=train, validation=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "result = tabular_model.evaluate([X_val,Y_val])\n",
    "pred_df = tabular_model.predict(X_test)\n",
    "tabular_model.save_model(\"tabTransformer/model\")\n",
    "loaded_model = TabularModel.load_from_checkpoint(\"tabTransformer/model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
