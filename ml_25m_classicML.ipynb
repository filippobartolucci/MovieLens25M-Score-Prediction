{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML_25M Classic ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "seed = 42\n",
    "pca = None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')\n",
    "\n",
    "if not os.path.exists('results/pca'):\n",
    "    os.mkdir('results/pca')\n",
    "\n",
    "if not os.path.exists('results/no_pca'):\n",
    "    os.mkdir('results/no_pca')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('rating', axis=1)\n",
    "Y = df['rating']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training samples: {X_train.shape}')\n",
    "print(f'Number of testing samples: {X_test.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('classicML/RFR'):\n",
    "    os.system('rm -r classicML/RFR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "n_tree = [ i for i in range(50, 100, 5)]\n",
    "criterion = [\"squared_error\", \"friedman_mse\", \"poisson\"]\n",
    "\n",
    "rfr_best_mse = float('inf')\n",
    "rfr_best_n = None\n",
    "rfr_best_c = None\n",
    "rfr_best = None\n",
    "\n",
    "i = 0\n",
    "max_iter = len(n_tree) * len(criterion)\n",
    "\n",
    "history_rfr = []\n",
    "\n",
    "for n, c in itertools.product(n_tree, criterion):\n",
    "    i += 1\n",
    "    log_name = f\"n_estimators={n}, criterion={c}\"\n",
    "    writer = SummaryWriter(f\"classicML/RFR/{log_name}\")\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=n, criterion=c)\n",
    "    rf.fit(X_train, Y_train)\n",
    "    Y_pred = rf.predict(X_test)\n",
    "    mse = np.mean((Y_pred - Y_test)**2)\n",
    "\n",
    "    history_rfr.append([n, c, mse])\n",
    "\n",
    "    writer.add_scalar('Loss', mse, n)\n",
    "    writer.add_hparams(\n",
    "        {'n_estimators': n, 'criterion': c},\n",
    "        {'mse': mse}\n",
    "    )\n",
    "    writer.flush()\n",
    "\n",
    "    if mse < rfr_best_mse:\n",
    "        rfr_best_mse = mse\n",
    "        rfr_best_n = n\n",
    "        rfr_best_c = c\n",
    "        rfr_best = copy.deepcopy(rf)\n",
    "    \n",
    "    print(\" Iteration: {}/{} - N_estimator: {} - Criterion: {} - MSE: {:.4f} - Best MSE: {:.4f}\".format(i, max_iter, n, c, mse, rfr_best_mse))\n",
    "\n",
    "Y_pred = rfr_best.predict(X_test)\n",
    "rf_r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(\"\\nRFR Hyperparameter Tuning Results\")\n",
    "print(\" - N estimators: \", rfr_best_n)\n",
    "print(\" - Criterion: \", rfr_best_c)\n",
    "print(\" - MSE: \", rfr_best_mse)\n",
    "print(\" - R2: \", rf_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results with sns \n",
    "history_rfr = pd.DataFrame(history_rfr, columns=['n_estimators', 'criterion', 'mse'])\n",
    "sns.lineplot(x='n_estimators', y='mse', hue='criterion', data=history_rfr, palette='Set1', marker='o')\n",
    "plt.title('Random Forest Regressor')\n",
    "plt.scatter(rfr_best_n, rfr_best_mse, marker='o', color='r', s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pca is not None:\n",
    "    history_rfr.to_csv('results/pca/RFR.csv', index=False, header=\"n_estimators,criterion,mse\".split(','))\n",
    "else:\n",
    "    history_rfr.to_csv('results/no_pca/RFR.csv', index=False, header=\"n_estimators,criterion,mse\".split(','))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('classicML/SVR'):\n",
    "    os.system('rm -r classicML/SVR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support vector regression tuning\n",
    "c = [0.001, 0.01, 0.1, 1]\n",
    "epsilon = [0.001, 0.01, 0.1, 1]\n",
    "kernel = ['linear', 'poly', 'rbf']\n",
    "\n",
    "svr_best_mse = float('inf')\n",
    "svr_best_c = None\n",
    "svr_best_epsilon = None\n",
    "svr_best_kernel = None\n",
    "svr_best = None\n",
    "\n",
    "i = 0\n",
    "max_iter = len(c) * len(epsilon) * len(kernel)\n",
    "\n",
    "history_svr = []\n",
    "\n",
    "for c_, epsilon_, kernel_ in itertools.product(c, epsilon, kernel):\n",
    "    i += 1\n",
    "    log_name = f\"c={c_}, epsilon={epsilon_}, kernel={kernel_}\"\n",
    "    writer = SummaryWriter(f\"classicML/SVR/{log_name}\")\n",
    "\n",
    "    svr = SVR(C=c_, epsilon=epsilon_, kernel=kernel_)\n",
    "    svr.fit(X_train, Y_train)\n",
    "    Y_pred = svr.predict(X_test)\n",
    "    mse = np.mean((Y_pred - Y_test)**2)\n",
    "\n",
    "    writer.add_hparams(\n",
    "        {'c': c_, 'epsilon': epsilon_, 'kernel': kernel_},\n",
    "        {'mse': mse}\n",
    "    )\n",
    "    writer.flush()\n",
    "\n",
    "    history_svr.append([c_, epsilon_, kernel_, mse])\n",
    "\n",
    "    if mse < svr_best_mse:\n",
    "        svr_best_mse = mse\n",
    "        svr_best_c = c_\n",
    "        svr_best_epsilon = epsilon_\n",
    "        svr_best_kernel = kernel_\n",
    "        svr_best = copy.deepcopy(svr)\n",
    "    \n",
    "    print(\" Iteration: {}/{} - C: {} - Epsilon: {} -  Kernel: {} - MSE: {:.4f} - Best MSE: {:.4f}\".format(i, max_iter, c_, epsilon_, kernel_, mse, svr_best_mse), end='\\r')\n",
    "\n",
    "Y_pred = svr_best.predict(X_test)\n",
    "svr_r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(\"\\nSVR Hyperparameter Tuning Results\")\n",
    "print(\" - C: \", svr_best_c)\n",
    "print(\" - Epsilon: \", svr_best_epsilon)\n",
    "print(\" - Kernel: \", svr_best_kernel)\n",
    "print(\" - MSE: \", svr_best_mse)\n",
    "print(\" - R2: \", svr_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_svr = pd.DataFrame(history_svr, columns=['c', 'epsilon', 'kernel', 'mse'])\n",
    "\n",
    "# plot results with sns\n",
    "sns.lineplot(x='c', y='mse', hue='kernel', data=history_svr, palette='Set1', marker='o')\n",
    "plt.title('Support Vector Regression')\n",
    "plt.xscale('log')\n",
    "plt.scatter(svr_best_c, svr_best_mse, marker='o', color='r', s=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "if pca is not None:\n",
    "    history_svr.to_csv('results/pca/SVR.csv', index=False, header=\"c,epsilon,kernel,mse\")\n",
    "else:\n",
    "    history_svr.to_csv('results/no_pca/SVR.csv', index=False, header=\"c,epsilon,kernel,mse\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('classicML/LinearR'):\n",
    "    os.system('rm -r classicML/LinearR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_name = f\"linear_regression\"\n",
    "writer = SummaryWriter(f\"classicML/LinearR/{log_name}\")\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, Y_train)\n",
    "Y_pred = lr.predict(X_test)\n",
    "mse = np.mean((Y_pred - Y_test)**2)\n",
    "\n",
    "writer.add_scalar('Loss', mse)\n",
    "writer.flush()\n",
    "\n",
    "Y_pred = lr.predict(X_test)\n",
    "lr_r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(\"Linear Regression Results\")\n",
    "print(\" - MSE: \", mse)\n",
    "print(\" - R2: \", lr_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pca is not None:\n",
    "    with open('results/pca/LinearR.txt', 'w') as f:\n",
    "        f.write(\"Linear Regression Results - MSE: {}, R2: {}\".format(mse, lr_r2))\n",
    "else:\n",
    "    with open('results/no_pca/LinearR.txt', 'w') as f:\n",
    "        f.write(\"Linear Regression Results - MSE: {}, R2: {}\".format(mse, lr_r2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('classicML/RidgeR'):\n",
    "    os.system('rm -r classicML/RidgeR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge regression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "alpha = [0.0001, 0.001, 0.1, 0.5, 1, 5, 10, 20]\n",
    "\n",
    "rr_best_mse = float('inf')\n",
    "rr_best_a = None\n",
    "rr_best = None\n",
    "\n",
    "history_rr = []\n",
    "\n",
    "i = 0\n",
    "max_iter = len(alpha)\n",
    "\n",
    "for a in alpha:\n",
    "    i += 1\n",
    "    log_name = f\"alpha={a}\"\n",
    "    writer = SummaryWriter(f\"classicML/RidgeR/{log_name}\")\n",
    "\n",
    "    rr = Ridge(alpha=a)\n",
    "    rr.fit(X_train, Y_train)\n",
    "    Y_pred = rr.predict(X_test)\n",
    "    mse = np.mean((Y_pred - Y_test)**2)\n",
    "\n",
    "    writer.add_scalar('Loss', mse, a)\n",
    "    writer.add_hparams(\n",
    "        {'alpha': a},\n",
    "        {'mse': mse}\n",
    "    )\n",
    "    \n",
    "    writer.flush()\n",
    "\n",
    "    history_rr.append((a,mse))\n",
    "\n",
    "    if mse < rr_best_mse:\n",
    "        rr_best_mse = mse\n",
    "        rr_best_a = a\n",
    "        rr_best = copy.deepcopy(rr)\n",
    "\n",
    "    print(\"Iteration: {}/{} - Alpha: {} - MSE: {:.4f} - Best MSE: {:.4f}\".format(i, max_iter, a, mse, rr_best_mse), end='\\r')\n",
    "\n",
    "Y_pred = rr_best.predict(X_test)\n",
    "rr_r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(\"\\nRidge Regression Hyperparameter Tuning Results\")\n",
    "print(\" - Alpha: \", rr_best_a)\n",
    "print(\" - MSE: \", rr_best_mse)\n",
    "print(\" - R2: \", rr_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize': (11.7, 8.27)})\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "history_rr = np.array(history_rr)\n",
    "\n",
    "ax = sns.lineplot(x=history_rr[:, 0], y=history_rr[:, 1], marker='o')\n",
    "\n",
    "ax.set(xlabel='Alpha', ylabel='MSE')\n",
    "ax.set_title('Ridge Regression')\n",
    "ax.scatter(rr_best_a, rr_best_mse, color='red', s=50, marker='o', zorder=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pca is not None:\n",
    "    np.savetxt('results/pca/RidgeR.csv', history_rr, delimiter=',', header='alpha,mse')\n",
    "else:\n",
    "    np.savetxt('results/no_pca/RidgeR.csv', history_rr, delimiter=',', header='alpha,mse')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('classicML/LassoR'):\n",
    "    os.system('rm -r classicML/LassoR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso regression\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "alpha = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "\n",
    "lasso_best_mse = float('inf')\n",
    "lasso_best_a = None\n",
    "lasso_best = None\n",
    "\n",
    "i = 0\n",
    "max_iter = len(alpha)\n",
    "\n",
    "history_lasso = []\n",
    "\n",
    "for a in alpha:\n",
    "    i += 1\n",
    "    log_name = f\"alpha={a}\"\n",
    "    writer = SummaryWriter(f\"classicML/LassoR/{log_name}\")\n",
    "\n",
    "    lasso = Lasso(alpha=a)\n",
    "    lasso.fit(X_train, Y_train)\n",
    "    Y_pred = lasso.predict(X_test)\n",
    "    mse = np.mean((Y_pred - Y_test)**2)\n",
    "\n",
    "    history_lasso.append((a,mse))\n",
    "\n",
    "    writer.add_scalar('Loss', mse, a)\n",
    "    writer.add_hparams(\n",
    "        {'alpha': a},\n",
    "        {'mse': mse}\n",
    "    )\n",
    "\n",
    "    if mse < lasso_best_mse:\n",
    "        lasso_best_mse = mse\n",
    "        lasso_best_a = a\n",
    "        lasso_best = copy.deepcopy(lasso)\n",
    "\n",
    "    print(\"Iteration: {}/{} - Alpha: {} - MSE: {:.4f} - Best MSE: {:.4f}\".format(i, max_iter, a, mse, lasso_best_mse), end='\\r')\n",
    "\n",
    "Y_pred = lasso_best.predict(X_test)\n",
    "lasso_r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(\"\\nLasso Regression Hyperparameter Tuning Results\")\n",
    "print(\" - Alpha: \", lasso_best_a)\n",
    "print(\" - MSE: \", lasso_best_mse)\n",
    "print(\" - R2: \", lasso_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "history_lasso = np.array(history_lasso)\n",
    "\n",
    "ax = sns.lineplot(x=history_lasso[:,0], y=history_lasso[:,1], marker='o')\n",
    "ax.set(xscale=\"log\")\n",
    "ax.set(xlabel='Alpha', ylabel='MSE')\n",
    "ax.set_title('Lasso Regression')\n",
    "ax.scatter(lasso_best_a, lasso_best_mse, color='red', s=50, marker='o', zorder=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pca is not None:\n",
    "    np.savetxt('results/pca/LassoR.csv', history_lasso, delimiter=',', header='alpha,mse')\n",
    "else:\n",
    "    np.savetxt('results/no_pca/LassoR.csv', history_lasso, delimiter=',', header='alpha,mse')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('classicML/ElasticNetR'):\n",
    "    os.system('rm -r classicML/ElasticNetR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elastic net\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "alpha = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "\n",
    "elastic_best_mse = float('inf')\n",
    "elastic_best_a = None\n",
    "elastic_best = None\n",
    "\n",
    "i = 0\n",
    "max_iter = len(alpha)\n",
    "\n",
    "history_elastic = []\n",
    "\n",
    "for a in alpha:\n",
    "    i += 1\n",
    "    log_name = f\"alpha={a}\"\n",
    "    writer = SummaryWriter(f\"classicML/ElasticNetR/{log_name}\")\n",
    "\n",
    "    elastic = ElasticNet(alpha=a)\n",
    "    elastic.fit(X_train, Y_train)\n",
    "    Y_pred = elastic.predict(X_test)\n",
    "    mse = np.mean((Y_pred - Y_test)**2)\n",
    "\n",
    "    writer.add_scalar('Loss', mse, a)\n",
    "    writer.add_hparams(\n",
    "        {'alpha': a},\n",
    "        {'mse': mse}\n",
    "    )\n",
    "\n",
    "    history_elastic.append((a,mse))\n",
    "\n",
    "    if mse < elastic_best_mse:\n",
    "        elastic_best_mse = mse\n",
    "        elastic_best_a = a\n",
    "        elastic_best = copy.deepcopy(elastic)\n",
    "\n",
    "    print(\"Iteration: {}/{} - Alpha: {} - MSE: {:.4f} - Best MSE: {:.4f}\".format(i, max_iter, a, mse, elastic_best_mse), end='\\r')\n",
    "\n",
    "Y_pred = elastic_best.predict(X_test)\n",
    "elastic_r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(\"\\nElastic Net Regression Hyperparameter Tuning Results\")\n",
    "print(\" - Alpha: \", elastic_best_a)\n",
    "print(\" - MSE: \", elastic_best_mse)\n",
    "print(\" - R2: \", elastic_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "history_elastic = np.array(history_elastic)\n",
    "history_elastic = history_elastic[history_elastic[:,0].argsort()]\n",
    "\n",
    "ax = sns.lineplot(x=history_elastic[:,0], y=history_elastic[:,1], marker='o')\n",
    "ax.set(xscale=\"log\")\n",
    "ax.set(xlabel='alpha', ylabel='MSE')\n",
    "ax.set_title('Elastic Net Regression')\n",
    "ax.scatter(elastic_best_a, elastic_best_mse, color='red', s=50, marker='o', zorder=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pca is not None:\n",
    "    np.savetxt('results/pca/ElasticNetR.csv', history_elastic, delimiter=',', header='alpha,mse')\n",
    "else:\n",
    "    np.savetxt('results/no_pca/ElasticNetR.csv', history_elastic, delimiter=',',header='alpha,mse')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
