{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analytics - ML25M "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "import itertools\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "\n",
    "    Args:\n",
    "        seed: the seed to use. \n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "\n",
    "fix_random(42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "movies.csv contains the following fields:\n",
    "\n",
    "* movieId - a unique identifier for each movie.\n",
    "* title - the title of the movie.\n",
    "* genres - a pipe-separated list of genres for the movie.\n",
    "\n",
    "It will be used to get the movie title and the genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv('./ml-25m/movies.csv')\n",
    "genres = movies_df['genres'].str.get_dummies(sep='|')\n",
    "movies_df = pd.concat([movies_df, genres], axis=1)\n",
    "movies_df.drop('genres', axis=1, inplace=True)\n",
    "movies_df.head(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "genome-scores.csv contains the following fields:\n",
    "\n",
    "* movieId - a unique identifier for each movie.\n",
    "* tagId - a unique identifier for each tag.\n",
    "* relevance - a score from 0.0 to 1.0 representing the relevance of the tag to the movie.\n",
    "\n",
    "Combined with the tags.csv file, this will be used to assign tags and their relevance to each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.read_csv('./ml-25m/genome-scores.csv')\n",
    "tags_df = pd.read_csv('./ml-25m/genome-tags.csv')\n",
    "ratings_df = pd.read_csv('./ml-25m/ratings.csv')\n",
    "\n",
    "df = movies_df.merge(scores_df, on='movieId')\n",
    "df = df.merge(tags_df, on='tagId')\n",
    "df = df.pivot_table(index=['movieId', 'title'], columns='tag', values='relevance', fill_value=0).reset_index().rename_axis(None, axis=1)\n",
    "\n",
    "# average rating for each movie\n",
    "ratings_df = ratings_df.groupby(['movieId'])['rating'].mean().reset_index()\n",
    "# round ratings to the nearest 0.5\n",
    "ratings_df['rating'] = ratings_df['rating'].apply(lambda x: round(x * 2) / 2)\n",
    "\n",
    "# # mode rating for each movie\n",
    "# ratings_df = ratings_df.groupby(['movieId'])['rating'].agg(lambda x: x.value_counts().index[0]).reset_index()\n",
    "\n",
    "df = ratings_df.merge(df, on='movieId')\n",
    "\n",
    "# movieId and title are not needed for the model\n",
    "df.drop(['movieId', 'title'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of samples: {df.shape[0]}')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratings distribution\n",
    "sns.countplot(x='rating', data=df, palette='flare')\n",
    "plt.title('Ratings distribution')\n",
    "for p in plt.gca().patches:\n",
    "    plt.gca().annotate('{:.0f}'.format(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "plt.show()\n",
    "\n",
    "# mean, median, and standard deviation of ratings\n",
    "print(\"Mode: \", df['rating'].mode())\n",
    "print('Median: {:.2f}'.format(df['rating'].median()))\n",
    "print('Std: {:.2f}'.format(df['rating'].std()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LABELS = df.rating.nunique()\n",
    "print(f'Number of labels: {N_LABELS}')\n",
    "\n",
    "X = df.drop('rating', axis=1)\n",
    "Y = df['rating']\n",
    "\n",
    "# encode Y\n",
    "Y = LabelEncoder().fit_transform(Y)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X, Y)\n",
    "X = lda.transform(X)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Number of training samples: {X_train.shape}')\n",
    "print(f'Number of testing samples: {X_test.shape}')\n",
    "\n",
    "results = pd.DataFrame(columns=['Model', 'Accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with 100 trees\n",
    "random_forest = RandomForestClassifier(n_estimators=50,\n",
    "                                bootstrap = True,\n",
    "                                max_features = 'sqrt')\n",
    "\n",
    "# Fit on training data\n",
    "random_forest.fit(X_train, Y_train)\n",
    "\n",
    "# accuracy\n",
    "accuracy = random_forest.score(X_test, Y_test)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "results = pd.concat([results, pd.DataFrame({'Model': 'Random Forest', 'Accuracy': accuracy}, index=[0])], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_classifier = LogisticRegression()\n",
    "logistic_classifier.fit(X_train, Y_train)\n",
    "\n",
    "# accuracy\n",
    "accuracy = logistic_classifier.score(X_test, Y_test)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "results = pd.concat([results, pd.DataFrame({'Model': 'Logistic Regression', 'Accuracy': accuracy}, index=[0])], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = GaussianNB()\n",
    "naive_bayes.fit(X_train, Y_train)\n",
    "\n",
    "# accuracy\n",
    "accuracy = naive_bayes.score(X_test, Y_test)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "results = pd.concat([results, pd.DataFrame({'Model': 'Naive Bayes', 'Accuracy': accuracy}, index=[0])], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "# accuracy\n",
    "accuracy = knn.score(X_test, Y_test)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "results = pd.concat([results, pd.DataFrame({'Model': 'KNN', 'Accuracy': accuracy}, index=[0])], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "svm.fit(X_train, Y_train)\n",
    "\n",
    "# accuracy\n",
    "accuracy = svm.score(X_test, Y_test)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "results = pd.concat([results, pd.DataFrame({'Model': 'SVM', 'Accuracy': accuracy}, index=[0])], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Number of training samples: {X_train.shape[0]}')\n",
    "print(f'Number of validation samples: {X_val.shape[0]}')\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(Y_train, dtype=torch.long)), batch_size=batch, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(Y_val, dtype=torch.long)), batch_size= 64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(Y_test, dtype=torch.long)), batch_size= 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effective Number of Samples\n",
    "def get_weights_effective_num_of_samples(no_of_classes, beta, samples_per_cls):\n",
    "    effective_num = 1.0 - np.power(beta, samples_per_cls)\n",
    "    class_weights = (1.0 - beta) / np.array(effective_num)\n",
    "    class_weights = class_weights / np.sum(class_weights)*no_of_classes\n",
    "    return {i: w for i, w in enumerate(class_weights)}\n",
    "\n",
    "weights = get_weights_effective_num_of_samples(N_LABELS, 0.999, np.bincount(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_model(input_size, hidden_size, num_classes, dropout_prob=0, depth=1):\n",
    "    model = [\n",
    "        torch.nn.Linear(input_size, hidden_size),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(dropout_prob)\n",
    "    ]\n",
    "\n",
    "    for i in range(depth):\n",
    "        model.append(torch.nn.Linear(hidden_size, hidden_size))\n",
    "        model.append(torch.nn.ReLU())\n",
    "        model.append(torch.nn.Dropout(dropout_prob))\n",
    "\n",
    "    model.append(torch.nn.Linear(hidden_size, num_classes))\n",
    "    model.append(torch.nn.Softmax(dim=1))\n",
    "\n",
    "    return torch.nn.Sequential(*model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_sizes = [32, 64, 128, 256]\n",
    "nums_epochs = [50, 100]\n",
    "depth = [2, 3, 4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "learning_rate = [0.01]\n",
    "step_size_lr_decay = [10, 25, 50]\n",
    "momentum = [0, 0.99]\n",
    "dropout_prob = 0.2\n",
    "patience = 10\n",
    "\n",
    "hyperparameters = itertools.product(hidden_sizes, depth, nums_epochs, batch_sizes, learning_rate, step_size_lr_decay, momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process\n",
    "def train_model(model, criterion, optimizer, scheduler, epochs, data_loader, val_loader, device, writer):\n",
    "    n_iter = 0\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_since_last_improvement = 0\n",
    " \n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        start_epoch = time.time()\n",
    "\n",
    "        for data, targets in data_loader:\n",
    "            data, targets = data.to(device), targets.to(device) \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(data)\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(y_pred, targets)\n",
    "            writer.add_scalar(\"Loss/train\", loss, n_iter)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            n_iter += 1\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Compute Val Loss\n",
    "        labels, _, y_pred = test_model(model, val_loader, device)\n",
    "        val_loss = criterion(y_pred, labels)\n",
    "        writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'models/best_model.pth')\n",
    "            epochs_since_last_improvement = 0\n",
    "        else:\n",
    "            epochs_since_last_improvement += 1\n",
    "\n",
    "        if epochs_since_last_improvement >= patience:\n",
    "            break\n",
    "\n",
    "        print('Epoch [{}/{}] - {:.2f} seconds - loss: {:.4f} - val_loss: {:.4f} - patience: {}'.format(epoch+1, epochs, time.time() - start_epoch, loss.item(), val_loss.item(), epochs_since_last_improvement), end='\\r')\n",
    "        \n",
    "    print('\\nTraining ended after {:.2f} seconds'.format(time.time() - start))\n",
    "\n",
    "    # Restore best model\n",
    "    model.load_state_dict(torch.load('models/best_model.pth'))\n",
    "    return model\n",
    "\n",
    "# evaluation process\n",
    "def test_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in data_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            y_pred.append(model(data))\n",
    "            y_test.append(targets)\n",
    "\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    y_test = torch.cat(y_test, dim=0)\n",
    "\n",
    "    y_pred_c = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "    return y_test, y_pred_c, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "best_model = None\n",
    "\n",
    "for hidden_size, depth, num_epochs, batch, lr, step_size, momentum in hyperparameters:\n",
    "    \n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "\n",
    "    print(\"\\nTraining with hidden_size={}, depth={}, num_epochs={}, batch={}, lr={}, step_size={}, momentum={}\".format(hidden_size, depth, num_epochs, batch, lr, step_size, momentum))\n",
    "    log_name = \"dim:\"+str(hidden_size)+\"_depth:\"+str(depth)+\"_epochs:\"+str(num_epochs)+\"_batch:\"+str(batch)+\"_lr:\"+str(lr)+\"_step_size:\"+str(step_size)+\"_momentum:\"+str(momentum)\n",
    "    \n",
    "    if os.path.exists('runs/'+log_name):\n",
    "        print(\"Model already trained, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # start tensorboard\n",
    "    writer = SummaryWriter('runs/'+log_name)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(Y_train, dtype=torch.long)), batch_size=batch, shuffle=True)\n",
    "\n",
    "    model = get_nn_model(X_train.shape[1], hidden_size, N_LABELS, dropout_prob, depth=depth)\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(list(weights.values()), dtype=torch.float32).to(device))\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)\n",
    "\n",
    "    # train\n",
    "    model = train_model(model, criterion, optimizer, scheduler, num_epochs, train_loader, train_loader, device, writer)\n",
    "\n",
    "    # validate\n",
    "    y_test, y_pred_c, y_pred = test_model(model, test_loader, device)\n",
    "\n",
    "    metrics = classification_report(y_test.cpu(), y_pred_c.cpu(), output_dict=True, zero_division=0)\n",
    "    writer.add_scalar('metrics/test accuracy', metrics['accuracy'])\n",
    "\n",
    "    writer.add_hparams({'hidden_size': hidden_size, 'depth': depth, 'num_epochs': num_epochs, 'batch': batch, 'lr': lr, 'step_size': step_size, 'momentum': momentum}, {'hparam/accuracy': metrics['accuracy']})\n",
    "\n",
    "    if metrics['accuracy'] > best_acc:\n",
    "        best_acc = metrics['accuracy']\n",
    "        best_model = model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    \n",
    "    torch.save(model.state_dict(), 'models/'+log_name+'.pth')\n",
    "\n",
    "    writer.flush()\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "y_test, y_pred_c, y_pred = test_model(best_model, test_loader, device)\n",
    "print(classification_report(y_test.cpu(), y_pred_c.cpu()))\n",
    "accuracy = accuracy_score(y_test.cpu(), y_pred_c.cpu())\n",
    "\n",
    "results = pd.concat([results, pd.DataFrame({'Model': 'NeuralNetwork', 'Accuracy': accuracy}, index=[0])], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning for Tabular Data\n",
    "\n",
    "[PyTorch Tabular](https://github.com/manujosephv/pytorch_tabular#installation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
