{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "\n",
    "    Args:\n",
    "        seed: the seed to use. \n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "\n",
    "fix_random(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# PyTorch Device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 9946\n",
      "Number of validation samples: 1106\n",
      "Number of testing samples: 2764\n",
      "\n",
      "Number of features: 552\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('rating', axis=1)\n",
    "Y = df['rating']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "\n",
    "Y_train = Y_train.to_numpy()\n",
    "Y_val = Y_val.to_numpy()\n",
    "Y_test = Y_test.to_numpy()\n",
    "\n",
    "Y_train = Y_train.reshape(-1, 1)\n",
    "Y_val = Y_val.reshape(-1, 1)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_val = pca.transform(X_val)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "print(f'Number of training samples: {X_train.shape[0]}')\n",
    "print(f'Number of validation samples: {X_val.shape[0]}')\n",
    "print(f'Number of testing samples: {X_test.shape[0]}')\n",
    "print(f'\\nNumber of features: {X_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperparameter combinations: 72\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "nums_epochs = [200]\n",
    "batch_sizes = [32, 64, 128]\n",
    "patience = [10]\n",
    "n_d_a = [64, 128, 256]\n",
    "n_shared = [2]\n",
    "n_indipendent = [2]\n",
    "n_step = [3,4,5,6,7,8,9,10]\n",
    "gamma = [1.3]\n",
    "epsilon = [1e-8]\n",
    "\n",
    "hyperparameters = itertools.product(n_d_a, n_step,n_indipendent,n_shared, gamma, epsilon,nums_epochs, batch_sizes)\n",
    "n_comb = len(n_d_a)*len(n_step)*len(n_indipendent)*len(n_shared)*len(gamma)*len(epsilon)*len(nums_epochs)*len(batch_sizes)\n",
    "print (f'Number of hyperparameter combinations: {n_comb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(n_d_a, n_step,n_indipendent,n_shared, gamma, epsilon):\n",
    "    model = TabNetRegressor(\n",
    "        # n_d: the dimensionality of the output space of the feature transformer network (default 64)\n",
    "        n_d=n_d_a,\n",
    "        # n_a: the dimensionality of the output space of the attention network (default 64)\n",
    "        n_a=n_d_a,\n",
    "        # n_steps: the number of sequential steps in the attention mechanism (default 3)\n",
    "        n_steps=n_step,\n",
    "        # gamma: the scaling factor for the feature transformer network (default 1.3)\n",
    "        gamma=gamma,\n",
    "        # optimizerm name of optimizer to use (default Adam)\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        # n_independent: the number of independent feature transformer networks to use (default 2)\n",
    "        n_independent=n_indipendent,\n",
    "        # n_shared: the number of shared feature transformer networks to use (default 2)\n",
    "        n_shared=n_shared,\n",
    "        # epsilon: a small value to add to the denominator of the feature importance calculation to avoid division by zero (default 1e-15)\n",
    "        epsilon=epsilon,\n",
    "        # seed: the random seed to use for reproducibility (default None)\n",
    "        seed=42,  \n",
    "        verbose=1\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iterations 1/72 - Hyperparameters:  batch_sizes=32, nums_epochs=200, n_d = 64, n_a=64, n_step=3, n_indipendent=2, n_shared=2, gamma=1.3, epsilon=1e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.76327 | val_0_mse: 0.37864 |  0:00:13s\n",
      "epoch 1  | loss: 0.2808  | val_0_mse: 0.23665 |  0:00:27s\n",
      "epoch 2  | loss: 0.20526 | val_0_mse: 0.10049 |  0:00:42s\n",
      "epoch 3  | loss: 0.10957 | val_0_mse: 0.05906 |  0:00:55s\n",
      "epoch 4  | loss: 0.07281 | val_0_mse: 0.06235 |  0:01:09s\n",
      "epoch 5  | loss: 0.04873 | val_0_mse: 0.01915 |  0:01:22s\n",
      "epoch 6  | loss: 0.03963 | val_0_mse: 0.01618 |  0:01:35s\n",
      "epoch 7  | loss: 0.03608 | val_0_mse: 0.01176 |  0:01:48s\n",
      "epoch 8  | loss: 0.03231 | val_0_mse: 0.01518 |  0:02:01s\n",
      "epoch 9  | loss: 0.03567 | val_0_mse: 0.01816 |  0:02:17s\n",
      "epoch 10 | loss: 0.03959 | val_0_mse: 0.01679 |  0:02:30s\n",
      "epoch 11 | loss: 0.03309 | val_0_mse: 0.0165  |  0:02:44s\n",
      "epoch 12 | loss: 0.0302  | val_0_mse: 0.01721 |  0:02:58s\n",
      "epoch 13 | loss: 0.02811 | val_0_mse: 0.01465 |  0:03:11s\n",
      "epoch 14 | loss: 0.03189 | val_0_mse: 0.05378 |  0:03:25s\n",
      "epoch 15 | loss: 0.02905 | val_0_mse: 0.0543  |  0:03:38s\n",
      "epoch 16 | loss: 0.02978 | val_0_mse: 0.0331  |  0:03:52s\n",
      "epoch 17 | loss: 0.02429 | val_0_mse: 0.02152 |  0:04:06s\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_0_mse = 0.01176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model MSE: 0.012397 - Best MSE: 0.012397\n",
      "Model R2 Score: 0.944071 - Best R2 Score: 0.944071\n",
      "\n",
      "Iterations 2/72 - Hyperparameters:  batch_sizes=64, nums_epochs=200, n_d = 64, n_a=64, n_step=3, n_indipendent=2, n_shared=2, gamma=1.3, epsilon=1e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.91322 | val_0_mse: 0.44907 |  0:00:09s\n",
      "epoch 1  | loss: 0.17522 | val_0_mse: 0.20618 |  0:00:18s\n",
      "epoch 2  | loss: 0.14037 | val_0_mse: 0.10915 |  0:00:28s\n",
      "epoch 3  | loss: 0.12945 | val_0_mse: 0.11945 |  0:00:38s\n",
      "epoch 4  | loss: 0.11363 | val_0_mse: 0.1006  |  0:00:47s\n",
      "epoch 5  | loss: 0.09397 | val_0_mse: 0.06025 |  0:00:56s\n",
      "epoch 6  | loss: 0.06891 | val_0_mse: 0.04598 |  0:01:06s\n",
      "epoch 7  | loss: 0.06752 | val_0_mse: 0.08414 |  0:01:16s\n",
      "epoch 8  | loss: 0.03783 | val_0_mse: 0.02421 |  0:01:25s\n",
      "epoch 9  | loss: 0.02932 | val_0_mse: 0.01594 |  0:01:35s\n",
      "epoch 10 | loss: 0.03177 | val_0_mse: 0.0177  |  0:01:44s\n",
      "epoch 11 | loss: 0.0226  | val_0_mse: 0.02537 |  0:01:54s\n",
      "epoch 12 | loss: 0.02003 | val_0_mse: 0.02292 |  0:02:03s\n",
      "epoch 13 | loss: 0.01869 | val_0_mse: 0.01581 |  0:02:13s\n",
      "epoch 14 | loss: 0.02052 | val_0_mse: 0.01207 |  0:02:22s\n",
      "epoch 15 | loss: 0.02142 | val_0_mse: 0.01572 |  0:02:32s\n",
      "epoch 16 | loss: 0.02508 | val_0_mse: 0.00878 |  0:02:43s\n",
      "epoch 17 | loss: 0.01776 | val_0_mse: 0.00766 |  0:02:54s\n",
      "epoch 18 | loss: 0.02179 | val_0_mse: 0.01042 |  0:03:03s\n",
      "epoch 19 | loss: 0.01406 | val_0_mse: 0.01136 |  0:03:12s\n",
      "epoch 20 | loss: 0.01618 | val_0_mse: 0.01188 |  0:03:23s\n",
      "epoch 21 | loss: 0.01575 | val_0_mse: 0.00924 |  0:03:56s\n",
      "epoch 22 | loss: 0.01605 | val_0_mse: 0.02326 |  0:04:08s\n",
      "epoch 23 | loss: 0.01434 | val_0_mse: 0.02049 |  0:04:20s\n",
      "epoch 24 | loss: 0.01713 | val_0_mse: 0.00859 |  0:04:33s\n",
      "epoch 25 | loss: 0.01363 | val_0_mse: 0.01028 |  0:04:44s\n",
      "epoch 26 | loss: 0.01271 | val_0_mse: 0.03324 |  0:04:53s\n",
      "epoch 27 | loss: 0.01258 | val_0_mse: 0.01016 |  0:05:02s\n",
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_mse = 0.00766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model MSE: 0.008318 - Best MSE: 0.008318\n",
      "Model R2 Score: 0.962473 - Best R2 Score: 0.962473\n",
      "\n",
      "Iterations 3/72 - Hyperparameters:  batch_sizes=128, nums_epochs=200, n_d = 64, n_a=64, n_step=3, n_indipendent=2, n_shared=2, gamma=1.3, epsilon=1e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.65315 | val_0_mse: 1.25198 |  0:00:05s\n",
      "epoch 1  | loss: 0.30322 | val_0_mse: 0.36639 |  0:00:11s\n",
      "epoch 2  | loss: 0.2582  | val_0_mse: 0.3643  |  0:00:16s\n",
      "epoch 3  | loss: 0.24437 | val_0_mse: 0.15897 |  0:00:22s\n",
      "epoch 4  | loss: 0.14972 | val_0_mse: 0.10124 |  0:00:27s\n",
      "epoch 5  | loss: 0.10065 | val_0_mse: 0.11218 |  0:00:33s\n",
      "epoch 6  | loss: 0.07446 | val_0_mse: 0.06795 |  0:00:38s\n",
      "epoch 7  | loss: 0.05395 | val_0_mse: 0.03607 |  0:00:44s\n",
      "epoch 8  | loss: 0.04008 | val_0_mse: 0.04405 |  0:00:50s\n",
      "epoch 9  | loss: 0.03634 | val_0_mse: 0.02483 |  0:00:56s\n",
      "epoch 10 | loss: 0.02516 | val_0_mse: 0.02107 |  0:01:01s\n",
      "epoch 11 | loss: 0.02424 | val_0_mse: 0.01704 |  0:01:09s\n",
      "epoch 12 | loss: 0.02067 | val_0_mse: 0.01158 |  0:01:15s\n",
      "epoch 13 | loss: 0.01928 | val_0_mse: 0.02224 |  0:01:20s\n",
      "epoch 14 | loss: 0.01694 | val_0_mse: 0.01275 |  0:01:26s\n",
      "epoch 15 | loss: 0.01658 | val_0_mse: 0.02593 |  0:01:31s\n",
      "epoch 16 | loss: 0.01725 | val_0_mse: 0.02399 |  0:01:37s\n",
      "epoch 17 | loss: 0.01561 | val_0_mse: 0.01163 |  0:01:42s\n",
      "epoch 18 | loss: 0.01461 | val_0_mse: 0.00839 |  0:01:47s\n",
      "epoch 19 | loss: 0.01422 | val_0_mse: 0.01754 |  0:01:53s\n",
      "epoch 20 | loss: 0.01913 | val_0_mse: 0.01146 |  0:01:58s\n",
      "epoch 21 | loss: 0.01273 | val_0_mse: 0.00771 |  0:02:04s\n",
      "epoch 22 | loss: 0.01513 | val_0_mse: 0.00715 |  0:02:09s\n",
      "epoch 23 | loss: 0.01527 | val_0_mse: 0.01441 |  0:02:16s\n",
      "epoch 24 | loss: 0.01619 | val_0_mse: 0.00933 |  0:02:21s\n",
      "epoch 25 | loss: 0.01169 | val_0_mse: 0.00774 |  0:02:27s\n",
      "epoch 26 | loss: 0.01281 | val_0_mse: 0.01357 |  0:02:33s\n",
      "epoch 27 | loss: 0.01166 | val_0_mse: 0.01732 |  0:02:38s\n",
      "epoch 28 | loss: 0.01023 | val_0_mse: 0.01055 |  0:02:43s\n",
      "epoch 29 | loss: 0.01391 | val_0_mse: 0.01284 |  0:02:48s\n",
      "epoch 30 | loss: 0.01606 | val_0_mse: 0.01106 |  0:02:54s\n",
      "epoch 31 | loss: 0.01233 | val_0_mse: 0.04176 |  0:02:59s\n",
      "epoch 32 | loss: 0.01087 | val_0_mse: 0.00805 |  0:03:04s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_mse = 0.00715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model MSE: 0.007774 - Best MSE: 0.007774\n",
      "Model R2 Score: 0.964929 - Best R2 Score: 0.964929\n",
      "\n",
      "Iterations 4/72 - Hyperparameters:  batch_sizes=32, nums_epochs=200, n_d = 64, n_a=64, n_step=4, n_indipendent=2, n_shared=2, gamma=1.3, epsilon=1e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.85539 | val_0_mse: 0.3611  |  0:00:18s\n",
      "epoch 1  | loss: 0.26385 | val_0_mse: 0.21781 |  0:00:39s\n"
     ]
    }
   ],
   "source": [
    "current_iter = 0\n",
    "\n",
    "best_model = None\n",
    "best_mse = float('inf')\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "for n_d_a, n_step,n_indipendent,n_shared, gamma, epsilon,nums_epochs, batch_sizes in hyperparameters:\n",
    "    current_iter += 1\n",
    "\n",
    "    print(\"\\nIterations {}/{} - Hyperparameters:  batch_sizes={}, nums_epochs={}, n_d = {}, n_a={}, n_step={}, n_indipendent={}, n_shared={}, gamma={}, epsilon={}\".format(\n",
    "        current_iter, n_comb, batch_sizes, nums_epochs, n_d_a, n_d_a, n_step, n_indipendent, n_shared, gamma, epsilon ))\n",
    "\n",
    "    model = get_model(n_d_a, n_step, n_indipendent, n_shared, gamma, epsilon)\n",
    "\n",
    "    # train\n",
    "    model.fit(\n",
    "                X_train=X_train,\n",
    "                y_train=Y_train,\n",
    "                eval_set=[(X_val, Y_val)],\n",
    "                eval_metric=['mse'],\n",
    "                # patience: the number of epochs to wait without improvement in validation loss before early stopping (default 10)\n",
    "                patience=10,\n",
    "                # batch_size: the number of samples per batch (default 1024)\n",
    "                batch_size=batch_sizes,\n",
    "                # virtual_batch_size: the number of samples per virtual batch (default 128)\n",
    "                virtual_batch_size=128,\n",
    "                # num_workers: the number of worker processes to use for data loading (default 0)\n",
    "                num_workers=0,\n",
    "                # drop_last: whether to drop the last incomplete batch if the dataset size is not divisible by the batch size (default False)\n",
    "                drop_last=False,\n",
    "                # max_epochs: the maximum number of epochs to train for (default 100)\n",
    "                max_epochs=nums_epochs,\n",
    "            )\n",
    "\n",
    "    # Predict\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    # Calculate the MSE\n",
    "    mse = mean_squared_error(Y_test, preds)\n",
    "    \n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_model = model\n",
    "        # # save model\n",
    "        # torch.save(best_model, 'best_tab_model.pt')\n",
    "        # # save config\n",
    "        # with open('best_tab_model_config.json', 'w') as f:\n",
    "        #     json.dump({'n_d':n_d, 'n_a':n_a, 'n_step':n_step, 'n_indipendent':n_indipendent, 'n_shared':n_shared, 'gamma':gamma, 'epsilon':epsilon, 'batch_sizes':batch_sizes, 'nums_epochs':nums_epochs }, f)\n",
    "            \n",
    "            \n",
    "    print(\"Model MSE: {:.6f} - Best MSE: {:.6f}\".format(mse, best_mse))\n",
    "    print(\"Model R2 Score: {:.6f} - Best R2 Score: {:.6f}\".format(r2_score(Y_test, preds), r2_score(Y_test, best_model.predict(X_test))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "# Calculate the RMSE\n",
    "rmse = mean_squared_error(Y_test, preds)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'R2 Score: {r2_score(Y_test, preds)}')\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results with sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_context('talk')\n",
    "sns.set_palette('colorblind')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(Y_test, preds, s=1)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('True Values vs Predictions')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
