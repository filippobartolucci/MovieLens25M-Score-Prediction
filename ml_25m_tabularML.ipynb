{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "pca = None\n",
    "\n",
    "def fix_random(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "\n",
    "fix_random(42)\n",
    "\n",
    "# check if tabular ML folder exists\n",
    "if not os.path.exists('tabularML'):\n",
    "    os.makedirs('tabularML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# PyTorch Device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('rating', axis=1)\n",
    "Y = df['rating']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "\n",
    "Y_train = Y_train.to_numpy()\n",
    "Y_val = Y_val.to_numpy()\n",
    "Y_test = Y_test.to_numpy()\n",
    "\n",
    "Y_train = Y_train.reshape(-1, 1)\n",
    "Y_val = Y_val.reshape(-1, 1)\n",
    "\n",
    "print(f'Number of training samples: {X_train.shape[0]}')\n",
    "print(f'Number of validation samples: {X_val.shape[0]}')\n",
    "print(f'Number of testing samples: {X_test.shape[0]}')\n",
    "print(f'\\nNumber of features: {X_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_val = pca.transform(X_val)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "print(f'Number of training samples: {X_train.shape[0]}')\n",
    "print(f'Number of validation samples: {X_val.shape[0]}')\n",
    "print(f'Number of testing samples: {X_test.shape[0]}')\n",
    "print(f'\\nNumber of features: {X_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "nums_epochs = [200]\n",
    "batch_sizes = [256]\n",
    "patience = [10]\n",
    "n_d_a = [16]\n",
    "n_shared = [2]\n",
    "n_indipendent = [2]\n",
    "n_step = [6]\n",
    "gamma = [1.3]\n",
    "epsilon = [1e-8]\n",
    "\n",
    "hyperparameters = itertools.product(n_d_a, n_step,n_indipendent,n_shared, gamma, epsilon,nums_epochs, batch_sizes)\n",
    "n_comb = len(n_d_a)*len(n_step)*len(n_indipendent)*len(n_shared)*len(gamma)*len(epsilon)*len(nums_epochs)*len(batch_sizes)\n",
    "print (f'Number of hyperparameter combinations: {n_comb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(n_d_a, n_step,n_indipendent,n_shared, gamma, epsilon):\n",
    "    model = TabNetRegressor(\n",
    "        # n_d: the dimensionality of the output space of the feature transformer network (default 64)\n",
    "        n_d=n_d_a,\n",
    "        # n_a: the dimensionality of the output space of the attention network (default 64)\n",
    "        n_a=n_d_a,\n",
    "        # n_steps: the number of sequential steps in the attention mechanism (default 3)\n",
    "        n_steps=n_step,\n",
    "        # gamma: the scaling factor for the feature transformer network (default 1.3)\n",
    "        gamma=gamma,\n",
    "        # optimizerm name of optimizer to use (default Adam)\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        # n_independent: the number of independent feature transformer networks to use (default 2)\n",
    "        n_independent=n_indipendent,\n",
    "        # n_shared: the number of shared feature transformer networks to use (default 2)\n",
    "        n_shared=n_shared,\n",
    "        # epsilon: a small value to add to the denominator of the feature importance calculation to avoid division by zero (default 1e-15)\n",
    "        epsilon=epsilon,\n",
    "        # seed: the random seed to use for reproducibility (default None)\n",
    "        seed=42,  \n",
    "        verbose=1\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_folder = None\n",
    "if pca is not None:\n",
    "    if os.path.exists('tabularML/pca'):\n",
    "        os.system('rm -r tabularML/pca')\n",
    "    else:\n",
    "        os.makedirs('tabularML/pca')\n",
    "    pca_folder = 'pca'\n",
    "else:\n",
    "    if os.path.exists('tabularML/no_pca'):\n",
    "        os.system('rm -r tabularML/no_pca')\n",
    "    else:\n",
    "        os.makedirs('tabularML/no_pca')\n",
    "    pca_folder = 'no_pca'\n",
    "\n",
    "current_iter = 0\n",
    "\n",
    "best_mse = float('inf')\n",
    "best_model = None\n",
    "best_n_d = None\n",
    "best_n_a = None\n",
    "best_n_step = None\n",
    "best_n_indipendent = None\n",
    "best_n_shared = None\n",
    "best_gamma = None\n",
    "best_batch_size = None\n",
    "\n",
    "for n_d_a, n_step,n_indipendent,n_shared, gamma, epsilon,nums_epochs, batch_sizes in hyperparameters:\n",
    "    current_iter += 1\n",
    "\n",
    "    print(\"\\nIterations {}/{} - Hyperparameters:  batch_sizes={}, nums_epochs={}, n_d={}, n_a={}, n_step={}, n_indipendent={}, n_shared={}, gamma={}, epsilon={}\".format(\n",
    "        current_iter, n_comb, batch_sizes, nums_epochs, n_d_a, n_d_a, n_step, n_indipendent, n_shared, gamma, epsilon ))\n",
    "\n",
    "    model = get_model(n_d_a, n_step, n_indipendent, n_shared, gamma, epsilon)\n",
    "    \n",
    "    log_name = \"n_d:\"+str(n_d_a)+\"n_a:\"+str(n_d_a)+\"n_step:\"+str(n_step)+\"n_indipendent:\"+str(n_indipendent)+\"n_shared:\"+str(n_shared)+\"gamma:\"+str(gamma)+\"epsilon:\"+str(epsilon)\n",
    "    \n",
    "    # start tensorboard\n",
    "    writer = SummaryWriter('tabularML/'+pca_folder+'/'+log_name)\n",
    "    \n",
    "    # train\n",
    "    model.fit(\n",
    "                X_train=X_train,\n",
    "                y_train=Y_train,\n",
    "                eval_set=[(X_val, Y_val)],\n",
    "                eval_metric=['mse'],\n",
    "                # patience: the number of epochs to wait without improvement in validation loss before early stopping (default 10)\n",
    "                patience=10,\n",
    "                # batch_size: the number of samples per batch (default 1024)\n",
    "                batch_size=1024,\n",
    "                # virtual_batch_size: the number of samples per virtual batch (default 128)\n",
    "                virtual_batch_size=128,\n",
    "                # num_workers: the number of worker processes to use for data loading (default 0)\n",
    "                num_workers=0,\n",
    "                # drop_last: whether to drop the last incomplete batch if the dataset size is not divisible by the batch size (default False)\n",
    "                drop_last=False,\n",
    "                # max_epochs: the maximum number of epochs to train for (default 100)\n",
    "                max_epochs=nums_epochs,\n",
    "            )\n",
    "    \n",
    "    writer.add_hparams({'n_d':n_d_a, 'n_a':n_d_a, 'n_step':n_step, 'n_indipendent':n_indipendent, 'n_shared':n_shared, 'gamma':gamma, 'epsilon':epsilon, 'batch_sizes':batch_sizes, 'nums_epochs':nums_epochs }, {'hparam/mse': model.best_cost})\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    mse = mean_squared_error(Y_test, preds)\n",
    "    \n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_n_d = n_d_a\n",
    "        best_n_a = n_d_a\n",
    "        best_n_step = n_step\n",
    "        best_n_indipendent = n_indipendent\n",
    "        best_n_shared = n_shared\n",
    "        best_gamma = gamma\n",
    "        best_batch_size = batch_sizes\n",
    "        best_model = copy.deepcopy(model)             \n",
    "            \n",
    "    print(\"Model MSE: {:.6f} - Best MSE: {:.6f}\".format(mse, best_mse))\n",
    "    print(\"Model R2 Score: {:.6f} - Best R2 Score: {:.6f}\".format(r2_score(Y_test, preds), r2_score(Y_test, best_model.predict(X_test))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = best_model.history\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['loss'], label='train')\n",
    "plt.plot(history['val_0_mse'], label='validation')\n",
    "plt.title('Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "sns.set_context('talk')\n",
    "sns.set_palette('colorblind')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(Y_test, preds, s=1)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('True Values vs Predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and validation losses\n",
    "train_losses = history['loss']\n",
    "valid_losses = history['val_0_mse']\n",
    "\n",
    "train_array = np.array(train_losses)\n",
    "valid_array = np.array(valid_losses)\n",
    "\n",
    "# create dataframe\n",
    "df = pd.DataFrame({'training loss': train_array, 'validation loss': valid_array})\n",
    "\n",
    "df.to_csv('tabularML/'+pca_folder+'/loss.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
